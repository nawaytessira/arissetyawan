@article{Abbas2017,
abstract = {Feedforward neural networks (FFNN) have been utilised for various research in machine learning and they have gained a significantly wide acceptance. However, it was recently noted that the feedforward neural network has been functioning slower than needed. As a result, it has created critical bottlenecks among its applications. Extreme Learning Machines (ELM) were suggested as alternative learning algorithms instead of FFNN. The former is characterised by single-hidden layer feedforward neural networks (SLFN). It selects hidden nodes randomly and analytically determines their output weight. This review aims to, first, present a short mathematical explanation to explain the basic ELM. Second, because of its notable simplicity, efficiency, and remarkable generalisation performance, ELM has had wide uses in various domains, such as computer vision, biomedical engineering, control and robotics, system identification, etc. Thus, in this review, we will aim to present a complete view of these ELM advances for different applications. Finally, ELM's strengths and weakness will be presented, along with its future perspectives.},
author = {Abbas, Musatafa and Albadr, Abbood and Tiun, Sabrina},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/abbas2017.pdf:pdf},
journal = {International Journal of Applied Engineering Research ISSN},
keywords = {Extreme Learning Machine,Single-Hidden Layer Feedforward Neural Networks},
number = {14},
pages = {973--4562},
title = {{Extreme Learning Machine: A Review}},
url = {http://www.ripublication.com},
volume = {12},
year = {2017}
}
@article{Almeida2015,
author = {Almeida, Augusto},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elm-python.pdf:pdf},
title = {{Python Extreme Learning Machine ( ELM ) Documentation}},
year = {2015}
}
@article{Alom2017,
abstract = {Extreme Learning Machines (ELM) has been introduced as a new algorithm for training single hidden layer feedforward neural networks instead of the classical gradient-based approaches. Based on the consistency property of data, which enforces similar samples to share similar properties, ELM is a biologically inspired learning algorithm that learns much faster with good generalization and performs well in classification tasks. However, the stochastic characteristics of hidden layer outputs from the random generation of the weight matrix in current ELMs leads to the possibility of unstable outputs in the learning and testing phases. This is detrimental to the overall performance when many repeated trials are conducted. To cope with this issue, we present a new ELM approach, named State Preserving Extreme Leaning Machine (SPELM). SPELM ensures the overall training and testing performance of the classical ELM while monotonically increases its accuracy by preserving state variables. For evaluation, experiments are performed on different benchmark datasets including applications in face recognition, pedestrian detection, and network intrusion detection for cyber security. Several popular feature extraction techniques, namely Gabor, pyramid histogram of oriented gradients, and local binary pattern are also incorporated with SPELM. Experimental results show that our SPELM algorithm yields the best performance on tested data over ELM and RELM. {\textcopyright} 2016, Springer Science+Business Media New York.},
author = {Alom, Md Zahangir and Sidike, Paheding and Taha, Tarek M. and Asari, Vijayan K.},
doi = {10.1007/s11063-016-9552-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/alom2016.pdf:pdf},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Extreme learning machine (ELM),Face recognition,Feature extraction,Intrusion detection,Pedestrian detection,State preserving ELM},
number = {2},
pages = {703--725},
publisher = {Springer US},
title = {{State Preserving Extreme Learning Machine: A Monotonically Increasing Learning Approach}},
volume = {45},
year = {2017}
}
@article{Balasundaram2013,
abstract = {In this paper, extreme learning machine (ELM) for $\epsilon$-insensitive error loss function-based regression problem formulated in 2-norm as an unconstrained optimization problem in primal variables is proposed. Since the objective function of this unconstrained optimization problem is not twice differentiable, the popular generalized Hessian matrix and smoothing approaches are considered which lead to optimization problems whose solutions are determined using fast Newton-Armijo algorithm. The main advantage of the algorithm is that at each iteration, a system of linear equations is solved. By performing numerical experiments on a number of interesting synthetic and real-world datasets, the results of the proposed method are compared with that of ELM using additive and radial basis function hidden nodes and of support vector regression (SVR) using Gaussian kernel. Similar or better generalization performance of the proposed method on the test data in comparable computational time over ELM and SVR clearly illustrates its efficiency and applicability. {\textcopyright} 2012 Springer-Verlag London Limited.},
author = {Balasundaram, S. and Kapil},
doi = {10.1007/s00521-011-0798-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/balasundaram2012.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Generalized Hessian matrix,Newton method,Single hidden layer feedforward neural networks,Smoothing technique,Support vector regression},
number = {3-4},
pages = {559--567},
title = {{On extreme learning machine for ??-insensitive regression in the primal by Newton method}},
volume = {22},
year = {2013}
}
@article{Bazi2014,
abstract = {Recently, a new machine learning approach that is termed as the extreme learning machine (ELM) has been introduced in the literature. This approach is characterized by a unified formulation for regression, binary, and multiclass classification problems, and the related solution is given in an analytical compact form. In this letter, we propose an efficient classification method for hyperspectral images based on this machine learning approach. To address the model selection issue that is associated with the ELM, we develop an automatic-solution-based differential evolution (DE). This simple yet powerful evolutionary optimization algorithm uses cross-validation accuracy as a performance indicator for determining the optimal ELM parameters. Experimental results obtained from four benchmark hyperspectral data sets confirm the attractive properties of the proposed DE-ELM method in terms of classification accuracy and computation time.},
author = {Bazi, Y and Alajlan, N and Melgani, F and AlHichri, H and Malek, S and Yager, R R},
doi = {10.1109/lgrs.2013.2286078},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/bazi2014.pdf:pdf},
isbn = {1545-598X},
issn = {1545-598X},
journal = {Ieee Geoscience and Remote Sensing Letters},
keywords = {CHLOROPHYLL CONCENTRATION,Differential evolution (DE),Engineering, Electrical {\&} Electronic,FUSION,GAUSSIAN PROCESS REGRESSION,Geochemistry {\&} Geophysics,Imaging Science {\&} Photographic Technology,Remote,SPECTRAL-SPATIAL CLASSIFICATION,Sensing,extraction,extreme learning machine (ELM),feature,hyperspectral images},
number = {6},
pages = {1066--1070},
title = {{Differential Evolution Extreme Learning Machine for the Classification of Hyperspectral Images}},
volume = {11},
year = {2014}
}
@article{Bencherif2015,
author = {Bencherif, Mohamed A and Bazi, Yakoub and Member, Senior and Guessoum, Abderrezak and Alajlan, Naif and Member, Senior and Melgani, Farid and Member, Senior and Alhichri, Haikel},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/bencherif2015.pdf:pdf},
journal = {IEEE Geoscience and Remote Sensing Letters},
number = {3},
pages = {527--531},
title = {{Fusion of Extreme Learning Machine and Graph-Based Optimization Methods for Active Classification of Remote Sensing Images}},
volume = {12},
year = {2015}
}
@article{Berraho2016,
author = {Berraho, Sanae and Margae, Samira El and {Ait Kerroum}, Mounir and Fakhri, Youssef},
doi = {10.1007/s11042-016-4174-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/berraho2016.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {Texture classification,Digital curvelet transform,,digital curvelet transform,dimension reduction,extreme learning,lsda,machine,texture classification,these authors contributed equally},
publisher = {Multimedia Tools and Applications},
title = {{Texture classification based on curvelet transform and extreme learning machine with reduced feature set}},
url = {http://link.springer.com/10.1007/s11042-016-4174-8},
year = {2016}
}
@article{Cao2011,
abstract = {In this paper, we introduce a new learning method for composite function wavelet neural networks (CFWNN) by combining the differential evolution (DE) algorithm with extreme learning machine (ELM), in short, as CWN-E-ELM. The recently proposed CFWNN trained with ELM (CFWNN-ELM) has several promising features. But the CFWNN-ELM may have some redundant nodes due to the number of hidden nodes assigned a priori and the input weight matrix and the hidden node parameter vector randomly generated once and never changed during the learning phase. The introduction of DE into CFWNN-ELM is to search for the optimal network parameters and to reduce the number of hidden nodes used in the network. Simulations on several artificial function approximations, real-world data regressions and a chaotic signal prediction problem show some advantages of the proposed CWN-E-ELM. Compared with CFWNN-ELM, CWN-E-ELM has a much more compact network size and Compared with several relevant methods, CWN-E-ELM is able to achieve a better generalization performance.},
author = {Cao, J W and Lin, Z P and Huang, G B},
doi = {10.1007/s11063-011-9176-y},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/cao2011.pdf:pdf},
isbn = {1370-4621},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {CLASSIFICATION,Composite functions,Computer Science, Artificial Intelligence,Differential evolution,Extreme learning machine,REPRESENTATION,Wavelet neural networks},
number = {3},
pages = {251--265},
title = {{Composite Function Wavelet Neural Networks with Differential Evolution and Extreme Learning Machine}},
volume = {33},
year = {2011}
}
@article{Cao2012,
abstract = {In this paper, we propose an improved learning algorithm named self-adaptive evolutionary extreme learning machine (SaE-ELM) for single hidden layer feedforward networks (SLFNs). In SaE-ELM, the network hidden node parameters are optimized by the self-adaptive differential evolution algorithm, whose trial vector generation strategies and their associated control parameters are self-adapted in a strategy pool by learning from their previous experiences in generating promising solutions, and the network output weights are calculated using the Moore–Penrose generalized inverse. SaE-ELM outperforms the evolutionary extreme learning machine (E-ELM) and the different evolutionary Levenberg–Marquardt method in general as it could self-adaptively determine the suitable control parameters and generation strategies involved in DE. Simulations have shown that SaE-ELM not only performs better than E-ELM with several manually choosing generation strategies and control parameters but also obtains better generalization performances than several related methods.},
author = {Cao, Jiuwen and Lin, Zhiping and Huang, Guang Bin},
doi = {10.1007/s11063-012-9236-y},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/cao2012.pdf:pdf},
isbn = {1370-4621},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Differential evolution,Extreme learning machine,Levenberg-Marquardt algorithm,Single hidden layer feedforward networks,Support vector machine},
number = {3},
pages = {285--305},
title = {{Self-adaptive evolutionary extreme learning machine}},
volume = {36},
year = {2012}
}
@article{Cao2017,
abstract = {To access, purchase, authenticate, or subscribe to the full-text of this article, please visit this link: http://dx.doi.org/10.1007/s11704-016-5171-9 Byline: Lele Cao (1,2,3), Fuchun Sun (1,2), Hongbo Li (1,2), Wenbing Huang (1,2) Keywords: multi-kernel learning},
author = {Cao, Lele and Sun, Fuchun and Li, Hongbo and Huang, Wenbing},
doi = {10.1007/s11704-016-5171-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/cao2016.pdf:pdf},
isbn = {1170401651},
issn = {20952236},
journal = {Frontiers of Computer Science},
keywords = {extreme learning machine,feature fusion,multi-kernel learning,online learning,robot recognition},
number = {2},
pages = {276--289},
title = {{Advancing the incremental fusion of robotic sensory features using online multi-kernel extreme learning machine}},
volume = {11},
year = {2017}
}
@article{Castano2013,
abstract = {It is well-known that single-hidden-layer feedforward networks (SLFNs) with additive models are universal approximators. However the training of these models was slow until the birth of extreme learning machine (ELM) "Huang et al. Neurocomputing 70(1-3):489-501 (2006)" and its later improvements. Before ELM, the faster algorithms for efficiently training SLFNs were gradient based ones which need to be applied iteratively until a proper model is obtained. This slow convergence implies that SLFNs are not used as widely as they could be, even taking into consideration their overall good performances. The ELM allowed SLFNs to become a suitable option to classify a great number of patterns in a short time. Up to now, the hidden nodes were randomly initiated and tuned (though not in all approaches). This paper proposes a deterministic algorithm to initiate any hidden node with an additive activation function to be trained with ELM. Our algorithm uses the information retrieved from principal components analysis to fit the hidden nodes. This approach considerably decreases computational cost compared to later ELM improvements and overcomes their performance. {\textcopyright} 2012 Springer Science+Business Media New York.},
author = {Casta{\~{n}}o, A. and Fern{\'{a}}ndez-Navarro, F. and Herv{\'{a}}s-Mart{\'{i}}nez, C.},
doi = {10.1007/s11063-012-9253-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/castao2012.pdf:pdf},
isbn = {1370-4621},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Classification,Extreme learning machine,Principal component analysis},
number = {3},
pages = {377--392},
title = {{PCA-ELM: A robust and pruned extreme learning machine approach based on principal component analysis}},
volume = {37},
year = {2013}
}
@article{Castano2016,
author = {Casta{\~{n}}o, A. and Fern{\'{a}}ndez-Navarro, F. and Riccardi, Annalisa and Herv{\'{a}}s-Mart{\'{i}}nez, C.},
doi = {10.1007/s00521-015-1974-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/castao2015.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Linear discriminant analysis,Neural networks,Principal component analysis},
number = {6},
pages = {1749--1760},
title = {{Enforcement of the principal component analysis–extreme learning machine algorithm by linear discriminant analysis}},
volume = {27},
year = {2016}
}
@article{Cervellera2016,
abstract = {The traditional extreme learning machine (ELM) approach is based on a random assignment of the hidden weight values, while the linear coefficients of the output layer are determined analytically. This brief presents an analysis based on geometric properties of the sampling points used to assign the weight values, investigating the replacement of random generation of such values with low-discrepancy sequences (LDSs). Such sequences are a family of sampling methods commonly employed for numerical integration, yielding a more efficient covering of multidimensional sets with respect to random sequences, without the need for any computationally intensive procedure. In particular, we prove that the universal approximation property of the ELM is guaranteed when LDSs are employed, and how an efficient covering affects the convergence positively. Furthermore, since LDSs are generated deterministically, the results do not have a probabilistic nature. Simulation results confirm, in practice, the good theoretical properties given by the combination of ELM with LDSs.},
author = {Cervellera, Cristiano and Macci{\`{o}}, Danilo},
doi = {10.1109/TNNLS.2015.2424999},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/cervellera2016.pdf:pdf},
isbn = {2162-237X},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Discrepancy,Extreme learning machines (ELMs),Low-discrepancy sequences (LDSs),universal approximation},
number = {4},
pages = {891--896},
title = {{Low-Discrepancy Points for Deterministic Assignment of Hidden Weights in Extreme Learning Machines}},
volume = {27},
year = {2016}
}
@article{Chaturvedi2017,
abstract = {Subjectivity detection is a task of natural language processing that aims to remove 'factual' or 'neutral' content, i.e., objective text that does not contain any opinion, from online product reviews. Such a pre-processing step is crucial to increase the accuracy of sentiment analysis systems, as these are usually optimized for the binary classification task of distinguishing between positive and negative content. In this paper, we extend the extreme learning machine (ELM) paradigm to a novel framework that exploits the features of both Bayesian networks and fuzzy recurrent neural networks to perform subjectivity detection. In particular, Bayesian networks are used to build a network of connections among the hidden neurons of the conventional ELM configuration in order to capture dependencies in high-dimensional data. Next, a fuzzy recurrent neural network inherits the overall structure generated by the Bayesian networks to model temporal features in the predictor. Experimental results confirmed the ability of the proposed framework to deal with standard subjectivity detection problems and also proved its capacity to address portability across languages in translation tasks.},
author = {Chaturvedi, Iti and Ragusa, Edoardo and Gastaldo, Paolo and Zunino, Rodolfo and Cambria, Erik},
doi = {10.1016/j.jfranklin.2017.06.007},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/chaturvedi2017.pdf:pdf},
issn = {00160032},
journal = {Journal of the Franklin Institute},
keywords = {Bayesian Networks,Extreme Learning Machine,Sentiment Analysis,Subjectivity Detection},
publisher = {Elsevier Ltd},
title = {{Bayesian network based extreme learning machine for subjectivity detection}},
url = {http://dx.doi.org/10.1016/j.jfranklin.2017.06.007},
year = {2017}
}
@article{Chen2017,
abstract = {The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster. [ABSTRACT FROM AUTHOR]},
author = {Chen, Cen and Li, Kenli and Ouyang, Aijia and Tang, Zhuo and Li, Keqin},
doi = {10.1109/TSMC.2017.2690673},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/chen2017.pdf:pdf},
issn = {21682232},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
keywords = {Big data,GPGPU,deep learning (DL),flink,hierarchical extreme learning machine (H-ELM),parallel},
number = {10},
pages = {2740--2753},
title = {{GPU-accelerated parallel hierarchical extreme learning machine on flink for big data}},
volume = {47},
year = {2017}
}
@article{Chen2016,
abstract = {Extreme learning machine (ELM) randomly generates parameters of hidden nodes and then analytically determines the output weights with fast learning speed. The ill-posed problem of parameter matrix of hidden nodes directly causes unstable performance, and the automatical selection problem of the hidden nodes is critical to holding the high efficiency of ELM. Focusing on the ill-posed problem and the automatical selection problem of the hidden nodes, this paper proposes the variational Bayesian extreme learning machine (VBELM). First, the Bayesian probabilistic model is involved into ELM, where the Bayesian prior distribution can avoid the ill-posed problem of hidden node matrix. Then, the variational approximation inference is employed in the Bayesian model to compute the posterior distribution and the independent variational hyperparameters approximately, which can be used to select the hidden nodes automatically. Theoretical analysis and experimental results elucidate that VBELM has stabler performance with more compact architectures, which presents probabilistic predictions comparison with traditional point predictions, and it also provides the hyperparameter criterion for hidden node selection.},
author = {Chen, Yarui and Yang, Jucheng and Wang, Chao and Park, DongSun},
doi = {10.1007/s00521-014-1710-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/chen2014.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {and information technology,approximation {\'{a}} bayesian model,chen {\'{a}} j,extreme learning machine {\'{a}},school of computer science,variational,wang,y,yang,{\'{a}} c,{\'{a}} probabilistic prediction},
number = {1},
pages = {185--196},
title = {{Variational Bayesian extreme learning machine}},
url = {http://link.springer.com/10.1007/s00521-014-1710-1},
volume = {27},
year = {2016}
}
@article{Chen2015,
abstract = {Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning co-processor in 0.35um CMOS for motor intention decoding in brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3{\%} for movement type. The same co-processor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5{\%} with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by {\~{}}2X.},
archivePrefix = {arXiv},
arxivId = {1509.07450},
author = {Chen, Yi and Yao, Enyi and Basu, Arindam},
eprint = {1509.07450},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/chen2015.pdf:pdf},
isbn = {19324545 (ISSN)},
issn = {1932-4545},
journal = {IEEE Transactions on Biomedical Circuits and Systems},
pages = {14p.},
pmid = {26672048},
title = {{A 128 channel Extreme Learning Machine based Neural Decoder for Brain Machine Interfaces}},
url = {http://arxiv.org/abs/1509.07450},
year = {2015}
}
@article{Chen2016a,
author = {Chen, Zhen and Xiao, Xianyong and Li, Changsong and Zhang, Yin and Hu, Qingquan},
doi = {10.1007/s00521-015-1909-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/chen2015.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Classification,Cost-sensitive extreme learning machine,Electrical power system,Transient stability status prediction},
number = {2},
pages = {321--331},
publisher = {Springer London},
title = {{Real-time transient stability status prediction using cost-sensitive extreme learning machine}},
url = {http://dx.doi.org/10.1007/s00521-015-1909-9},
volume = {27},
year = {2016}
}
@article{Chen2013,
abstract = {This paper proposes a modified ELM algorithm that properly selects the input weights and biases before training the output weights of single-hidden layer feedforward neural networks with sigmoidal activation function and proves mathematically the hidden layer output matrix maintains full column rank. The modified ELM avoids the randomness compared with the ELM. The experimental results of both regression and classification problems show good performance of the modified ELM algorithm.},
author = {Chen, Zhixiang X. and Zhu, Houying Y. and Wang, Yuguang G.},
doi = {10.1007/s00521-012-0860-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/chen2012.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Feedforward neural networks,Moore-Penrose generalized inverse},
number = {3-4},
pages = {541--550},
title = {{A modified extreme learning machine with sigmoidal activation functions}},
volume = {22},
year = {2013}
}
@article{Chichester2014,
author = {Chichester, Eds and Wiley, U K},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/luo2014.pdf:pdf},
number = {4},
pages = {979--984},
title = {{Extreme learning machine ( ELM ) was proposed in [ 1 ] and [ 12 ], which has become a popular research topic in machine learning in recent years [ 11 ]. It is proved that single-hidden layer feedforward neural networks with arbitrary hidden parameters and}},
volume = {25},
year = {2014}
}
@article{Czarnecki2015,
abstract = {Machine learning methods are becoming more and more popular in the field of computer-aided drug design. The specific data characteristic, including sparse, binary representation as well as noisy, imbalanced datasets, presents a challenging binary classification problem. Currently, two of the most successful models in such tasks are the Support Vector Machine (SVM) and Random Forest (RF). In this paper, we introduce a Weighted Tanimoto Extreme Learning Machine (T-WELM), an extremely simple and fast method for predicting chemical compound biological activity and possibly other data with discrete, binary representation. We show some theoretical properties of the proposed model including the ability to learn arbitrary sets of examples. Further analysis shows numerous advantages of T-WELM over SVMs, RFs and traditional Extreme Learning Machines (ELM) in this particular task. Experiments performed on 40 large datasets of thousands of chemical compounds show that T-WELMs achieve much better classification results and are at the same time faster in terms of both training time and further classification than both ELM models and other state-of-the-art methods in the field.},
author = {Czarnecki, Wojciech Marian},
doi = {10.1109/MCI.2015.2437312},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/czarnecki2015.pdf:pdf},
issn = {1556603X},
journal = {IEEE Computational Intelligence Magazine},
number = {3},
pages = {19--29},
title = {{Weighted Tanimoto Extreme Learning Machine with Case Study in Drug Discovery}},
volume = {10},
year = {2015}
}
@article{Deng2010,
abstract = {This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pre- trained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-by- layer pre-training we “unroll” the generative model to form a deep auto-encoder, whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram, individual spectrogram segments predicted by their respective binary codes are combined using an overlap- and-add method. Experimental results on speech spectrogram coding demonstrate that the binary codes produce a log- spectral distortion that is approximately 2 dB lower than a sub- band vector quantization technique over the entire frequency range of wide-band speech.},
author = {Deng, Li and Seltzer, Michael and Yu, Dong and Acero, Alex and Mohamed, Abdel-Rahman and Hinton, Geoffrey},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/deng2010.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1692--1695},
title = {{Binary Coding of Speech Spectrograms Using a Deep Auto-encoder}},
year = {2010}
}
@article{Ding2017,
author = {Ding, Shifei and Guo, Lili and Hou, Yanlu},
doi = {10.1007/s00521-015-2170-y},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ding2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {CKELM,Convolutional neural network (CNN),Deep learning (DL),Extreme learning machine (ELM)},
number = {8},
pages = {1975--1984},
publisher = {Springer London},
title = {{Extreme learning machine with kernel model based on deep learning}},
volume = {28},
year = {2017}
}
@article{Ding2013,
author = {Ding, Shifei and Ma, Gang and Shi, Zhongzhi},
doi = {10.1007/s11063-013-9326-5},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/shifeiding2013.pdf:pdf},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {elm,minimal norm least-squares solution,r-rbf-nn,wrelm},
number = {3},
pages = {245--260},
title = {{A Rough RBF Neural Network Based on Weighted Regularized Extreme Learning Machine}},
url = {http://link.springer.com/10.1007/s11063-013-9326-5},
volume = {40},
year = {2013}
}
@article{Ding2013a,
author = {Ding, Shifei and Ma, Gang and Shi, Zhongzhi},
doi = {10.1007/s00521-013-1385-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ding2013{\_}3.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Affinity propagation (AP),Extreme learning machine (ELM),Radial basis function neural network (RBFNN)},
pages = {1--9},
title = {{A novel self-adaptive extreme learning machine based on affinity propagation for radial basis function neural network}},
year = {2013}
}
@article{Ding2014,
author = {Ding, Shifei and Xu, Xinzheng and Nie, Ru},
doi = {10.1007/s00521-013-1522-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ding2013{\_}2.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Classification,Extreme learning machine,Neural Networks,Regression,Single-hidden-layer feedforward networks},
number = {3-4},
pages = {549--556},
title = {{Extreme learning machine and its applications}},
volume = {25},
year = {2014}
}
@article{Ding2016,
author = {Ding, Shifei and Zhang, Jian and Xu, Xinzheng and Zhang, Yanan},
doi = {10.1007/s00521-015-1918-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ding2015.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Wavelet kernel function,Wavelet–mix kernel function,Weighted method},
number = {4},
pages = {1033--1040},
publisher = {Springer London},
title = {{A wavelet extreme learning machine}},
volume = {27},
year = {2016}
}
@article{Ding2015,
abstract = {Extreme learning machine (ELM) is a new learning algorithm for the single hidden layer feedforward neural networks. Compared with the conventional neural network learning algorithm it overcomes the slow training speed and over-fitting problems. ELM is based on empirical risk minimization theory and its learning process needs only a single iteration. The algorithm avoids multiple iterations and local minimization. It has been used in various fields and applications because of better generalization ability, robustness, and controllability and fast learning rate. In this paper, we make a review of ELM latest research progress about the algorithms, theory and applications. It first analyzes the theory and the algorithm ideas of ELM, then tracking describes the latest progress of ELM in recent years, including the model and specific applications of ELM, finally points out the research and development prospects of ELM in the future.},
author = {Ding, Shifei and Zhao, Han and Zhang, Yanan and Xu, Xinzheng and Nie, Ru},
doi = {10.1007/s10462-013-9405-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ding2013.pdf:pdf},
issn = {15737462},
journal = {Artificial Intelligence Review},
keywords = {Extreme learning machine (ELM),Least-squares,Local minimum,Over-fitting,Single-hidden layer feedforward neural networks (S},
number = {1},
pages = {103--115},
title = {{Extreme learning machine: algorithm, theory and applications}},
volume = {44},
year = {2015}
}
@article{Ding2017a,
abstract = {Mortality prediction for patients in intensive care unit (ICU) is necessary to prioritize resources as well as to help the medical staff to make decisions, and hence more accurate methods for identifying high risk patients are very important for improving clinical care. However, many existing approaches including some scoring systems now being used in the hospital are not good enough since they try to establish a global/average offline model, which may be unsuitable for a specific patient. Thus, a more robust and effective monitoring model adaptable to individual patients is needed. To establish a more personalized model, this study proposes a two-step framework, in which the first step is for clustering and while the second one is for mortality predication. A novel method combining just-in-time learning (JITL) and extreme learning machine (ELM), referred to JITL-ELM, is proposed for mortality prediction, which applies global optimization of variables and neighborhood of appropriate samples to build an accurate patient-specific model. In addition, a simplified JITL-ELM with less key physiological variables is developed. In the experiment, 4000 real clinical records of ICU patients are collected to validate the proposed algorithm, of which the AUC index is 0.8568, which is much better than the existing traditional global/average models, and furthermore the simplified JITL-ELM still performs well.},
author = {Ding, Yangyang and Wang, Youqing and Zhou, Donghua},
doi = {10.1016/j.neucom.2017.10.044},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/ding2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine (ELM),Intensive care unit (ICU),Just-in-time learning (JITL),Mortality prediction,Patient-specific model},
pages = {1--8},
publisher = {Elsevier B.V.},
title = {{Mortality prediction for ICU patients combining just-in-time learning and extreme learning machine}},
volume = {0},
year = {2017}
}
@article{Ebtehaj2016,
abstract = {{\textcopyright} 2016 Springer-Verlag London The minimum velocity required to prevent sediment deposition in open channels is examined in this study. The parameters affecting transport are first determined and then categorized into different dimensionless groups, including “movement,” “transport,” “sediment,” “transport mode,” and “flow resistance.” Six different models are presented to identify the effect of each of these parameters. The feed-forward neural network (FFNN) is used to predict the densimetric Froude number (Fr) and the extreme learning machine (ELM) algorithm is utilized to train it. The results of this algorithm are compared with back propagation (BP), genetic programming (GP) and existing sediment transport equations. The results indicate that FFNN-ELM produced better results than FNN-BP, GP and existing sediment transport methods in both training (RMSE = 0.26 and MARE = 0.052) and testing (RMSE = 0.121 and MARE = 0.023). Moreover, the performance of FFNN-ELM is examined for different pipe diameters.},
author = {Ebtehaj, Isa and Bonakdari, Hossein and Shamshirband, Shahaboddin},
doi = {10.1007/s00366-016-0446-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ebtehaj2016.pdf:pdf},
issn = {14355663},
journal = {Engineering with Computers},
keywords = {Bed load,Extreme learning machines (ELM),Limit of deposition,Sediment transport,Storm water},
number = {4},
pages = {691--704},
publisher = {Springer London},
title = {{Extreme learning machine assessment for estimating sediment transport in open channels}},
volume = {32},
year = {2016}
}
@article{Ertugrul2017,
abstract = {? 2016 The Natural Computing Applications ForumIn this study, a novel approach was proposed based on extreme learning machine (ELM) for developing correlations in order to calculate higher heating values (HHVs, kj/kg) of waste frying oils from their physical properties such as density (?, kg/m3) and kinematic viscosity (v, mm2/s) values. These values can easily be determined by using laboratory equipment. For developing the correlations, an experimental dataset from the literature covering 35 samples was collected to be employed in the training and validation steps. The obtained optimum parameters of artificial neural network in the training stage by ELM were employed to develop new correlations. The HHVs calculated by using density-based correlation (HHV = 50823.183 ? 12.34095?) showed the mean absolute and relative errors of 145.8048 kJ/kg and 0.3695 {\%}, respectively. In the case of the viscosity-based correlation (HHV = 40172.85 ? 17.93615v), they were found as 129.04 kJ/kg and 0.327 {\%}, respectively. Additionally, new correlations were performed better than those available in the literature and those obtained by other machine learning methods; therefore, it is highly suggested that the proposed approach can be used for developing new correlations.},
author = {Ertuğrul, {\"{O}}mer F. and Altun, Şehmus},
doi = {10.1007/s00521-016-2233-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/erturul2016.pdf:pdf},
isbn = {0052101622},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Higher heating value,Mathematical modeling,Waste frying oils},
number = {11},
pages = {3145--3152},
title = {{Developing correlations by extreme learning machine for calculating higher heating values of waste frying oils from their physical properties}},
volume = {28},
year = {2017}
}
@article{Fan2014,
abstract = {Compared with traditional learning methods such as the back propagation (BP) method, extreme learning machine provides much faster learning speed and needs less human intervention, and thus has been widely used. In this paper we combine the L (1/2) regularization method with extreme learning machine to prune extreme learning machine. A variable learning coefficient is employed to prevent too large a learning increment. A numerical experiment demonstrates that a network pruned L (1/2) regularization has fewer hidden nodes but provides better performance than both the original network and the network pruned by L (2) regularization.},
author = {Fan, Ye-tian and Wu, Wei and Yang, Wen-yu and Fan, Qin-wei and Wang, Jian},
doi = {10.1631/jzus.C1300197},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/fan2014.pdf:pdf},
isbn = {1869-1951},
issn = {1869-1951},
journal = {Journal of Zhejiang University SCIENCE C},
keywords = {10,1631,2 regularizer,c1300197,doi,elm,extreme learning machine,jzus,l 1,network pruning},
number = {2},
pages = {119--125},
title = {{A pruning algorithm with L 1/2 regularizer for extreme learning machine}},
url = {http://link.springer.com/10.1631/jzus.C1300197},
volume = {15},
year = {2014}
}
@article{Fu2016,
abstract = {
Damage location detection has direct relationship with the field of aerospace structure as the detection system can inspect any exterior damage that may affect the operations of the equipment. In the literature, several kinds of learning algorithms have been applied in this field to construct the detection system and some of them gave good results. However, most learning algorithms are time-consuming due to their computational complexity so that the real-time requirement in many practical applications cannot be fulfilled. Kernel extreme learning machine (kernel ELM) is a learning algorithm, which has good prediction performance while maintaining extremely fast learning speed. Kernel ELM is originally applied to this research to predict the location of impact event on a clamped aluminum plate that simulates the shell of aerospace structures. The results were compared with several previous work, including support vector machine (SVM), and conventional back-propagation neural networks (BPNN). The comparison result reveals the effectiveness of kernel ELM for impact detection, showing that kernel ELM has comparable accuracy to SVM but much faster speed on current application than SVM and BPNN.
},
author = {Fu, Heming and Vong, Chi-Man and Wong, Pak-Kin and Yang, Zhixin},
doi = {10.1007/s00521-014-1568-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/fu2014.pdf:pdf},
isbn = {0941-0643,14333058},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {damage location detection {\'{a}},kernel elm {\'{a}}},
number = {1},
pages = {121--130},
title = {{Fast detection of impact location using kernel extreme learning machine}},
url = {http://link.springer.com/10.1007/s00521-014-1568-2},
volume = {27},
year = {2016}
}
@article{Geng2017,
abstract = {The control of product quality of complex chemical processes strictly depends on the measure of the key process variables. However, the online measure device is extremely expensive, and these devices are hard to protect. Meanwhile, there is a delay for these online measure devices. Therefore, the soft sensor technology plays a vital role in measuring the key process variables. Extreme Learning Machine (ELM) is an efficient and simple single layer feed-forward neural networks (SLFNs) to building an exact soft sensor model. However, unsuitable selected hidden nodes and random parameters will greatly affect the performance of the ELM. Therefore, this paper proposes a novel Self-Organizing Extreme Learning Machine (SOELM) algorithm constructed by the biological neuron-glia interaction principle to solve the issue of the ELM. Firstly, the weights between input layer nodes and the CNS are tuned iteratively by the Hebbian learning rule. Then the network structure is adjusted self-organizing by Mutual Information (MI) among different structures of networks. Secondly, the weights between the CNS and output layer nodes are obtained by the ELM. The experimental results based on different UCI data sets prove that the SOELM has a better generalization capability and stability than that of the ELM. Moreover, our proposed method is developed as a soft sensor model for accurately predicting the key variables of the Purified Terephthalic Acid (PTA) process.},
author = {Geng, Zhiqiang and Dong, Jungen and Chen, Jie and Han, Yongming},
doi = {10.1016/j.engappai.2017.03.011},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/geng2017.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Complicated chemical processes,Extreme Learning Machine,Hebbian learning rule,Mutual Information,Self-Organizing,Soft sensor},
number = {December 2016},
pages = {38--50},
publisher = {Elsevier Ltd},
title = {{A new Self-Organizing Extreme Learning Machine soft sensor model and its applications in complicated chemical processes}},
url = {http://dx.doi.org/10.1016/j.engappai.2017.03.011},
volume = {62},
year = {2017}
}
@article{Ghimire2016,
author = {Ghimire, Deepak and Lee, Joonwhoan},
doi = {10.1007/s11042-015-2839-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ghimire2015.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Cast shadow detection,Co-training,Foreground segmentation,Online sequential extreme learning machine},
number = {18},
pages = {11181--11197},
title = {{Online sequential extreme learning machine-based co-training for dynamic moving cast shadow detection}},
volume = {75},
year = {2016}
}
@article{Glauner2015,
abstract = {This report describes the difficulties of training neural networks and in particular deep neural networks. It then provides a literature review of training methods for deep neural networks, with a focus on pre-training. It focuses on Deep Belief Networks composed of Restricted Boltzmann Machines and Stacked Autoencoders and provides an outreach on further and alternative approaches. It also includes related practical recommendations from the literature on training them. In the second part, initial experiments using some of the covered methods are performed on two databases. In particular, experiments are performed on the MNIST hand-written digit dataset and on facial emotion data from a Kaggle competition. The results are discussed in the context of results reported in other research papers. An error rate lower than the best contribution to the Kaggle competition is achieved using an optimized Stacked Autoencoder.},
archivePrefix = {arXiv},
arxivId = {1504.06825},
author = {Glauner, Patrick O.},
eprint = {1504.06825},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/read-later-mixed/glauner2015.pdf:pdf},
number = {April},
pages = {1--56},
title = {{Comparison of Training Methods for Deep Neural Networks}},
url = {http://arxiv.org/abs/1504.06825},
year = {2015}
}
@article{Golestaneh2017,
abstract = {Incorporating the time-frequency localization properties of wavelets and the learning abilities of neural network (NN), the approximate reasoning characteristics of fuzzy inference system and the advantages of ELM (one-pass learning and good generalization performance at extremely fast learning speed) can exhibit their characteristics to reveal an effective solution in many applications. Following that, this paper presents a novel structure called fuzzy wavelet extreme learning machine (FW-ELM). The main objectives of FW-ELM are to significantly reduce the network complexity by reducing the number of linear learning parameters, and to decrease the sensitivity to random initialization procedure while the acceptable accuracy and generalization performances are preserved. In the proposed structure, each fuzzy rule corresponds to a sub-wavelet neural network and consists of wavelets with different dilations and translations. In this model, in order to achieve a balance between network complexity and performance accuracy, in the THEN-part of each fuzzy rule, one coefficient is considered for each two inputs. In this work, first, the equivalence of an FW model and an SLFN is proved and then ELM can be directly applied to the model. All free parameters of membership functions and wavelet coefficients are generated randomly, and only the output weights are determined analytically using a one-pass learning method. To evaluate FW-ELM, it is compared with popular fuzzy models like OS-Fuzzy-ELM, Simpl{\_}eTS, ANFIS and several other relevant algorithms such as ELM, BP and SVR on various benchmark datasets. Simulation results demonstrate the remarkable efficiency resulting from the proposed approach. Performance accuracy of FW-ELM is shown to be comparable with OS-Fuzzy-ELM and better than the rest of the well-known methods.},
author = {Golestaneh, Pegah and Zekri, Maryam and Sheikholeslam, Farid},
doi = {10.1016/j.fss.2017.12.006},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/golestaneh2017.pdf:pdf},
isbn = {8415683111},
issn = {01650114},
journal = {Fuzzy Sets and Systems},
keywords = {Extreme learning machine (ELM),Fuzzy wavelet (FW) model,Wavelet neural network (WNN)},
pages = {1--19},
publisher = {Elsevier B.V.},
title = {{Fuzzy wavelet extreme learning machine}},
url = {https://doi.org/10.1016/j.fss.2017.12.006},
volume = {1},
year = {2017}
}
@article{Guendil2017,
author = {Guendil, Zied and Lachiri, Zied and Maaoui, Choubeila},
doi = {10.1007/s13735-017-0128-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/guendil2017.pdf:pdf},
issn = {2192662X},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {DEAP,Extreme Learning Machine,Physiological data,VA quadrant,VAD space},
number = {3},
pages = {251--261},
publisher = {Springer London},
title = {{Computational framework for emotional VAD prediction using regularized Extreme Learning Machine}},
volume = {6},
year = {2017}
}
@article{Guo2017,
abstract = {This paper considers the two-stage capacitated facility location problem (TSCFLP) in which products manufactured in plants are delivered to customers via storage depots. Customer demands are satisfied subject to limited plant production and limited depot storage capacity. The objective is to determine the locations of plants and depots in order to minimize the total cost including the fixed cost and transportation cost. However, the problem is known to be NP-hard. A practicable exact algorithm is impossible to be developed. In order to solve large-sized problems encountered in the practical decision process, an efficient alternative approximate method becomes more valuable. This paper aims to propose a hybrid evolutionary algorithm framework with machine learning fitness approximation for delivering better solutions in a reasonable amount of computational time. In our study, genetic operators are adopted to perform the search process and a local search strategy is used to refine the best solution found in the population. To avoid the expensive consumption of computational time during the fitness evaluating process, the framework uses extreme machine learning to approximate the fitness of most individuals. Moreover, two heuristics based on the characteristics of the problem is incorporated to generate a good initial population. Computational experiments are performed on two sets of test instances from the recent literature. The performance of the proposed algorithm is evaluated and analyzed. Compared with other algorithms in the literature, the proposed algorithm can find the optimal or near-optimal solutions in a reasonable amount of computational time. By employing the proposed algorithm, facilities can be positioned more efficiently, which means the fixed cost and the transportation cost can be decreased significantly, and organizations can enhance competitiveness by using the optimized facility location scheme.},
archivePrefix = {arXiv},
arxivId = {1605.06722},
author = {Guo, Peng and Cheng, Wenming and Wang, Yi},
doi = {10.1016/j.eswa.2016.11.025},
eprint = {1605.06722},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/guo2017.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Evolutionary algorithm,Extreme machine learning,Facility location,Fitness approximation,Local search},
pages = {57--68},
publisher = {Elsevier Ltd},
title = {{Hybrid evolutionary algorithm with extreme machine learning fitness function evaluation for two-stage capacitated facility location problems}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.11.025},
volume = {71},
year = {2017}
}
@article{Guo2016,
author = {Guo, Wei and Xu, Tao and Lu, Zonglei},
doi = {10.1007/s00521-015-1903-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/guo2015.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Chaotic time series prediction,Differential evolution,Efficient extreme learning machine,Integrated parameter selection,Reduced complete orthogonal decomposition},
number = {4},
pages = {883--898},
publisher = {Springer London},
title = {{An integrated chaotic time series prediction model based on efficient extreme learning machine and differential evolution}},
url = {http://dx.doi.org/10.1007/s00521-015-1903-2},
volume = {27},
year = {2016}
}
@article{Guo2017a,
abstract = {{\textcopyright} 2016 The Natural Computing Applications Forum An M-estimator-based online sequential extreme learning machine (M-OSELM) is proposed to predict chaotic time series with outliers. The M-OSELM develops from the online sequential extreme learning machine (OSELM) algorithm and retains the same excellent sequential learning ability as OSELM, but replaces the conventional least-squares cost function with a robust M-estimator-based cost function to enhance the robustness of the model to outliers. By minimizing the M-estimator-based cost function, the possible outliers are prevented from entering the model's output weights updating scheme. Meanwhile, in the sequential learning process of M-OSELM, a sequential parameter estimation approach based on error sliding window is introduced to estimate the threshold value of the M-estimator function for online outlier detection. Thanks to the built-in median operation and sliding window strategy, this approach is efficient to provide a stable estimator continuously without high computational costs, and then the potential outliers can be effectively detected. Simulation results show that the proposed M-OSELM has an excellent immunity to outliers and can always achieve better performance than its counterparts for prediction of chaotic time series when the training dataset contains outliers, ensuring at the same time all benefits of an online sequential approach.},
author = {Guo, Wei and Xu, Tao and Tang, Keming},
doi = {10.1007/s00521-016-2301-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/guo2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Chaotic time series prediction,Extreme learning machine,M-estimator,Online sequential learning,Outliers},
number = {12},
pages = {4093--4110},
publisher = {Springer London},
title = {{M-estimator-based online sequential extreme learning machine for predicting chaotic time series with outliers}},
volume = {28},
year = {2017}
}
@article{Han2015,
author = {Han, Dong Hong and Zhang, Xin and Wang, Guo Ren},
doi = {10.1007/s11390-015-1566-6},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/han2015.pdf:pdf},
isbn = {1139001515},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {classification,concept drift,distributed computing,extreme learning machine,uncertain data stream},
number = {4},
pages = {874--887},
title = {{Classifying Uncertain and Evolving Data Streams with Distributed Extreme Learning Machine}},
volume = {30},
year = {2015}
}
@article{Han2017,
abstract = {How to determine the network structure is an open problem in extreme learning machine (ELM). Error minimized extreme learning machine (EM-ELM) is a simple and efficient approach to determine the number of hidden nodes. However, similar to other constructive ELM, EM-ELM lays much emphasis on the convergence accuracy, which may obtain a single-hidden-layer feedforward neural networks (SLFN) with good convergence performance but bad condition. In this paper, an effective approach based on error minimized ELM and particle swarm optimization (PSO) is proposed to adaptively determine the structure of SLFN for regression problem. In the new method, to establish a compact and well-conditioning SLFN, the hidden node optimized by PSO is added to the SLFN one by one. Moreover, not only the regression accuracy but also the condition value of the hidden output matrix of the network is considered in the optimization process. Experiment results on various regression problems verify that the proposed algorithm achieves better generalization performance with fewer hidden nodes than other constructive ELM.},
author = {Han, Fei and Zhao, Min Ru and Zhang, Jian Ming and Ling, Qing Hua},
doi = {10.1016/j.neucom.2016.09.092},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/han2016.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Condition value,Extreme learning machine,Generalization performance,Network structure,Particle swarm optimization},
number = {January},
pages = {133--142},
publisher = {Elsevier},
title = {{An improved incremental constructive single-hidden-layer feedforward networks for extreme learning machine based on particle swarm optimization}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.09.092},
volume = {228},
year = {2017}
}
@article{Haykin1998,
abstract = {This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.},
author = {Haykin, Simon},
doi = {10.1109/79.487040},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/haykin1998.pdf:pdf},
isbn = {0023527617},
issn = {10535888},
journal = {Convergence},
pages = {1--16},
pmid = {940},
title = {{Neural Networks}},
year = {1998}
}
@book{Hu2017,
abstract = {Diagnosis of benign and malignant microcalcifications in digital mammography using Computer-aided Diagnosis (CAD) system is critical for the early diagnosis of breast cancer. Wavelet transform based diagnosis methods are effective to accomplish this task, but limited by representing the correlation within each wavelet scale, these methods neglect the correlation between wavelet scales. In this paper, we apply the hidden Markov tree model of dual-tree complex wavelet transform (DTCWT-HMT) for microcalcification diagnosis in digital mammography. DTCWT-HMT can effectively capture the correlation between different wavelet coefficients and model the statistical dependencies and non-Gaussian statistics of real signals, is used to characterize microcalcifications for the diagnosis of benign and malignant cases. The combined features which consist of the DTCWT-HMT features and the DTCWT features are optimized by genetic algorithm (GA). Extreme learning machine (ELM), an efficient learning theory is employed as the classifier to diagnose the benign and malignant microcalcifications. The validity of the proposed method is evaluated on the Nijmegen, MIAS and DDSM datasets using area under curve (AUC) of receiver operating characteristic (ROC). The AUC values of 0.9856, 0.9941 and 0.9168 of the proposed method are achieved on Nijmegen, MIAS and DDSM, respectively. We compare the proposed method with state-of-the-art diagnosis methods, and the experimental results show the effectiveness of the proposed method for the diagnosis of the benign and malignant microcalcifications in mammograms in terms of the accuracy and stability.},
author = {Hu, Kai and Yang, Wei and Gao, Xieping},
booktitle = {Expert Systems with Applications},
doi = {10.1016/j.eswa.2017.05.062},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/hu2017.pdf:pdf},
isbn = {8673158292201},
issn = {09574174},
keywords = {Digital mammography,Dual-tree complex wavelet transform,Extreme learning machine,Feature extraction,Hidden Markov tree model,Microcalcification diagnosis},
pages = {1339--1351},
publisher = {Elsevier Ltd},
title = {{Microcalcification diagnosis in digital mammography using extreme learning machine based on hidden Markov tree model of dual-tree complex wavelet transform}},
url = {http://dx.doi.org/10.1016/j.eswa.2017.05.062},
volume = {86},
year = {2017}
}
@article{Huang2015,
abstract = {Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives.},
archivePrefix = {arXiv},
arxivId = {1412.3684},
author = {Huang, Gao and Huang, Guang Bin and Song, Shiji and You, Keyou},
doi = {10.1016/j.neunet.2014.10.001},
eprint = {1412.3684},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/survey-papers/627defcd36477b6d39ec5fc4d02a479e7c37.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Classification,Clustering,Extreme learning machine,Feature learning,Regression},
pages = {32--48},
pmid = {25462632},
publisher = {Elsevier Ltd},
title = {{Trends in extreme learning machines: A review}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.10.001},
volume = {61},
year = {2015}
}
@article{Huang2015a,
abstract = {Extreme learning machine (ELM), which was originally proposed for "generalized" single-hidden layer feedforward neural networks (SLFNs), provides efficient unified learning solutions for the applications of feature learning, clustering, regression and classification. Different from the common understanding and tenet that hidden neurons of neural networks need to be iteratively adjusted during training stage, ELM theories show that hidden neurons are important but need not be iteratively tuned. In fact, all the parameters of hidden nodes can be independent of training samples and randomly generated according to any continuous probability distribution. And the obtained ELM networks satisfy universal approximation and classification capability. The fully connected ELM architecture has been extensively studied. However, ELM with local connections has not attracted much research attention yet. This paper studies the general architecture of locally connected ELM, showing that: 1) ELM theories are naturally valid for local connections, thus introducing local receptive fields to the input layer; 2) each hidden node in ELM can be a combination of several hidden nodes (a subnetwork), which is also consistent with ELM theories. ELM theories may shed a light on the research of different local receptive fields including true biological receptive fields of which the exact shapes and formula may be unknown to human beings. As a specific example of such general architectures, random convolutional nodes and a pooling structure are implemented in this paper. Experimental results on the NORB dataset, a benchmark for object recognition, show that compared with conventional deep learning solutions, the proposed local receptive fields based ELM (ELM-LRF) reduces the error rate from 6.5{\%} to 2.7{\%} and increases the learning speed up to 200 times.},
author = {Huang, Guang Bin and Bai, Zuo and Kasun, Liyanaarachchi Lekamalage Chamara and Vong, Chi Man},
doi = {10.1109/MCI.2015.2405316},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/huang2015 (magazine).pdf:pdf},
isbn = {1556-603X},
issn = {1556603X},
journal = {IEEE Computational Intelligence Magazine},
number = {2},
pages = {18--29},
title = {{Local receptive fields based extreme learning machine}},
volume = {10},
year = {2015}
}
@article{Huang2006,
abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to "Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks", Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25-29 July, 2004. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1311.4555},
author = {Huang, Guang Bin and Zhu, Qin Yu and Siew, Chee Kheong},
doi = {10.1016/j.neucom.2005.12.126},
eprint = {1311.4555},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/huang2006.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Back-propagation algorithm,Extreme learning machine,Feedforward neural networks,Random node,Real-time learning,Support vector machine},
number = {1-3},
pages = {489--501},
pmid = {16856651},
title = {{Extreme learning machine: Theory and applications}},
volume = {70},
year = {2006}
}
@article{Huang2015b,
author = {Huang, Guang-bin},
doi = {10.1007/s12559-015-9333-0.The},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/uzair2016.pdf:pdf},
journal = {Tc},
keywords = {-extreme},
number = {x},
pages = {2--3},
title = {{Extreme learning machine}},
year = {2015}
}
@article{Huang2012,
abstract = {Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the "generalized" single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.},
author = {Huang, Guang-Bin and Zhou, Hongming and Ding, Xiaojian and Zhang, Rui},
doi = {10.1109/TSMCB.2011.2168604},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/huang2012.pdf:pdf},
isbn = {1083-4419},
issn = {1941-0492},
journal = {IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics},
number = {2},
pages = {513--29},
pmid = {21984515},
title = {{Extreme learning machine for regression and multiclass classification}},
volume = {42},
year = {2012}
}
@article{Huang2004,
abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradient- based learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single- hidden layer feedforward neural networks (SLFNs) which ran- domly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on real- world benchmarking function approximation and classification problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.},
author = {Huang, Guang-bin and Zhu, Qin-yu and Siew, Chee-kheong},
doi = {10.1109/IJCNN.2004.1380068},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/huang2004{\_}ELM{\_}(first{\_}appereance).pdf:pdf},
isbn = {0780383591},
issn = {1098-7576},
journal = {IEEE International Joint Conference on Neural Networks},
pages = {985--990},
title = {{Extreme Learning Machine : A New Learning Scheme of Feedforward Neural Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1380068},
volume = {2},
year = {2004}
}
@article{Huang2016,
abstract = {—This paper proposes a computationally efficient method for traffic sign recognition (TSR). This proposed method consists of two modules: 1) extraction of histogram of oriented gradient variant (HOGv) feature and 2) a single classifier trained by extreme learning machine (ELM) algorithm. The presented HOGv feature keeps a good balance between redundancy and local details such that it can represent distinctive shapes bet-ter. The classifier is a single-hidden-layer feedforward network. Based on ELM algorithm, the connection between input and hidden layers realizes the random feature mapping while only the weights between hidden and output layers are trained. As a result, layer-by-layer tuning is not required. Meanwhile, the norm of output weights is included in the cost function. Therefore, the ELM-based classifier can achieve an optimal and generalized solution for multiclass TSR. Furthermore, it can balance the recognition accuracy and computational cost. Three datasets, including the German TSR benchmark dataset, the Belgium traffic sign classification dataset and the revised mapping and assessing the state of traffic infrastructure (revised MASTIF) dataset, are used to evaluate this proposed method. Experimental results have shown that this proposed method obtains not only high recognition accuracy but also extremely high computational efficiency in both training and recognition processes in these three datasets. Index Terms—Extreme learning machine (ELM), HOG vari-ant (HOGv), traffic sign recognition (TSR).},
author = {Huang, Zhiyong and Yu, Yuanlong and Gu, Jason and Liu, Huaping},
doi = {10.1109/TCYB.2016.2533424},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/huang2016.pdf:pdf},
isbn = {9781479958252},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
pages = {1--14},
title = {{An Efficient Method for Traffic Sign Recognition Based on Extreme Learning Machine}},
year = {2016}
}
@article{Hussain2017,
abstract = {OAPA In wireless telephony and audio data mining applications, it is desirable that noise suppression can be made robust against changing noise conditions and operate in real time (or faster). The learning effectiveness and speed of artificial neural networks are therefore critical factors in applications for speech enhancement tasks. To address these issues, we present an extreme learning machine (ELM) framework, aimed at the effective and fast removal of background noise from a single-channel speech signal, based on a set of randomly chosen hidden units and analytically determined output weights. Because feature learning with shallow ELM may not be effective for natural signals, such as speech, even with a large number of hidden nodes, hierarchical ELM (H-ELM) architectures are deployed by leveraging sparse auto-encoders. In this manner, we not only keep all the advantages of deep models in approximating complicated functions and maintaining strong regression capabilities, but we also overcome the cumbersome and time-consuming features of both greedy layer-wise pre-training and back-propagation (BP) based fine tuning schemes, which are typically adopted for training deep neural architectures. The proposed ELM framework was evaluated on the Aurora {\&} {\#}x2013;4 speech databases. The Aurora4 task provides relatively limited training data, and test speech data corrupted with both additive noise and convolutive distortions for matched and mismatched channels and signal-to-noise ratio (SNR) conditions. In addition, the task includes a subset of testing data involving noise types and SNR levels that are not seen in the training data. The experimental results indicate that when the amount of training data is limited, both ELM and H-ELM based speech enhancement techniques consistently outperform the conventional BP-based shallow and deep learning algorithms, in terms of standardized objective evaluations, under various testing conditions.},
author = {Hussain, T. and Siniscalchi, S.M. and Lee, C. and Wang, S. and Tsao, Y. and Liao, W.},
doi = {10.1109/ACCESS.2017.2766675},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/hussain2017.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Artificial Neural Networks,Extreme Learning Machine,Hierarchical Extreme Learning Machines,Speech Enhancement},
pages = {1--13},
title = {{Experimental Study on Extreme Learning Machine Applications for Speech Enhancement}},
volume = {5},
year = {2017}
}
@article{Ibrahim2018,
author = {Ibrahim, Wisam and Abadeh, Mohammad Saniee},
doi = {10.1007/s00521-018-3346-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ibrahim2018.pdf:pdf},
isbn = {0123456789},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {Protein fold recognition,Deep learning,Extreme lea,deep learning {\'{a}} extreme,discriminant analysis,learning machine {\'{a}} linear,protein fold recognition {\'{a}}},
publisher = {Springer London},
title = {{Protein fold recognition using Deep Kernelized Extreme Learning Machine and linear discriminant analysis}},
url = {http://link.springer.com/10.1007/s00521-018-3346-z},
volume = {0123456789},
year = {2018}
}
@article{Iosifidis2016,
abstract = {In this paper, we propose a novel extension of the extreme learning machine (ELM) algorithm for single-hidden layer feedforward neural network training that is able to incorporate subspace learning (SL) criteria on the optimization process followed for the calculation of the network's output weights. The proposed graph embedded ELM (GEELM) algorithm is able to naturally exploit both intrinsic and penalty SL criteria that have been (or will be) designed under the graph embedding framework. In addition, we extend the proposed GEELM algorithm in order to be able to exploit SL criteria in arbitrary (even infinite) dimensional ELM spaces. We evaluate the proposed approach on eight standard classification problems and nine publicly available datasets designed for three problems related to human behavior analysis, i.e., the recognition of human face, facial expression, and activity. Experimental results denote the effectiveness of the proposed approach, since it outperforms other ELM-based classification schemes in all the cases.},
author = {Iosifidis, Alexandros and Tefas, Anastasios and Pitas, Ioannis},
doi = {10.1109/TCYB.2015.2401973},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/iosifidis2016.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Extreme learning machine (ELM),Facial image classification,Graph embedding,Human action recognition},
number = {1},
pages = {311--324},
pmid = {25751883},
title = {{Graph embedded extreme learning machine}},
volume = {46},
year = {2016}
}
@article{Janakiraman2015,
abstract = {Homogeneous charge compression ignition (HCCI) is a futuristic automotive engine technology that can significantly improve fuel economy and reduce emissions. HCCI engine operation is constrained by combustion instabilities, such as knock, ringing, misfires, high-variability combustion, and so on, and it becomes important to identify the operating envelope defined by these constraints for use in engine diagnostics and controller design. HCCI combustion is dominated by complex nonlinear dynamics, and a first-principle-based dynamic modeling of the operating envelope becomes intractable. In this paper, a machine learning approach is presented to identify the stable operating envelope of HCCI combustion, by learning directly from the experimental data. Stability is defined using thresholds on combustion features obtained from engine in-cylinder pressure measurements. This paper considers instabilities arising from engine misfire and high-variability combustion. A gasoline HCCI engine is used for generating stable and unstable data observations. Owing to an imbalance in class proportions in the data set, the models are developed both based on resampling the data set (by undersampling and oversampling) and based on a cost-sensitive learning method (by overweighting the minority class relative to the majority class observations). Support vector machines (SVMs) and recently developed extreme learning machines (ELM) are utilized for developing dynamic classifiers. The results compared against linear classification methods show that cost-sensitive nonlinear ELM and SVM classification algorithms are well suited for the problem. However, the SVM envelope model requires about 80{\%} more parameters for an accuracy improvement of 3{\%} compared with the ELM envelope model indicating that ELM models may be computationally suitable for the engine application. The proposed modeling approach shows that HCCI engine misfires and high-variability combustion can be predicted ahead of time, gi- en the present values of available sensor measurements, making the models suitable for engine diagnostics and control applications.},
author = {Janakiraman, V M and Nguyen, XuanLong and Sterniak, J and Assanis, D},
doi = {10.1109/TNNLS.2014.2311466},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/read-later-mixed/janakiraman2015.pdf:pdf},
isbn = {2162-237X},
issn = {2162-237X},
journal = {Neural Networks and Learning Systems, IEEE Transactions on},
keywords = {control system synthesis,fuel economy,internal com},
number = {1},
pages = {98--112},
title = {{Identification of the Dynamic Operating Envelope of HCCI Engines Using Class Imbalance Learning}},
volume = {26},
year = {2015}
}
@article{Javed2014,
author = {Javed, Kamran},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/javed2015.pdf:pdf},
keywords = {Prognostics ; Data-driven ; Extreme learning Machi},
number = {2014BESA2021},
pages = {1--14},
title = {{A robust and reliable data-driven prognostics approach based on Extreme Learning Machine and Fuzzy Clustering}},
url = {https://tel.archives-ouvertes.fr/tel-01126861},
year = {2014}
}
@article{Jia2016,
author = {Jia, Lu and Li, Ming and Zhang, Peng and Wu, Yan},
doi = {10.1109/TGRS.2016.2578438},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/jia2016.pdf:pdf},
issn = {01962892},
number = {10},
pages = {1--14},
title = {{SAR Image Change Detection Based on Correlation Kernel and Multistage Extreme Learning Machine}},
volume = {54},
year = {2016}
}
@article{Jiang2017,
abstract = {Extreme learning machine (ELM) is extended from the generalized single hidden layer feedforward networks where the input weights of the hidden layer nodes can be assigned randomly. It has been widely used for its much faster learning speed and less manual works. Considering the field of multi-label text classification, in this paper, we propose an ELM based algorithm combined with L21-norm minimization of the output weights matrix called L21-norm Minimization ELM, which not only fully inherits the merits of ELM but also facilitates group sparsity and reduces complexity of the learning model. Extensive experiments on several benchmark data sets show that our proposed algorithm can obtain superior performances compared with other common multi-label classification algorithms.},
author = {Jiang, Mingchu and Pan, Zhisong and Li, Na},
doi = {10.1016/j.neucom.2016.04.069},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/jiang2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,L21-norm minimization,Multi-label learning,Text categorization},
pages = {4--10},
title = {{Multi-label text categorization using L21-norm minimization extreme learning machine}},
volume = {261},
year = {2017}
}
@article{Jiang2016,
abstract = {Wi-Fi-based indoor localization with high capability and feasibility needs to implement lifelong online learning mechanism. However, the characteristic of Wi-Fi is wide variability, which lies in not only the fluctuation of signal strength value, but also the increase or decrease in the number of access points (APs). The traditional algorithms are effective for signal fluctuation, but cannot handle the dimension-changing problem of features caused by increase and decrease in APs' number. To solve this problem, we propose a Feature Adaptive Online Sequential Extreme Learning Machine (FA-OSELM) algorithm. It can transfer the original model to a new one with a small number of data with new features, so as to make the new model suitable for the new feature dimension. The experiments show that the FA-OSELM can get higher accuracy with a small amount of new data, and it is an effective method to make lifelong indoor localization practical. {\textcopyright} 2014, The Natural Computing Applications Forum.},
author = {Jiang, Xinlong and Liu, Junfa and Chen, Yiqiang and Liu, Dingjun and Gu, Yang and Chen, Zhenyu},
doi = {10.1007/s00521-014-1714-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/jiang2014.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Feature adaptive,Indoor localization,Lifelong,Online Sequential Extreme Learning Machine (OS-ELM},
number = {1},
pages = {215--225},
title = {{Feature Adaptive Online Sequential Extreme Learning Machine for lifelong indoor localization}},
volume = {27},
year = {2016}
}
@article{Jin2017,
abstract = {OAPA In online sequential applications, a machine learning model needs to have a self-updating ability to handle the situation, that the training set is changing. Conventional incremental extreme learning machine (ELM) and online sequential ELM are usually achieved in two approaches: directly updating the output weight and recursively computing the left pseudo inverse of the hidden layer output matrix. In this paper, we develop a novel solution for incremental and decremental ELM, via recursively updating and downdating the generalized inverse of the hidden layer output matrix. By preserving the global optimality and best generalization performance, our approach implements node incremental ELM (N-IELM) and sample incremental ELM (S-IELM) in a universal form, and overcomes the problem of self-starting and numerical instability in the conventional online sequential ELM. We also propose sample decremental ELM (S-DELM), which is the first decremental version of ELM. The experiments on regression and classification problems with real-world datasets demonstrate the feasibility and effectiveness of the proposed algorithms with encouraging performances.},
author = {Jin, B. and Jing, Z. and Zhao, H.},
doi = {10.1109/ACCESS.2017.2758645},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/jin2017.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Approximation error,Classification algorithms,Computational complexity,Computational modeling,Extreme learning machine,Machine learning algorithms,Neurons,Training,decremental ELM,generalized inverse,incremental ELM,online sequential ELM},
number = {c},
pages = {1--14},
title = {{Incremental and Decremental Extreme Learning Machine based on Generalized Inverse}},
volume = {3536},
year = {2017}
}
@article{Jin2016,
abstract = {Extreme learning machine (ELM) is one of the most important and efficient machine learning algorithms for pattern classification due to its fast learning speed. In this paper, we propose a new ensemble based ELM approach for cross-modality face matching. Different to traditional face recognition methods, the proposed approach integrates the voting-base extreme learning machine (V-ELM) with a novel feature learning based face descriptor. Firstly, the discriminant feature learning is proposed to learn the cross-modality feature representation. Then, we used common subspace learning based method to reduce the obtained cross-modality features. Finally, Voting ELM is utilized as the classifier to improve the recognition accuracy and to speed up the feature learning process. Experiments conducted on two different heterogeneous face recognition scenarios demonstrate the effectiveness of our proposed approach.},
author = {Jin, Yi and Cao, Jiuwen and Wang, Yizhi and Zhi, Ruicong},
doi = {10.1007/s11042-015-2650-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/jin2015.pdf:pdf},
isbn = {1573-7721},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Canonical correlation analysis,Cross-modality matching,Extreme learning machine,Feature learning,Neural network},
number = {19},
pages = {11831--11846},
title = {{Ensemble based extreme learning machine for cross-modality face matching}},
volume = {75},
year = {2016}
}
@article{Jørgensen2007,
abstract = {This paper aims to provide a basis for the improvement of software estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. A web-based library of these cost estimation papers is provided to ease the identification of relevant estimation research results. The review results combined with other knowledge provide support for recommendations for future software cost estimation research, including: 1) Increase the breadth of the search for relevant studies, 2) Search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) Conduct more studies on estimation methods commonly used by the software industry, and, 4) Increase the awareness of how properties of the data sets impact the results when evaluating estimation methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {J{\o}rgensen, M and Shepperd, M J},
doi = {10.1109/TSE.2007.256943},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/jorgensen2007.pdf:pdf},
isbn = {0098-5589},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {research methods,software cost estimation,software cost prediction,software effort estimation,software effort prediction,systematic review},
number = {1},
pages = {33--53},
pmid = {4027147},
title = {{A Systematic Review of Software Development Cost Estimation Studies}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4027147},
volume = {33},
year = {2007}
}
@article{Kan2013,
abstract = {Unmanned aerial vehicles (UAVs) rely on global positioning system (GPS) information to ascertain its position for navigation during mission execution. In the absence of GPS information, the capability of a UAV to carry out its intended mission is hindered. In this paper, we learn alternative means for UAVs to derive real-time positional reference information so as to ensure the continuity of the mission. We present extreme learning machine as a mechanism for learning the stored digital elevation information so as to aid UAVs to navigate through terrain without the need for GPS. The proposed algorithm accommodates the need of the on-line implementation by supporting multi-resolution terrain access, thus capable of generating an immediate path with high accuracy within the allowable time scale. Numerical tests have demonstrated the potential benefits of the approach.},
author = {Kan, Ee May and Lim, Meng Hiot and Ong, Yew Soon and Tan, Ah Hwee and Yeo, Swee Ping},
doi = {10.1007/s00521-012-0866-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/kan2012.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machines (ELM),Terrain-based navigation,Unmanned aerial vehicles (UAVs)},
number = {3-4},
pages = {469--477},
title = {{Extreme learning machine terrain-based navigation for unmanned aerial vehicles}},
volume = {22},
year = {2013}
}
@article{Karami2016,
author = {Karami, Hojat and Karimi, Sohrab and Bonakdari, Hossein and Shamshirband, Shahabodin},
doi = {10.1007/s00521-016-2588-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/karami2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Artificial neural network,Discharge coefficient,Extreme learning machine,Genetic programming,Weir},
pages = {1--7},
publisher = {Springer London},
title = {{Predicting discharge coefficient of triangular labyrinth weir using extreme learning machine, artificial neural network and genetic programming}},
year = {2016}
}
@article{Karpagachelvi2012,
abstract = {An Electrocardiogram or ECG is an electrical recording of the heart and is used in the investigation of heart disease. This ECG can be classified as normal and abnormal signals. The classification of the ECG signals is presently performed with the support vector machine. The generalization performance of the SVM classifier is not sufficient for the correct classification of ECG signals. To overcome this problem, the ELM classifier is used which works by searching for the best value of the parameters that tune its discriminant function and upstream by looking for the best subset of features that feed the classifier. The experiments were conducted on the ECG data from the Physionet arrhythmia database to classify five kinds of abnormal waveforms and normal beats. In this paper, a thorough experimental study was done to show the superiority of the generalization capability of the Extreme Learning Machine (ELM) that is presented and compared with support vector machine (SVM) approach in the automatic classification of ECG beats. In particular, the sensitivity of the ELM classifier is tested and that is compared with SVM combined with two classifiers, and they are the k-nearest Neighbor Classifier and the radial basis function neural network classifier, with respect to the curse of dimensionality and the number of available training beats. The obtained results clearly confirm the superiority of the ELM approach as compared with traditional classifiers.},
author = {Karpagachelvi, S. and Arthanari, M. and Sivakumar, M.},
doi = {10.1007/s00521-011-0572-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/karpagachelvi2011.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Electrocardiogram (ECG) signals classification,Extreme learning machine,Feature detection,Feature reduction,Generalization capability,Model selection issue,Support vector machine},
number = {6},
pages = {1331--1339},
pmid = {18779082},
title = {{Classification of electrocardiogram signals with support vector machines and extreme learning machine}},
volume = {21},
year = {2012}
}
@article{Kassani2018,
abstract = {An extreme learning machine (ELM) is a popular analytic single hidden layer feedforward neural network because of its rapid learning capacity. However, vanilla dense ELMs are affected by the overfitting problem when the number of hidden neurons is high. Further direct consequences of the density are decreases in both the training and prediction speeds. In this study, we propose an incremental method for sparsifying the ELM using a newly devised indicator driven by the condition number in the ELM design matrix, in which we call sparse pseudoinverse incremental-ELM (SPI-ELM). SPI-ELM exhibits better generalization performance and lower run-time complexity compared with ELM. However, the sparsification process may negatively affect the learning speed of SPI-ELM; thus, we introduce an iterative matrix decomposition algorithm to address this issue. We also demonstrate that there is a useful relationship between the condition number in the ELM design matrix and the number of hidden neurons. This relationship helps to understand the random weights and nonlinear activation functions in ELMs. We evaluated the SPI-ELM method based on 20 benchmark data sets from the University of California Irvine repository and three real-world databases from the computer vision domain.},
author = {Kassani, Peyman Hosseinzadeh and Teoh, Andrew Beng Jin and Kim, Euntai},
doi = {10.1016/j.neucom.2018.01.087},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/kassani2018.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Hidden neuron,Least squares estimation,Matrix decomposition,Sparsity},
publisher = {Elsevier B.V.},
title = {{Sparse pseudoinverse incremental extreme learning machine}},
url = {https://doi.org/10.1016/j.neucom.2018.01.087},
year = {2018}
}
@article{Kasun2016,
abstract = {Data may often contain noise or irrelevant information which negatively affect the generalization capability of machine learning algorithms. The objective of dimension reduction algorithms such as Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), random projection (RP) and auto-encoder (AE) is to reduce the noise or irrelevant information of the data. The features of PCA (eigenvectors) and linear AE is not able to represent data as parts (e.g. nose in a face image); On the other hand, NMF and non-linear AE is maimed by slow learning speed and RP only represents a subspace of original data. This paper introduces a dimension reduction framework which to some extend represents data as parts, has fast learning speed and learns the between-class scatter subspace. To this end, this paper investigates a linear and nonlinear dimension reduction framework referred to as Extreme Learning Machine Auto-Encoder (ELM-AE) and Sparse Extreme Learning Machine Auto-Encoder (SELM-AE). In contrast to tied weight auto-encoder (TAE), the hidden neurons in ELMAE and SELM-AE need not be tuned, their parameters (e.g, input weights in additive neurons) are initialized using orthogonal and sparse random weights respectively. Experimental results on USPS handwritten digit recognition dataset, CIFAR-10 object recognition and NORB object recognition data set show the efficacy of linear and non-linear ELM-AE and SELM-AE in terms of discriminative capability, sparsity, training time and Normalized Mean Square Error (NMSE).},
author = {Kasun, Liyanaarachchi Lekamalage Chamara and Yang, Yan and Huang, Guang-Bin and Zhang, Zhengyou},
doi = {10.1109/TIP.2016.2570569},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/kasun2016.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
number = {8},
pages = {3906--3918},
pmid = {28113427},
title = {{Dimension Reduction With Extreme Learning Machine}},
url = {http://ieeexplore.ieee.org/document/7471467/},
volume = {25},
year = {2016}
}
@article{Kasun2013,
abstract = {Restricted Boltzmann Machines (RBM) and auto encoders, learns to represent features in a dataset meaningfully and used as the basic building blocks to create deep networks. This paper introduces Extreme Learning Machine based Auto Encoder (ELM-AE), which learns feature representations using singular values and is used as the basic building block for Multi Layer Extreme Learning Machine (ML-ELM). ML-ELM performance is better than auto encoders based deep networks and Deep Belief Networks (DBN), while in par with Deep Boltzmann Machines (DBM) for MNIST dataset. HoweverML- ELM is significantly faster than any state−of−the−art deep networks.},
author = {Kasun, Liyanaarachchi Lekamalage Chamara and Zhou, Hongming and Huang, Guang-bin and Vong, ChiMan},
doi = {10.1109/MIS.2013.140},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/kasun2013.pdf:pdf},
isbn = {1541-1672},
issn = {1541-1672},
journal = {IEEE Intelligent System},
number = {4},
pages = {1--4},
title = {{Representational Learning with Extreme Learning Machine for Big Data}},
url = {http://www.ntu.edu.sg/home/egbhuang/pdf/ELM{\_}Auto{\_}Encoder.pdf},
year = {2013}
}
@article{Kitchenham2007,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
archivePrefix = {arXiv},
arxivId = {1304.1186},
author = {Kitchenham, Barbara and Charters, Stuart},
doi = {10.1145/1134285.1134500},
eprint = {1304.1186},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/kitchenham2007.pdf:pdf},
isbn = {1595933751},
issn = {00010782},
journal = {Engineering},
number = {4ve},
pages = {1051},
pmid = {10853839},
title = {{Guidelines for performing Systematic Literature reviews in Software Engineering Version 2.3}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Guidelines+for+performing+Systematic+Literature+Reviews+in+Software+Engineering{\#}0{\%}5Cnhttp://www.dur.ac.uk/ebse/resources/Systematic-reviews-5-8.pdf},
volume = {45},
year = {2007}
}
@article{Ksieniewicz2017,
abstract = {Remote sensing and hyperspectral data analysis are areas offering wide range of valuable practical applications. However, they generate massive and complex data that is very difficult to be analyzed by a human being. Therefore, methods for efficient data representation and data mining are of high interest to these fields. In this paper, we introduce a novel pipeline for feature extraction and classification of hyperspectral images. To obtain a compressed representation we propose to extract a set of statistical-based properties from these images. This allows for embedding feature space into fourteen channels, obtaining a significant dimensionality reduction. These features are used as an input for the ensemble learning based on randomized neural networks. We introduce a novel method for forming ensembles of Extreme Learning Machines based on randomized feature subspaces and a trained combiner. It is based on continuous outputs and uses a perceptron-based learning scheme to calculate weights assigned to each classifier and class independently. Extensive experiments carried on a number of benchmarks images prove that using proposed feature extraction and extreme learning ensemble leads to a significant gain in classification accuracy.},
author = {Ksieniewicz, Pawe{\l} and Krawczyk, Bartosz and Wo{\'{z}}niak, Micha{\l}},
doi = {10.1016/j.neucom.2016.04.076},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/ksieniewicz2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Computer vision,Dimensionality reduction,Ensemble learning,Extreme Learning Machines,Feature extraction,Hyperspectral imaging,Image classification},
number = {2017},
title = {{Ensemble of Extreme Learning Machines with trained classifier combination and statistical features for hyperspectral data}},
year = {2017}
}
@article{Kuang2017,
author = {Kuang, Yuxiang and Wu, Qun and Shao, Junkai and Wu, Jianfeng and Wu, Xuehua},
doi = {10.1007/s10586-017-0985-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/kuang2017.pdf:pdf},
isbn = {1058601709852},
issn = {15737543},
journal = {Cluster Computing},
keywords = {ELM-LLE,Movement recognition,Multi-classification,Surface EMG},
number = {4},
pages = {3051--3059},
publisher = {Springer US},
title = {{Extreme learning machine classification method for lower limb movement recognition}},
volume = {20},
year = {2017}
}
@article{Kutlu2017,
abstract = {{\textcopyright} 2017 The Natural Computing Applications Forum Electroencephalography (EEG) has been used as a promising tool for investigation of brain activity during cognitive processes. The aim of this study is to reveal whether EEG signals can be used for classifying cognitive processes: arithmetic tasks and text reading. A recently introduced EEG database, which is constructed from 18 healthy subjects during a slide show including 60 slides of simple arithmetic tasks and easily readable texts, is used for this purpose. Multi-order difference plot-based time-domain attributes, number of values in specified regions after scattering the sequential difference values with several degrees, are extracted. For classification, improved extreme learning machine (ELM) scheme, namely luELM, by the use of lower–upper triangularization method instead of singular value decomposition which has disadvantages when used with huge data is proposed. As a result, higher accuracy results are achieved with reduced training time for proposed luELM classifier than traditional ELM classifier for both subject-dependent and subject-independent analysis.},
author = {Kutlu, Yakup and Yayık, Apdullah and Yildirim, Esen and Yildirim, Serdar},
doi = {10.1007/s00521-017-3142-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/kutlu2017.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Cognitive processes,Extreme learning machine,Lower–upper triangularization,MoDP method,Optimized nodes},
pages = {1--10},
title = {{LU triangularization extreme learning machine in EEG cognitive task classification}},
year = {2017}
}
@article{Lan2013,
author = {Lan, Yuan and Hu, Zongjiang and Soh, Yeng Chai and Huang, Guang-Bin},
doi = {10.1007/s00521-012-0946-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/lan2012.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {based extreme learning,kernelized extreme learning machine,learning,learning machine {\'{a}},machine {\'{a}} optimization method,machine {\'{a}} regularized extreme,speaker verification {\'{a}} extreme,{\'{a}}},
number = {3-4},
pages = {417--425},
title = {{An extreme learning machine approach for speaker recognition}},
url = {http://link.springer.com/10.1007/s00521-012-0946-x},
volume = {22},
year = {2013}
}
@article{Lauren2017,
abstract = {The unstructured nature of clinical narratives makes them complex for automatically extracting information. Feature learning is an important precursor to document classification, a sub-discipline of natural language processing (NLP). In NLP, word and document embeddings are an effective approach for generating word and document representations (vectors) in a low-dimensional space. This paper uses skip-gram and paragraph vectors-distributed bag of words (PV-DBOW) with multiple discriminant analysis (MDA) to arrive at discriminant document embeddings. A kernel-based extreme learning machine (ELM) is used to map the clinical texts to the medical code. Experimental results on clinical texts indicate overall improvement especially for the minority classes.},
author = {Lauren, Paula and Qu, Guangzhi and Zhang, Feng and Lendasse, Amaury},
doi = {10.1016/j.neucom.2017.01.117},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/lauren2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Clinical narratives,Document classification,Document embeddings,Extreme learning machines,Feature learning,Multiple discriminant analysis,PV-DBOW,Skip-gram,Word embeddings},
publisher = {Elsevier B.V.},
title = {{Discriminant document embeddings with an extreme learning machine for classifying clinical narratives}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.01.117},
year = {2017}
}
@article{Lee2013,
author = {Lee, Kevin and Man, Zhihong and Wang, Dianhui and Cao, Zhenwei},
doi = {10.1007/s00521-012-0847-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/lee2012.pdf:pdf},
isbn = {9781612849720},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Classification,Extreme learning machine,FIR filter,Linear separability,Microarray gene expression data},
number = {3-4},
pages = {457--468},
title = {{Classification of bioinformatics dataset using finite impulse response extreme learning machine for cancer diagnosis}},
volume = {22},
year = {2013}
}
@article{Li2013,
author = {Li, Bin and Li, Yibin and Rong, Xuewen},
doi = {10.1007/s00521-012-0858-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/li2012.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Differential evolution algorithm,Extreme learning machine,Single hidden layer feed-forward neural networks,Tunable activation function},
number = {3-4},
pages = {531--539},
title = {{The extreme learning machine learning algorithm with tunable activation function}},
volume = {22},
year = {2013}
}
@article{Li2013a,
author = {Li, Guoqiang and Niu, Peifeng},
doi = {10.1007/s00521-011-0771-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/li2011.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Least square method,Ridge regression,Single hidden layer feedforward neural networks},
number = {3-4},
pages = {803--810},
title = {{An enhanced extreme learning machine based on ridge regression for regression}},
volume = {22},
year = {2013}
}
@article{Li2017,
abstract = {Extreme learning machine (ELM) is an efficient learning algorithm for multi-classification and regression. However, original ELM doesn't consider the weight of each sample in training-set, which may cause the accuracy decreasing especially in imbalanced datasets. Even if each training sample is assigned with an extra weight, the problem on how to determinate the weight adaptively still remains. Inspiration by AdaBoost algorithm, we embed the weighted ELM algorithm in AdaBoost framework. In the meanwhile, we incorporate spatial and spectral information in composite kernel for each sample, which has a good performance in hyperspectral image (HSI) classification. By combining composite kernel methods and Adaboost framework with weighted ELM, a novel algorithm, namely AdaBoost composite kernel extreme learning machines denoted as AdaBoost-WCKELM is proposed. Experimental results demonstrate that the proposed method outperforms current state-of-the-art algorithms and derives a good improvement in HSI classification accuracy.},
author = {Li, Lu and Wang, Chengyi and Li, Wei and Chen, Jingbo},
doi = {10.1016/j.neucom.2017.09.004},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/li2017.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
title = {{Hyperspectral Image Classification by AdaBoost Weighted Composite Kernel Extreme Learning Machines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217314753},
year = {2017}
}
@article{Li2016,
abstract = {The extreme learning machine (ELM) has drawn insensitive research attentions due to its effectiveness in solving many machine learning problems. However, the matrix inversion operation involved in the algorithm is computational prohibitive and limits the wide applications of ELM in many scenarios. To overcome this problem, in this paper, we propose an inverse-free ELM to incrementally increase the number of hidden nodes, and update the connection weights progressively and optimally. Theoretical analysis proves the monotonic decrease of the training error with the proposed updating procedure and also proves the optimality in every updating step. Extensive numerical experiments show the effectiveness and accuracy of the proposed algorithm.},
author = {Li, Shuai and You, Zhu Hong and Guo, Hongliang and Luo, Xin and Zhao, Zhong Qiu},
doi = {10.1109/TCYB.2015.2434841},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/li2016.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Extreme learning machine (ELM),inverse-free,neural networks,optimal updates},
number = {5},
pages = {1229--1241},
title = {{Inverse-free extreme learning machine with optimal information updating}},
volume = {46},
year = {2016}
}
@article{Li2017a,
author = {Li, Xia and Niu, Peifeng and Li, Guoqiang and Liu, Jianping},
doi = {10.1007/s11063-017-9611-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/li2017.pdf:pdf},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Circulating fluidized bed boiler,Extreme learning machine,NOx emission,Relaxation factor,Teaching–learning-based optimization},
number = {2},
pages = {643--662},
publisher = {Springer US},
title = {{An Adaptive Extreme Learning Machine for Modeling NOx Emission of a 300 MW Circulating Fluidized Bed Boiler}},
volume = {46},
year = {2017}
}
@article{Li2014,
author = {Li, Xiaodong and Mao, Weijie and Jiang, Wei},
doi = {10.1007/s00521-014-1709-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/li2014.pdf:pdf},
isbn = {0941-0643$\backslash$r1433-3058},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {ELM kernel,Extreme learning machine,Minimal norm of weights,Multi-class ELM,Multiple-kernel learning (MKL),QCQP,SILP,extreme learning machine {\'{a}},learning,minimal norm of weights,mkl,multi-class elm,multiple-kernel,qcqp {\'{a}} silp {\'{a}},{\'{a}},{\'{a}} elm kernel {\'{a}}},
number = {1},
pages = {175--184},
title = {{Multiple-kernel-learning-based extreme learning machine for classification design}},
url = {http://dx.doi.org/10.1007/s00521-014-1709-7},
volume = {27},
year = {2014}
}
@article{Li2016a,
abstract = {How to predict stock price movements based on quantitative market data modeling is an attractive topic. In front of the market news and stock prices that are commonly believed as two important market data sources, how to extract and exploit the hidden information within the raw data and make both accurate and fast predictions simultaneously becomes a challenging problem. In this paper, we present the design and architecture of our trading signal mining platform that employs extreme learning machine (ELM) to make stock price prediction based on those two data sources concurrently. Comprehensive experimental comparisons between ELM and the state-of-the-art learning algorithms, including support vector machine (SVM) and back-propagation neural network (BP-NN), have been undertaken on the intra-day tick-by-tick data of the H-share market and contemporaneous news archives. The results have shown that (1) both RBF ELM and RBF SVM achieve higher prediction accuracy and faster prediction speed than BP-NN; (2) the RBF ELM achieves similar accuracy with the RBF SVM and (3) the RBF ELM has faster prediction speed than the RBF SVM. Simulations of a preliminary trading strategy with the signals are conducted. Results show that strategy with more accurate signals will make more profits with less risk.},
author = {Li, Xiaodong and Xie, Haoran and Wang, Ran and Cai, Yi and Cao, Jingjing and Wang, Feng and Min, Huaqing and Deng, Xiaotie},
doi = {10.1007/s00521-014-1550-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/li2014{\_}2.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Stock market prediction,Trading signal mining platform},
number = {1},
pages = {67--78},
title = {{Empirical analysis: stock market prediction via extreme learning machine}},
volume = {27},
year = {2016}
}
@article{Lian2014,
abstract = {Landslide hazard is a complex nonlinear dynamical system with uncertainty. The evolution of landslide is influenced by many factors such as tectonic, rainfall and reservoir level fluctuation. Using a time series model, total accumulative displacement of landslide can be divided into the trend component displacement and the periodic component displacement according to the response relation between dynamic changes in landslide displacement and inducing factors. In this paper, a novel neural network technique called ensemble of extreme learning machine (E-ELM) is proposed to investigate the interactions of different inducing factors affecting the evolution of landslide. Grey relational analysis is used to sieve out the more influential inducing factors as the inputs in E-ELM. Trend component displacement and periodic component displacement are forecasted, respectively; then, total predictive displacement is obtained by adding the calculated predictive displacement value of each sub. Performances of our model are evaluated by using real data from Baishuihe landslide in the Three Gorges Reservoir of China, and it provides a good representation of the measured slide displacement behavior.},
author = {Lian, Cheng and Zeng, Zhigang and Yao, Wei and Tang, Huiming},
doi = {10.1007/s00521-013-1446-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/lian2013.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Artificial neural networks,Displacement prediction,Ensemble,Extreme learning machine,Grey relational analysis,Landslide},
number = {1},
pages = {99--107},
title = {{Ensemble of extreme learning machine for landslide displacement prediction based on time series analysis}},
volume = {24},
year = {2014}
}
@article{Lim2013,
abstract = {Huang et al. (2004) has recently proposed an on-line sequential ELM (OS-ELM) that enables the extreme learning machine (ELM) to train data one-by-one as well as chunk-by-chunk. OS-ELM is based on recursive least squares-type algorithm that uses a constant forgetting factor. In OS-ELM, the parameters of the hidden nodes are randomly selected and the output weights are determined based on the sequentially arriving data. However, OS-ELM using a constant forgetting factor cannot provide satisfactory performance in time-varying or nonstationary environments. Therefore, we propose an algorithm for the OS-ELM with an adaptive forgetting factor that maintains good performance in time-varying or nonstationary environments. The proposed algorithm has the following advantages: (1) the proposed adaptive forgetting factor requires minimal additional complexity of O(N) where N is the number of hidden neurons, and (2) the proposed algorithm with the adaptive forgetting factor is comparable with the conventional OS-ELM with an optimal forgetting factor. {\textcopyright} 2012 Springer-Verlag London Limited.},
author = {seok Lim, Jun and Lee, Seokjin and Pang, Hee Suk},
doi = {10.1007/s00521-012-0873-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/lim2012.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,OS-ELM,RLS adaptive forgetting factor},
number = {3-4},
pages = {569--576},
title = {{Low complexity adaptive forgetting factor for online sequential extreme learning machine (OS-ELM) for application to nonstationary system estimations}},
volume = {22},
year = {2013}
}
@article{Liouane2017,
abstract = {{\textcopyright} 2017 Springer Science+Business Media, LLC One of the main objectives of smart homes is healthcare monitoring and assistance, especially for elderly and disabled people. Therefore, an accurate prediction of the inhabitant behavior is very helpful to provide the required assistance. This work aims to propose a prediction model that satisfies the accuracy as well as the rapidity of the learning phase. To do so, we propose to improve the existing extreme learning machine (ELM) model by defining a recurrent form. This form ensures a temporal relationship of inputs between observations at different time steps. The new model uses feedback connections to the input layer from the output layer which allows the output to be included in the long-term prediction. A recurrent dynamic network, with feedback connections of the output of the network, is proposed to predict the future series representing future activities of the inhabitant. The resulting model, called Recurrent Extreme Learning Machine (RELM), provides the ability to learn the human behavior and ensures a good balance between the learning time and the prediction accuracy. The input data is based on the real data representing the activities of persons belonging to the profile of first level (i.e. P 1 ) as measured by the dependency model called Functional Autonomy Measurement System (SMAF) used in the geriatric domain. The experimental results reveal that the proposed RELM model requires a minimum time during the learning phase with a better performance compared to existing models.},
author = {Liouane, Z. and Lemlouma, T. and Roose, P. and Weis, F. and Messaoud, H.},
doi = {10.1007/s10489-017-1062-5},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/liouane2017.pdf:pdf},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {accuracy,behavior prediction,elderly,elm,relm,smart home,time series prediction},
title = {{An improved extreme learning machine model for the prediction of human scenarios in smart homes}},
year = {2017}
}
@article{Liu2016,
author = {Liu, Bing and Xia, Shi-Xiong and Meng, Fan-Rong and Zhou, Yong},
doi = {10.1007/s00521-014-1777-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/liu2015.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {elm,learning,machine,manifold regularization {\'{a}} extreme,semi-supervised learning,{\'{a}},{\'{a}} random feature mapping},
number = {2},
pages = {255--269},
title = {{Manifold regularized extreme learning machine}},
url = {http://link.springer.com/10.1007/s00521-014-1777-8},
volume = {27},
year = {2016}
}
@article{Liu2017,
abstract = {{\textcopyright} 2016, The Natural Computing Applications Forum. It is practically and theoretically significant to approximate and simulate a system with fuzzy inputs and fuzzy outputs. This paper proposes a extreme learning machine (ELM)-based fuzzy regression model (FR ELM ) in which both inputs and outputs are triangular fuzzy numbers. Algorithm for training FR ELM is designed, and its computational complexity is analyzed. Furthermore, the convergence and error estimation for FR ELM are discussed. Numerical simulations show that the proposed FR ELM can effectively approximate a fuzzy input and fuzzy output system.},
author = {tao Liu, Hai and Wang, Jing and lin He, Yu and Ashfaq, Rana Aamir Raza},
doi = {10.1007/s00521-016-2232-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/liu2016.pdf:pdf},
isbn = {0052101622},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Fuzzy input and fuzzy output,Fuzzy linear regression,Triangular fuzzy number},
number = {11},
pages = {3465--3476},
publisher = {Springer London},
title = {{Extreme learning machine with fuzzy input and fuzzy output for fuzzy regression}},
volume = {28},
year = {2017}
}
@article{Liu2018,
author = {Liu, Huaping and Li, Fengxue and Xu, Xinying and Sun, Fuchun},
doi = {10.1016/j.neucom.2017.04.077},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/huapingliu2017.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Representation learning,extreme learning machine,local receptive field,multi-modal},
pages = {4--11},
publisher = {Elsevier B.V.},
title = {{Multi-modal local receptive field extreme learning machine for object recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217314169},
volume = {277},
year = {2018}
}
@article{Liu2016a,
abstract = {{\textcopyright} 2014, Springer-Verlag London. High accuracy and low overhead are two key features of a well-designed classifier for different classification scenarios. In this paper, we propose an improved classifier using a single-hidden layer feedforward neural network (SLFN) trained with extreme learning machine. The novel classifier first utilizes principal component analysis to reduce the feature dimension and then selects the optimal architecture of the SLFN based on a new localized generalization error model in the principal component space. Experimental and statistical results on the NSL-KDD data set demonstrate that the proposed classifier can achieve a significant performance improvement compared with previous classifiers.},
author = {Liu, Qiang and Yin, Jianping and Leung, Victor C M and Zhai, Jun Hai and Cai, Zhiping and Lin, Jiarun},
doi = {10.1007/s00521-014-1549-5},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/qiangliu2014.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Architecture selection algorithm,Extreme learning machine,Localized generalization error model,Principal component analysis},
number = {1},
pages = {59--66},
title = {{Applying a new localized generalization error model to design neural networks trained with extreme learning machine}},
url = {http://dx.doi.org/10.1007/s00521-014-1549-5},
volume = {27},
year = {2016}
}
@article{Liu2017a,
abstract = {Clustering generic data, i.e., data not specific to a particular field, is a challenging problem due to their diverse complex structures in the original feature space. Traditional approaches address this problem by complementing clustering with feature learning methods, which either capture the intrinsic structure of the data or represent the data such that clusters are better revealed. In this paper, we propose an approach referred to as Extreme Learning Machine for Joint Embedding and Clustering (ELM-JEC), which incorporates desirable properties of both types of feature learning methods at the same time, specifically by (1) preserving the manifold structure of the data in the original space; (2) maximizing the class separability of the data in the embedded space. Since either type of method has improved clustering performance in some cases, our motivation is to integrate the two desirable properties to further improve the accuracy and robustness of clustering. Additional notable features of ELM-JEC are that it provides nonlinear feature mappings and achieves feature learning and clustering in the same formulation. The proposed approach can be implemented using alternating optimization, and its clustering performance compares favorably with several state-of-the-art methods on the real-world benchmark datasets.},
author = {Liu, Tianchi and {Liyanaarachchi Lekamalage}, Chamara Kasun and Huang, Guang Bin and Lin, Zhiping},
doi = {10.1016/j.neucom.2017.01.115},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/liu2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Clustering,Embedding,Extreme learning machine,Feature learning,Manifold regularization,k-means},
publisher = {Elsevier B.V.},
title = {{Extreme Learning Machine for Joint Embedding and Clustering}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.01.115},
year = {2017}
}
@article{Liu2013,
author = {Liu, Xueyi and Li, Ping and Gao, Chuanhou},
doi = {10.1007/s00521-012-0859-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/liu2012.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Black-box model,Chaotic time series,Extreme learning machine,Generalization performance,Symmetry},
number = {3-4},
pages = {551--558},
title = {{Symmetric extreme learning machine}},
volume = {22},
year = {2013}
}
@article{Liu2016b,
author = {Liu, Xun and Deng, Chenwei and Wang, Shuigen and Huang, Guang-bin and Zhao, Baojun},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/liu2016.pdf:pdf},
journal = {Geoscience and Remote Sensing Letters, IEEE},
number = {12},
pages = {2039--2043},
title = {{Fast and Accurate Spatiotemporal Fusion Based Upon Extreme Learning Machine}},
volume = {13},
year = {2016}
}
@article{Lou2014,
abstract = {{\textless}p{\textgreater} Understanding and predicting dynamic change of algae population in freshwater reservoirs is particularly important, as algae-releasing cyanotoxins are carcinogens that would affect the health of public. However, the high complex nonlinearity of water variables and their interactions makes it difficult to model the growth of algae species. Recently, support vector machine (SVM) was reported to have advantages of only requiring a small amount of samples, high degree of prediction accuracy, and long prediction period to solve the nonlinear problems. In this study, the SVM-based prediction and forecast models for phytoplankton abundance in Macau Storage Reservoir (MSR) are proposed, in which the water parameters of pH, SiO {\textless}sub{\textgreater}2{\textless}/sub{\textgreater} , alkalinity, bicarbonate {\textless}math id="M1"{\textgreater} {\textless}mrow{\textgreater} {\textless}mo stretchy="false"{\textgreater}({\textless}/mo{\textgreater} {\textless}mrow{\textgreater} {\textless}msub{\textgreater} {\textless}mrow{\textgreater} {\textless}mtext{\textgreater}HCO{\textless}/mtext{\textgreater} {\textless}/mrow{\textgreater} {\textless}mtext{\textgreater}3{\textless}/mtext{\textgreater} {\textless}/msub{\textgreater} {\textless}msup{\textgreater} {\textless}mtext{\textgreater} {\textless}/mtext{\textgreater} {\textless}mo{\textgreater}-{\textless}/mo{\textgreater} {\textless}/msup{\textgreater} {\textless}/mrow{\textgreater} {\textless}mo stretchy="false"{\textgreater}){\textless}/mo{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} , dissolved oxygen (DO), total nitrogen (TN), UV {\textless}sub{\textgreater}254{\textless}/sub{\textgreater} , turbidity, conductivity, nitrate, total nitrogen (TN), orthophosphate {\textless}math id="M2"{\textgreater} {\textless}mrow{\textgreater} {\textless}mo stretchy="false"{\textgreater}({\textless}/mo{\textgreater} {\textless}mrow{\textgreater} {\textless}msub{\textgreater} {\textless}mrow{\textgreater} {\textless}mtext{\textgreater}PO{\textless}/mtext{\textgreater} {\textless}/mrow{\textgreater} {\textless}mn{\textgreater}4{\textless}/mn{\textgreater} {\textless}/msub{\textgreater} {\textless}msup{\textgreater} {\textless}mtext{\textgreater} {\textless}/mtext{\textgreater} {\textless}mrow{\textgreater} {\textless}mn{\textgreater}3{\textless}/mn{\textgreater} {\textless}mo{\textgreater}−{\textless}/mo{\textgreater} {\textless}/mrow{\textgreater} {\textless}/msup{\textgreater} {\textless}/mrow{\textgreater} {\textless}mo stretchy="false"{\textgreater}){\textless}/mo{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} , total phosphorus (TP), suspended solid (SS) and total organic carbon (TOC) selected from the correlation analysis of the 23 monthly water variables were included, with 8-year (2001–2008) data for training and the most recent 3 years (2009–2011) for testing. The modeling results showed that the prediction and forecast powers were estimated as approximately 0.76 and 0.86, respectively, showing that the SVM is an effective new way that can be used for monitoring algal bloom in drinking water storage reservoir. {\textless}/p{\textgreater}},
author = {Lou, Inchio and Xie, Zhengchao and {Kin Ung}, Wai and {Meng Mok}, Kai},
doi = {10.1007/978-3-319-04741-6_8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/lou2014.pdf:pdf},
isbn = {1024-123X},
issn = {18674542},
journal = {Adaptation, Learning, and Optimization},
keywords = {Algal bloom,Extreme leaning machine,Phytoplankton abundance,Prediction and forecast models},
pages = {95--111},
title = {{Freshwater algal bloom prediction by extreme learning machine in macau storage reservoirs}},
volume = {16},
year = {2014}
}
@article{Lu2017,
abstract = {{\textcopyright} 2017 Springer Science+Business Media, LLC, part of Springer Nature Recently, face recognition algorithms have made great progress in various real-world applications, e.g., authentication and criminal investigation. Deep-learning offers an end-to-end paradigm for vision recognition tasks and achieves good performance. However, designing and training the complex network architecture are time-consuming and labor-intensive. Moreover, under complex scenarios, illumination change, noise or occlusion in images degrade the performance of recognition algorithms. In order to ameliorate these issues, we propose an efficient three-layered low-rank supported extreme learning machine (LSELM) algorithm for face recognition which improves the recognition performance under complex scenarios with high efficiency. In the first layer, a given probe sample is clustered into certain training subspace as pre-clustering. In the second layer, with this subspace, a low-rank subspace of probe sample as robust feature which is insensitive to disguise, noise, variant expression or illumination will be recovered by low-rank decomposition. Furthermore, these low-rank discriminative features are coded to support training a forward neural network termed LSELM. Experimental results indicate that the proposed approach is on par with some deep-learning based face recognition algorithms on recognition performance but with less time complexity over some popular face datasets e.g., AR, Extend Yale-B, CMU PIE and LFW datasets.},
author = {Lu, T. and Guan, Y. and Zhang, Y. and Qu, S. and Xiong, Z.},
doi = {10.1007/s11042-017-5475-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/lu2017.pdf:pdf},
journal = {Multimedia Tools and Applications},
keywords = {Face recognition,Robust feature,Low-rank matrix re},
pages = {1--22},
publisher = {Multimedia Tools and Applications},
title = {{Robust and efficient face recognition via low-rank supported extreme learning machine}},
year = {2017}
}
@article{Luo2017,
abstract = {Extreme learning machine, as a generalized single-hidden-layer feedforward network, has achieved much attention for its extremely fast learning speed and good generalization performance. However, big data often makes a challenge in large scale learning of extreme learning machine due to the memory limitation of single machine as well as the distributed manner of large scale data in many applications. For the purpose of relieving the limitation of memory with big data, in this paper, we exploit a novel distributed model to implement the extreme learning machine algorithm in parallel for large-scale data set, namely distributed extreme learning machine (DELM). A corresponding algorithm is developed on the basis of alternating direction method of multipliers which has shown its effectiveness in distributed convex optimization. Finally, extensive experiments on some benchmark data sets are carried out to illustrate the effectiveness and superiority of the proposed DELM method with an analysis on the performance of speedup, scaleup and sizeup.},
author = {Luo, Minnan and Zhang, Lingling and Liu, Jun and Guo, Jun and Zheng, Qinghua},
doi = {10.1016/j.neucom.2016.03.112},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/luo2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Alternating direction method of multiplier,Extreme learning machine,Neuron work},
pages = {164--170},
publisher = {Elsevier B.V.},
title = {{Distributed extreme learning machine with alternating direction method of multiplier}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.03.112},
volume = {261},
year = {2017}
}
@article{Luo2017a,
abstract = {The stacked extreme learning machine (S-ELM) is an advanced framework of deep learning. It passes the 'reduced' outputs of the previous layer to the current layer, instead of directly propagating the previous outputs to the next layer in traditional deep learning. The S-ELM could address some large and complex data problems with a high accuracy and a relatively low requirement for memory. However, there is still room for improvement of the time complexity as well as robustness while using S-ELM. In this article, we propose an enhanced S-ELM by replacing the original principle component analysis (PCA) technique used in this algorithm with the correntropy-optimized temporal PCA (CTPCA), which is robust for outliers rejection and significantly improves the training speed. Then, the CTPCA-based S-ELM performs better than S-ELM in both accuracy and learning speed, when dealing with dataset disturbed by outliers. Furthermore, after integrating the extreme learning machine (ELM) sparse autoencoder (AE) method into the CTPCA-based S-ELM, the learning accuracy is further improved while spending a little more training time. Meanwhile, the sparser and more compact feature information are available by using the ELM sparse AE with more computational efforts. The simulation results on some benchmark datasets verify the effectiveness of our proposed methods.},
author = {Luo, Xiong and Xu, Yang and Wang, Weiping and Yuan, Manman and Ban, Xiaojuan and Zhu, Yueqin and Zhao, Wenbing},
doi = {10.1016/j.jfranklin.2017.08.014},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/xiongluo2017:},
issn = {00160032},
journal = {Journal of the Franklin Institute},
keywords = {correntropy,principal component analysis (PCA),sparse autoencoder,stacked extreme learning machine (S-ELM)},
publisher = {Elsevier Ltd},
title = {{Towards enhancing stacked extreme learning machine with sparse autoencoder by correntropy}},
url = {http://dx.doi.org/10.1016/j.jfranklin.2017.08.014},
year = {2017}
}
@article{Luo,
author = {Luo, Xiong and Yang, Xiaona and Wang, Weiping and Chang, Xiaohui and Wang, Xinyan and Zhao, Zhigang},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/luo2016s.pdf:pdf},
keywords = {cloud service,extreme learning machine,incremental learning,prediction},
pages = {74--82},
title = {{C LOUD C OMPUTING AND D ATA M INING A Novel Hidden Danger Prediction Method in Cloud- Based Intelligent Industrial Production Management Using Timeliness Managing Extreme Learning Machine}}
}
@article{Lv2016,
author = {Lv, Qi and Niu, Xin and Dou, Yong and Xu, Jiaqing and Lei, Yuanwu},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/lv2016.pdf:pdf},
number = {3},
pages = {434--438},
title = {{Classification of Hyperspectral Remote Sensing Image Using Hierarchical Local-Receptive-Field-Based Extreme Learning Machine}},
volume = {13},
year = {2016}
}
@article{Ma2014,
abstract = {Falls are one of the major causes leading to injury of elderly people. Using wearable devices for fall detection has a high cost and may cause inconvenience to the daily lives of the elderly. In this paper, we present an automated fall detection approach that requires only a low-cost depth camera. Our approach combines two computer vision techniques - shape based fall characterization and a learning based classifier to distinguish falls from other daily actions. Given a fall video clip, we extract Curvature Scale Space (CSS) features of human silhouettes at each frame and represent the action by a bag of CSS words (BoCSS). Then we utilize the Extreme Learning Machine (ELM) classifier to identify the BoCSS representation of a fall from those of other actions. In order to eliminate the sensitivity of ELM to its hyperparameters, we present a variable-length Particle Swarm Optimization algorithm (VPSO) to optimize the number of hidden neurons, corresponding input weights and biases of ELM. Using a low-cost Kinect depth camera, we build an action dataset that consists of 6 types of actions (falling, bending, sitting, squatting, walking, and lying) from 10 subjects. Experimenting with the dataset shows that our approach can achieve an upto 91.15{\%} sensitivity, 77.14{\%} specificity and 86.83{\%} accuracy. On a public dataset, our approach performs comparably to state-of-the-art fall detection methods that need multiple cameras.},
author = {Ma, Xin and Wang, Haibo and Xue, Bingxia and Zhou, Mingang and Ji, Bing and Li, Yibin},
doi = {10.1109/JBHI.2014.2304357},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/ma2014.pdf:pdf},
isbn = {2168-2208 (Electronic)$\backslash$r2168-2194 (Linking)},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Curvature scale space (CSS),extreme learning machine (ELM),fall detection,particle swarm optimization,shape contour},
number = {6},
pages = {1915--1922},
pmid = {25375688},
title = {{Depth-based human fall detection via shape features and improved extreme learning machine}},
volume = {18},
year = {2014}
}
@article{Maliha2016,
author = {Maliha, Ayman and Yusof, Rubiyah and Shapiai, Mohd Ibrahim},
doi = {10.1007/s00521-016-2754-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/maliha2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine (ELM),Object detection,Quadratic programming,Structured learning},
pages = {1--14},
publisher = {Springer London},
title = {{Extreme learning machine for structured output spaces}},
year = {2016}
}
@article{Man2016,
author = {Man, Zhihong and Huang, Guang-Bin},
doi = {10.1007/s00521-015-2087-5},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/man2015{\_}2.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
number = {2},
pages = {253--254},
publisher = {Springer London},
title = {{Guest editorial: Special issue on Extreme learning machine and applications (II)}},
url = {http://link.springer.com/10.1007/s00521-015-2087-5},
volume = {27},
year = {2016}
}
@article{Mantas2014,
author = {Mantas, Carlos J and Abell{\'{a}}n, Joaqu{\'{i}}n},
doi = {10.1016/j.eswa.2014.01.017},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/Mantas - Credal C45 based on imprecise probabilities to classify noisy data - 2014.pdf:pdf},
issn = {0957-4174},
journal = {EXPERT SYSTEMS WITH APPLICATIONS},
keywords = {imprecise dirichlet model,imprecise probabilities,uncertainty measures},
number = {10},
pages = {4625--4637},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications Credal-C4 . 5 : Decision tree based on imprecise probabilities to classify noisy data}},
url = {http://dx.doi.org/10.1016/j.eswa.2014.01.017},
volume = {41},
year = {2014}
}
@article{Mao2013,
author = {Mao, Wentao and Tian, Mei and Cao, Xizheng and Xu, Jiucheng},
doi = {10.1007/s00521-011-0804-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/mao2012.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Leave-one-out error bound,Multi-objective optimization},
number = {3-4},
pages = {521--529},
title = {{Model selection of extreme learning machine based on multi-objective optimization}},
volume = {22},
year = {2013}
}
@article{Mao2017,
abstract = {In many practical engineering applications, data tend to be collected in online sequential way with imbalanced class. Many traditional machine learning methods such as support vector machine and so on generally get biased classifier which leads to lower classification precision for minor class than major class. To get fast and efficient classification, a new online sequential extreme learning machine method with two-stage hybrid strategy is proposed. In offline stage, data-based strategy is employed, and the principal curve is introduced to model the distribution of minority class data. In online stage, algorithm-based strategy is employed, and a new leave-one-out cross-validation method using Sherman–Morrison matrix inversion lemma is proposed to tackle online imbalance data, meanwhile, with add-delete mechanism for updating network weights. And the rationality of this strategy is proved theoretically. The proposed method is evaluated on four UCI datasets and the real-world Macau air pollutant forecasting dataset. The experimental results show that, the proposed method outperforms the classical ELM, OS-ELM and meta-cognitive OS-ELM in terms of generalization performance and numerical stability.},
author = {Mao, Wentao and Wang, Jinwan and He, Ling and Tian, Yangyang},
doi = {10.1016/j.neucom.2016.05.111},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/mao2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Imbalance problem,Leave-one-out cross-validation,Online sequential learning,Principal curve},
number = {2017},
pages = {94--105},
publisher = {Elsevier B.V.},
title = {{Online sequential prediction of imbalance data with two-stage hybrid strategy by extreme learning machine}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.05.111},
volume = {261},
year = {2017}
}
@article{Mao2014,
abstract = {Considering the uncertainty of hidden neurons, choosing significant hidden nodes, called as model selection, has played an important role in the applications of extreme learning machines(ELMs). How to define and measure this uncertainty is a key issue of model selection for ELM. From the information geometry point of view, this paper presents a new model selection method of ELM for regression problems based on Riemannian metric. First, this paper proves theoretically that the uncertainty can be characterized by a form of Riemannian metric. As a result, a new uncertainty evaluation of ELM is proposed through averaging the Riemannian metric of all hidden neurons. Finally, the hidden nodes are added to the network one by one, and at each step, a multi-objective optimization algorithm is used to select optimal input weights by minimizing this uncertainty evaluation and the norm of output weight simultaneously in order to obtain better generalization performance. Experiments on five UCI regression data sets and cylindrical shell vibration data set are conducted, demonstrating that the proposed method can generally obtain lower generalization error than the original ELM, evolutionary ELM, ELM with model selection, and multi-dimensional support vector machine. Moreover, the proposed algorithm generally needs less hidden neurons and computational time than the traditional approaches, which is very favorable in engineering applications. {\textcopyright} 2013 Springer-Verlag London.},
author = {Mao, Wentao and Zheng, Yanbin and Mu, Xiaoxia and Zhao, Jinwei},
doi = {10.1007/s00521-013-1392-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/mao2013.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Model selection,Multi-objective optimization,Riemannian metric,Uncertainty},
number = {7-8},
pages = {1613--1625},
title = {{Uncertainty evaluation and model selection of extreme learning machine based on Riemannian metric}},
volume = {24},
year = {2014}
}
@article{Mirza2013,
abstract = {Most of the existing sequential learning methods for class imbalance learn data in chunks. In this paper, we propose a weighted online sequential extreme learning machine (WOS-ELM) algorithm for class imbalance learning (CIL). WOS-ELM is a general online learning method that alleviates the class imbalance problem in both chunk-by-chunk and one-by-one learning. One of the new features of WOS-ELM is that an appropriate weight setting for CIL is selected in a computationally efficient manner. In one-by-one learning of WOS-ELM, a new sample can update the classification model without waiting for a chunk to be completed. Extensive empirical evaluations on 15 imbalanced datasets show that WOS-ELM obtains comparable or better classification performance than competing methods. The computational time of WOS-ELM is also found to be lower than that of the competing CIL methods.},
author = {Mirza, Bilal and Lin, Zhiping and Toh, Kar Ann},
doi = {10.1007/s11063-013-9286-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/mirza2013.pdf:pdf},
isbn = {1370-4621},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Class imbalance,Extreme learning machine (ELM),Online sequential learning,Total error rate,Weighted least squares},
number = {3},
pages = {465--486},
title = {{Weighted online sequential extreme learning machine for class imbalance learning}},
volume = {38},
year = {2013}
}
@article{Muhammad2013,
author = {Muhammad, Ishaq Gul and Tepe, Kemal E. and Abdel-Raheem, Esam},
doi = {10.1007/s00521-011-0796-y},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/muhammad2012.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Analytical training,Equalization,OFDM,QAM slicer,Symbol detection},
number = {3-4},
pages = {491--500},
title = {{QAM equalization and symbol detection in OFDM systems using extreme learning machine}},
volume = {22},
year = {2013}
}
@article{Nair2017,
abstract = {Conventional Extreme Learning Machines utilize Moore–Penrose generalized pseudo-inverse to solve hidden layer activation matrix and perform analytical determination of output weights. Scalability is the major concern to be addressed in Extreme Learning Machines while dealing with large dataset. Motivated by these scalability concerns, this paper proposes a novel tensor decomposition based Extreme Learning Machine which utilize PARAFAC and TUCKER decomposition based techniques in a SPARK platform. This proposed Extreme Learning Machine achieve reduced training time and better accuracy when compared with a conventional Extreme Learning Machine.},
author = {Nair, Nikhitha K. and Asharaf, S.},
doi = {10.1016/j.bdr.2017.07.002},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/nair2017.pdf:pdf},
issn = {22145796},
journal = {Big Data Research},
keywords = {ALS,ELM,HOSVD,PARAFAC,TENSOR,TUCKER},
pages = {8--20},
publisher = {Elsevier Inc.},
title = {{Tensor Decomposition Based Approach for Training Extreme Learning Machines}},
url = {http://dx.doi.org/10.1016/j.bdr.2017.07.002},
volume = {10},
year = {2017}
}
@article{Nayak2017,
author = {Nayak, Deepak Ranjan and Dash, Ratnakar and Majhi, Banshidhar},
doi = {10.1016/j.neucom.2017.12.030},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/nayak2017.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Computer-aided diagnosis (CAD),Discrete ripplet-II transform (DR2T),Extreme learning machine (ELM),Magnetic resonance imaging (MRI),Modified PSO (MPSO)},
publisher = {Elsevier B.V.},
title = {{Discrete ripplet-II transform and modified PSO based improved evolutionary extreme learning machine for pathological brain detection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217318659},
year = {2017}
}
@article{Nayak2016,
author = {Nayak, P. K. and Mishra, S. and Dash, P. K. and Bisoi, Ranjeeta},
doi = {10.1007/s00521-015-2010-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/nayak2015.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Cumulative sum average filter,Extreme learning machine,Feature extraction,LLRBFNN,Pattern recognition,Power disturbance signals,Simultaneous power quality events,Teaching–learning-based optimization},
number = {7},
pages = {2107--2122},
publisher = {Springer London},
title = {{Comparison of modified teaching–learning-based optimization and extreme learning machine for classification of multiple power signal disturbances}},
volume = {27},
year = {2016}
}
@article{Nian2013,
author = {Nian, Rui and He, Bo and Lendasse, Amaury},
doi = {10.1007/s00521-012-0892-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/nian2012.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Dipole topology,Extreme learning machines,Geometrical topology hypothesis,Optimal cognition principle},
number = {3-4},
pages = {427--433},
title = {{3D object recognition based on a geometrical topology model and extreme learning machine}},
volume = {22},
year = {2013}
}
@article{Niu2016,
abstract = {Extreme learning machine (ELM) is a kind of feed-forward single hidden layer neural network, whose input weights and thresholds of hidden layers are generated ran-domly. Because the output-weights of ELM are calculated by the least-square method, the ELM presents a high speed on training and testing. However, the random input-weights and thresholds of hidden layers are not the best parameters, which can not pledge the training goals of the ELM to achieve the global minimum. In order to obtain the optimal input-weights and bias of hidden layer, this paper proposes the self-adjusting extreme learning machine, called SA-ELM. Based on the idea of the ameliorated teaching learning based optimization, the input-weights and the bias of hidden layer of extreme learning machine are adjusted with " teaching phase " and " learning phase " to minimize the objective function values. The SA-ELM is applied to the eight benchmark functions to test its validity and feasibility. Com-pared with ELM and fast learning network, the SA-ELM owns good regression accuracy and generalization performance. Besides, the SA-ELM is applied to build the thermal efficiency model of a 300 MW pulverized coal furnace. The experiment results reveal that the proposed algorithm owns engineering practical application value.},
author = {Niu, Peifeng and Ma, Yunpeng and Li, Mengning and Yan, Shanshan and Li, Guoqiang},
doi = {10.1007/s11063-016-9496-z},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/niu2016.pdf:pdf},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Ameliorated teaching learning based optimization,Boiler thermal efficiency model,Extreme learning machine,Fast learning network},
number = {3},
pages = {813--830},
publisher = {Springer US},
title = {{A Kind of Parameters Self-adjusting Extreme Learning Machine}},
volume = {44},
year = {2016}
}
@article{Oh2017,
abstract = {In this paper, we propose an efficient parameter tuning-free squared-loss mutual information (SMI) estimator in a form of a radial basis function (RBF) network. The input layer of the proposed network propagates a sample pair of two random variables to the hidden layer. The propagated samples are then transformed by a set of Gaussian RBF kernels with randomly determined kernel centers and widths similar to that in an extreme learning machine. The output layer adopts a linear weighting scheme which can be analytically estimated. Our empirical results show that the proposed estimator outperforms the competing state-of-the-art SMI estimators in terms of computational efficiency while showing the comparable estimation accuracy performance. Moreover, the proposed model achieves promising results in an application study of time-series change-points detection and driving stress.},
author = {Oh, Beom Seok and Sun, Lei and Ahn, Chung Soo and Yeo, Yong Kiang and Yang, Yan and Liu, Nan and Lin, Zhiping},
doi = {10.1016/j.neucom.2015.11.138},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/oh2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Change-points detection,Density ratio approximation,Driving stress,Electrocardiogram,Extreme learning machine,Squared-loss mutual information estimation},
number = {2017},
pages = {204--216},
publisher = {Elsevier B.V.},
title = {{Extreme learning machine based mutual information estimation with application to time-series change-points detection}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.11.138},
volume = {261},
year = {2017}
}
@article{Pacheco2018,
abstract = {The Extreme Learning Machine (ELM) is a single-hidden layer feedforward neural network (SLFN) learning algorithm that can learn effectively and quickly. The ELM training phase assigns the input weights and bias randomly and does not change them in the whole process. Although the network works well, the random weights in the input layer may affect the algorithm performance. Therefore, we propose a new approach to determine the input weights and bias for the ELM using the restricted Boltzmann machine (RBM), which we call RBM-ELM. We compare our new approach to the well-known ELM-AE and to the ELM-RO, a state of the art algorithm to select the input weights for the ELM. The experimental results show that the RBM-ELM achieves a better performance than the ELM and outperforms the ELM-AE and ELM-RO.},
archivePrefix = {arXiv},
arxivId = {1708.05376},
author = {Pacheco, Andre G.C. and Krohling, Renato A. and da Silva, Carlos A.S.},
doi = {10.1016/j.eswa.2017.11.054},
eprint = {1708.05376},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/pacheco2017.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Extreme learning machine,Neural networks,Restricted Boltzmann machine,Weights initialization},
pages = {77--85},
publisher = {Elsevier Ltd},
title = {{Restricted Boltzmann machine to determine the input weights for extreme learning machines}},
url = {https://doi.org/10.1016/j.eswa.2017.11.054},
volume = {96},
year = {2018}
}
@article{Pan2012,
author = {Pan, Chen and Park, Dong Sun and Yang, Yong and Yoo, Hyouck Min},
doi = {10.1007/s00521-011-0522-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/pan2011.pdf:pdf},
isbn = {0957-4174},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Image segmentation,Leukocyte,Real-time learning,Visual attention},
number = {6},
pages = {1217--1227},
title = {{Leukocyte image segmentation by visual attention and extreme learning machine}},
volume = {21},
year = {2012}
}
@article{Pang2017,
abstract = {A multi-graph is represented by a bag of graphs. Semi-supervised multi-graph classification is a partly supervised learning problem, which has a wide range of applications, such as bio-pharmaceutical activity tests, scientific publication categorization and online product recommendation. However, to the best of our knowledge, few research works have be reported. In this paper, we propose a semi-supervised multi-graph classification algorithm to handle the semi-supervised multi-graph classification problem. Our algorithm consists of three main steps, including the optimal subgraph feature selection, the subgraph feature representation of multi-graph and the semi-supervised classifier building. We first propose an evaluation criterion of the optimal subgraph features, which not only considers unlabeled multi-graphs but also considers the constraints between the multi-graph level and the graph level. Then, the optimal subgraph feature selection problem is equivalently converted into the problem of mining m most informative subgraph features. Based on those derived m subgraph features, every multi-graph is represented by an m-dimensional vector, where the ith dimension equals to 1 if at least one graph involved in the multi-graph contains the ith subgraph feature. At last, based on these vectors, semi-supervised extreme learning machine(semi-supervised ELM) is adopted to build the prediction model for predicting the labels of unseen multi-graphs. Extensive experiments on real-world and synthetic graph datasets show that the proposed algorithm is effective and efficient.},
author = {Pang, Jun and Gu, Yu and Xu, Jia and Yu, Ge},
doi = {10.1016/j.neucom.2017.01.114},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/pang2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Feature selection,Multi-graph,Semi-supervised},
pages = {1--12},
title = {{Semi-supervised multi-graph classification using optimal feature selection and extreme learning machine}},
volume = {0},
year = {2017}
}
@article{Parkavi2017,
abstract = {Extreme Learning Machine (ELM) is a high effective learning algorithm for the single hidden layer feed forward neural networks. Compared with the existing neural network learning algorithm it solves the slow training speed and over-fitting problems. It has been used in different fields and applications such as biomedical engineering, computer vision, remote sensing, chemical process and control and robotics. It has better generalization stability, sparsity, accuracy, robustness, optimal control and fast learning rate This paper introduces a brief review about ELM and MLELM, describing the principles and latest research progress about the algorithms, theories and applications. Next, Multilayer Extreme Learning Machine (MLELM) and other state-of-the-art classifiers are trained on this suitable training feature vector for classification of data. Deep learning has the advantage of approximating the complicated function and mitigating the optimization difficulty associated with deep models. Multilayer extreme learning machine is a learning algorithm of an Artificial Neural Network (ANN) which takes to be good for deep learning and extreme learning machine. This review presents a comprehensive view of these advances in ELM and MLELM which may be worthy of exploring in the future.},
author = {Parkavi, R Manju and Parkavi, R Manju and Shanthi, M and Bhuvaneshwari, M C},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/survey-papers/parkavi2017.pdf:pdf},
journal = {Advances in Science, Technology and Engineering Systems Journal},
keywords = {ann,artificial neural network,elm,extreme learning machine},
number = {1},
pages = {69--75},
title = {{Recent Trends in ELM and MLELM : A review}},
volume = {2},
year = {2017}
}
@article{Peng2017,
abstract = {Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks in which the parameters of hidden units are randomly generated and thus the output weights can be analytically calculated. From the hidden to output layer, ELM essentially learns the output weight matrix based on the least squares regression formula that can be used for both classification/regression and dimensionality reduction. In this paper, we impose the orthogonal constraint on the output weight matrix and then formulate an orthogonal extreme learning machine (OELM) model, which produces orthogonal basis functions and can have more locality preserving power from ELM feature space to output layer than ELM. Since the locality preserving ability is potentially related to the discriminating power, the OELM is expect to have more discriminating power than ELM. Considering the case that the number of hidden units is usually greater than the number of classes, we propose an effective method to optimize the OELM objective by solving an orthogonal procrustes problem. Experiments by pairwisely comparing OELM with ELM on three widely used image data sets show the effectiveness of learning orthogonal mapping especially when given only limited training samples.},
author = {Peng, Yong and Kong, Wanzeng and Yang, Bing},
doi = {10.1016/j.neucom.2017.05.058},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/peng2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Image classification,Orthogonal constraint,Orthogonal procrustes problem},
pages = {458--464},
publisher = {Elsevier B.V.},
title = {{Orthogonal extreme learning machine for image classification}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.05.058},
volume = {266},
year = {2017}
}
@article{Peng2017a,
abstract = {In order to seek non-propagation method to train generalized single-hidden layer feed forward neural networks, extreme learning machine was proposed, which has been proven to be an effective and efficient model for both multi-class classification and regression. Different from most of existing studies which consider extreme learning machine as a classifier, we make improvements on it to let it become a feature extraction model in this paper. Specifically, a discriminative extreme learning machine with supervised sparsity preserving (SPELM) model is proposed. From the hidden layer to output layer, SPELM performs as a subspace learning method by considering the discriminative as well as sparsity information of data. The sparsity information of data is identified by solving a supervised sparse representation objective. Experiments are conducted on four widely used image benchmark data sets and the classification results demonstrate the effectiveness of the proposed SPELM model.},
author = {Peng, Yong and Lu, Bao Liang},
doi = {10.1016/j.neucom.2016.05.113},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/peng2016.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Group sparsity,Image classification,Sparse representation,Sparsity preserving},
pages = {242--252},
publisher = {Elsevier B.V.},
title = {{Discriminative extreme learning machine with supervised sparsity preserving for image classification}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.05.113},
volume = {261},
year = {2017}
}
@article{Phoemphon2018,
abstract = { We investigate a practical integration of soft-computing for WSN localizations.  We propose a hybrid model applying Centroid and ELM optimizations.  We optimize the hybrid model using resultant vector based on PSO.  Our proposals consider several key factors effecting the location estimation precision, i.e., node density, sensing coverage, and heterogeneous topology. Abstract: Localization is one of the challenges in wireless sensor networks, especially those without the aid of a global positioning system. Use of a dedicated positioning device incurs additional cost and reduces battery life; therefore, a range-free localization scheme is promising as a cost-effective approach. However, the main limitation of this approach is that the estimation precision can be affected by factors such as node density, sensing coverage, and topology diversity. Thus, this study investigates and proposes a method for improving a traditional range-free-based localization method (centroid) that uses soft computing approaches in a hybrid model. This model integrates a fuzzy logic system into centroid and uses an extreme learning machine (ELM) optimization technique to capitalize on the strengths of both approaches: the former is properly used with low node density and short coverage, while the latter is used for the opposite—to achieve a robust location estimation scheme. The ratios of known nodes within the sensing coverage range to the total known nodes and of the sensing coverage range to the maximum coverage range are used as adaptive weights for this hybrid model. To further improve the efficiency, especially in heterogeneous topologies, the concept of resultant force vectors is applied to this hybrid model over particle swarm optimization to mitigate the effects of irregular deployments. The performance of the proposed method is extensively evaluated via simulations that demonstrate its effectiveness compared to other state-of-the-art soft-computing-based range-free localization schemes (i.e., centroid, a fuzzy logic system, and a support vector machine with a traditional ELM).},
author = {Phoemphon, Songyut and So-In, Chakchai and Niyato, Dusit},
doi = {10.1016/j.asoc.2018.01.004},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/phoemphon2018.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Extreme Learning Machine,Fuzzy Logic,Localization,Particle Swarm Optimization,Wireless Sensor Network},
number = {18},
publisher = {Elsevier B.V.},
title = {{A Hybrid Model using Fuzzy Logic and an Extreme Learning Machine with Vector Particle Swarm Optimization for Wireless Sensor Network Localization A Hybrid Model using Fuzzy Logic and an Extreme Learning Machine with Vector Particle Swarm Optimization for }},
url = {https://doi.org/10.1016/j.asoc.2018.01.004},
year = {2018}
}
@article{Preliminaries2016,
author = {Preliminaries, A},
doi = {10.1109/TNNLS.2016.2536649},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/read-later-mixed/vanli2016.pdf:pdf},
issn = {21622388},
pages = {1--13},
title = {{Sequential Nonlinear Learning for Distributed Multiagent Systems via Extreme Learning Machines}},
year = {2016}
}
@article{Qu2016,
abstract = {{\textcopyright} 2014, Springer-Verlag London.Due to the significant efficiency and simple implementation, extreme learning machine (ELM) algorithms enjoy much attention in regression and classification applications recently. Many efforts have been paid to enhance the performance of ELM from both methodology (ELM training strategies) and structure (incremental or pruned ELMs) perspectives. In this paper, a local coupled extreme learning machine (LC-ELM) algorithm is presented. By assigning an address to each hidden node in the input space, LC-ELM introduces a decoupler framework to ELM in order to reduce the complexity of the weight searching space. The activated degree of a hidden node is measured by the membership degree of the similarity between the associated address and the given input. Experimental results confirm that the proposed approach works effectively and generally outperforms the original ELM in both regression and classification applications.},
author = {Qu, Yanpeng},
doi = {10.1007/s00521-013-1542-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/qu2014.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Classification,Extreme learning machine,LC-ELM,Regression},
number = {1},
pages = {27--33},
title = {{Local coupled extreme learning machine}},
volume = {27},
year = {2016}
}
@article{Ranjan2017,
author = {Ranjan, Deepak and Ratnakar, Nayak and Majhi, Banshidhar},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/nayak2017.pdf:pdf},
keywords = {Pathological Brain Detection System (PBDS),Magneti,elm,extreme learning machine,jaya algorithm,magnetic resonance imaging,mri,o-dr2t,orthogonal discrete ripplet-ii transform,pathological brain detection system,pbds},
publisher = {Multimedia Tools and Applications},
title = {{Development of pathological brain detection system using Jaya optimized improved extreme learning machine and orthogonal ripplet-II transform}},
year = {2017}
}
@article{Report2016,
author = {Report, Technical},
doi = {10.13140/2.1.4601.1681},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/read-later-mixed/fatahi2014.pdf:pdf},
number = {November 2014},
title = {{MNIST handwritten digits Description and using}},
year = {2016}
}
@article{Riccardi2014,
abstract = {In this paper, the well known stagewise additive modeling using a multiclass exponential (SAMME) boosting algorithm is extended to address problems where there exists a natural order in the targets using a cost-sensitive approach. The proposed ensemble model uses an extreme learning machine (ELM) model as a base classifier (with the Gaussian kernel and the additional regularization parameter). The closed form of the derived weighted least squares problem is provided, and it is employed to estimate analytically the parameters connecting the hidden layer to the output layer at each iteration of the boosting algorithm. Compared to the state-of-the-art boosting algorithms, in particular those using ELM as base classifier, the suggested technique does not require the generation of a new training dataset at each iteration. The adoption of the weighted least squares formulation of the problem has been presented as an unbiased and alternative approach to the already existing ELM boosting techniques. Moreover, the addition of a cost model for weighting the patterns, according to the order of the targets, enables the classifier to tackle ordinal regression problems further. The proposed method has been validated by an experimental study by comparing it with already existing ensemble methods and ELM techniques for ordinal regression, showing competitive results.},
author = {Riccardi, A and Fernandez-Navarro, F and Carloni, S},
doi = {10.1109/tcyb.2014.2299291},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/riccardi2014.pdf:pdf},
isbn = {2168-2267},
issn = {21682267 (ISSN)},
journal = {Ieee Transactions on Cybernetics},
keywords = {Boosting,CLASSIFICATION,CLASSIFIERS,Computer Science, Artificial Intelligence,Computer Science, Cybernetics,ENSEMBLES,NEGATIVE CORRELATION,NEURAL-NETWORKS,SAMME algorithm,extreme learning machine,neural networks,ordinal regression},
number = {10},
pages = {1898--1909},
pmid = {25222730},
title = {{Cost-Sensitive AdaBoost Algorithm for Ordinal Regression Based on Extreme Learning Machine}},
volume = {44},
year = {2014}
}
@article{Rong2013,
author = {Rong, Hai-Jun and Zhao, Guang-She},
doi = {10.1007/s00521-011-0805-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/rong2012.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {feedforward network {\'{a}} extreme,layer,learning machine {\'{a}},nonlinear systems {\'{a}} single-hidden,sliding mode controller},
number = {3-4},
pages = {577--586},
title = {{Direct adaptive neural control of nonlinear systems with extreme learning machine}},
url = {http://link.springer.com/10.1007/s00521-011-0805-1},
volume = {22},
year = {2013}
}
@article{Sattar2017,
author = {Sattar, Ahmed M. A. and Ertuğrul, {\"{O}}mer Faruk and Gharabaghi, B. and McBean, E. A. and Cao, J.},
doi = {10.1007/s00521-017-2987-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/sattar2017.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {extreme,machine learning,management tool,pipe failure,water pipe network},
title = {{Extreme learning machine model for water network management}},
url = {http://link.springer.com/10.1007/s00521-017-2987-7},
year = {2017}
}
@article{Scardapane2015,
abstract = {The extreme learning machine (ELM) was recently proposed as a unifying framework for different families of learning algorithms. The classical ELM model consists of a linear combination of a fixed number of nonlinear expansions of the input vector. Learning in ELM is hence equivalent to finding the optimal weights that minimize the error on a dataset. The update works in batch mode, either with explicit feature mappings or with implicit mappings defined by kernels. Although an online version has been proposed for the former, no work has been done up to this point for the latter, and whether an efficient learning algorithm for online kernel-based ELM exists remains an open problem. By explicating some connections between nonlinear adaptive filtering and ELM theory, in this brief, we present an algorithm for this task. In particular, we propose a straightforward extension of the well-known kernel recursive least-squares, belonging to the kernel adaptive filtering (KAF) family, to the ELM framework. We call the resulting algorithm the kernel online sequential ELM (KOS-ELM). Moreover, we consider two different criteria used in the KAF field to obtain sparse filters and extend them to our context. We show that KOS-ELM, with their integration, can result in a highly efficient algorithm, both in terms of obtained generalization error and training time. Empirical evaluations demonstrate interesting results on some benchmarking datasets.},
author = {Scardapane, Simone and Comminiello, Danilo and Scarpiniti, Michele and Uncini, Aurelio},
doi = {10.1109/TNNLS.2014.2382094},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/scardapane2015.pdf:pdf},
isbn = {2162-237X},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Extreme learning machine (ELM),kernel,online learning,recursive least square (RLS).},
number = {9},
pages = {2214--2220},
pmid = {25561597},
title = {{Online Sequential Extreme Learning Machine with Kernels}},
volume = {26},
year = {2015}
}
@article{Serrano-Cinca2005,
abstract = {This paper uses Data Envelopment Analysis (DEA), a non-parametric approach to the estimation of production functions, in order to assess efficiency in dot com firms. These firms have two objectives: to make an impact in the Internet and to obtain revenues from their activities. For this reason, the outputs have been two: unique visitors-a web metric-and revenues. DEA efficiencies have been obtained under various input/output combinations. A ranking of dot com firms in terms of relative efficiency has been obtained. A method based on multivariate analysis has been proven to be successful at showing the strengths and weaknesses of individual dot com firms. It is shown that there is a relationship between the type of e-business (e-tailers, search/portal, content/communities), and the way in which efficiency is obtained. The paper suggests a new approach to the problem of deciding which inputs and outputs the model should contain.{\textcopyright} 2003 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1011.1669v3},
author = {Serrano-Cinca, Carlos and Fuertes-Call{\'{e}}n, Yolanda and Mar-Molinero, Cecilio},
doi = {10.1016/j.dss.2003.08.004},
eprint = {1011.1669v3},
file = {:C$\backslash$:/Users/arissetyawan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wahono - 2015 - A Systematic Literature Review of Software Defect Prediction Research Trends , Datasets , Methods and Frameworks(2).pdf:pdf},
isbn = {01679236 (ISSN)},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Data Envelopment Analysis,Dot com,Efficiency; E-business,Inputs/outputs selection,Principal components analysis,Web metrics},
number = {4},
pages = {557--573},
pmid = {27235000},
title = {{Measuring DEA efficiency in Internet companies}},
volume = {38},
year = {2005}
}
@article{Shen2017,
abstract = {Extreme learning machine (ELM) is a simple and effective method of single-hidden layer feedforward neural network. On this basis, there are many other methods are proposed to improve ELM. Weighted extreme learning machine is one of those methods. Weighted ELM is simple in theory and convenient in implementation and it can be applied directly into multiclass classification tasks. This paper improves previous weighted ELM for balance and optimization learning. From the experimental results, the improved weighted ELM obtains the better effects in solving the multiclass classification tasks.},
author = {Shen, Qing and Ban, Xiaojuan and Liu, Ruoyi and Wang, Yu},
doi = {10.1007/s00138-017-0828-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/shen2017.pdf:pdf},
isbn = {9781509012558},
issn = {14321769},
journal = {Machine Vision and Applications},
keywords = {Extreme learning machine,Multiclass classification,Weighted extreme learning machine},
number = {7},
pages = {743--753},
publisher = {Springer Berlin Heidelberg},
title = {{Decay-weighted extreme learning machine for balance and optimization learning}},
volume = {28},
year = {2017}
}
@article{Shukla2015,
abstract = {Extreme learning machine (ELM) is emerged as an effective, fast, and simple solution for real-valued classification problems. Various variants of ELM were recently proposed to enhance the performance of ELM. Circular complex-valued extreme learning machine (CC-ELM), a variant of ELM, exploits the capabilities of complex-valued neuron to achieve better performance. Another variant of ELM, weighted ELM (WELM) handles the class imbalance problem by minimizing a weighted least squares error along with regularization. In this paper, a regularized weighted CC-ELM (RWCC-ELM) is proposed, which incorporates the strength of both CC-ELM and WELM. Proposed RWCC-ELM is evaluated using imbalanced data sets taken from Keel repository. RWCC-ELM outperforms CC-ELM and WELM for most of the evaluated data sets.},
author = {Shukla, San Yam and Yadav, Ram Narayan},
doi = {10.1109/ACCESS.2015.2506601},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/shukla2015.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Class imbalance problem,Complex valued neural network,Extreme learning machine,Real valued classification,Regularization,Weighted least squares error},
pages = {3048--3057},
title = {{Regularized weighted circular complex-valued extreme learning machine for imbalanced learning}},
volume = {3},
year = {2015}
}
@article{Su2017,
author = {Su, Hongjun and Cai, Yue and Du, Qian and Member, Senior},
doi = {10.1109/JSTARS.2016.2591004},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/su2016.pdf:pdf},
issn = {21511535},
number = {1},
pages = {309--320},
title = {{Firefly-Algorithm-Inspired Framework With Band Selection and Extreme Learning Machine for Hyperspectral Image Classification}},
volume = {10},
year = {2017}
}
@article{Suri2015,
abstract = {In this paper, we show for the first time how unavoidable device variability of emerging non-volatile resistive memory devices can be exploited to design efficient low-power, low-footprint Extreme Learning Machine (ELM) architectures. In particular we utilize the uncontrollable Off-state resistance (Roff/HRS) spreads, of nanoscale filamentary- resistive memory devices, to realize random input weights and random hidden neuron biases; a characteristic requirement of ELM. We propose a novel RRAM-ELM architecture. To validate our approach, experimental data from different filamentary- resistive switching devices (CBRAM, OXRAM) is used for full network simulations. Learning capability of our RRAM-ELM architecture is illustrated with the help of two real world applications- (i) diabetes diagnosis test (classification) and (ii) SinC curve fitting (regression).},
author = {Suri, Manan and Parmar, Vivek},
doi = {10.1109/TNANO.2015.2441112},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/suri2015.pdf:pdf},
issn = {1536125X},
journal = {IEEE Transactions on Nanotechnology},
keywords = {Machine learning,Memory architecture,Nanoscale devices,Neuromorphic engineering,Random access memory,Stochastic systems},
number = {6},
pages = {963--968},
title = {{Exploiting Intrinsic Variability of Filamentary Resistive Memory for Extreme Learning Machine Architectures}},
volume = {14},
year = {2015}
}
@article{Tang2015,
abstract = {Ship detection on spaceborne images has attracted great interest in the applications of maritime security and traffic control. Optical images stand out from other remote sensing images in object detection due to their higher resolution and more visualized contents. However, most of the popular techniques for ship detection from optical spaceborne images have two shortcomings: 1) Compared with infrared and synthetic aperture radar images, their results are affected by weather conditions, like clouds and ocean waves, and 2) the higher resolution results in larger data volume, which makes processing more difficult. Most of the previous works mainly focus on solving the first problem by improving segmentation or classification with complicated algorithms. These methods face difficulty in efficiently balancing performance and complexity. In this paper, we propose a ship detection approach to solving the aforementioned two issues using wavelet coefficients extracted from JPEG2000 compressed domain combined with deep neural network (DNN) and extreme learning machine (ELM). Compressed domain is adopted for fast ship candidate extraction, DNN is exploited for high-level feature representation and classification, and ELM is used for efficient feature pooling and decision making. Extensive experiments demonstrate that, in comparison with the existing relevant state-of-the-art approaches, the proposed method requires less detection time and achieves higher detection accuracy.},
author = {Tang, J X and Deng, C W and Huang, G B and Zhao, B J},
doi = {10.1109/tgrs.2014.2335751},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/tang2015.pdf:pdf},
isbn = {0196-2892},
issn = {0196-2892},
journal = {Ieee Transactions on Geoscience and Remote Sensing},
keywords = {(ELM),ALGORITHM,CLASSIFICATION,Compressed domain,Engineering, Electrical {\&} Electronic,Geochemistry {\&} Geophysics,Imaging Science {\&} Photographic Technology,JPEG2000,Remote,SATELLITE IMAGERY,Sensing,TEXTURE FEATURES,TRACKS,deep neural network (DNN),detection,extreme learning machine,optical spaceborne image,remote sensing,ship},
number = {3},
pages = {1174--1185},
title = {{Compressed-Domain Ship Detection on Spaceborne Optical Image Using Deep Neural Network and Extreme Learning Machine}},
volume = {53},
year = {2015}
}
@article{Tang2015a,
abstract = {Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via $\backslash$ell {\_}{\{}1{\}} constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.},
author = {Tang, J and Deng, C and Guang, G-B},
doi = {10.1109/TNNLS.2015.2424995},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/tang2016.pdf:pdf},
isbn = {2162-2388 (Electronic) 2162-237X (Linking)},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {4},
pages = {809--821},
pmid = {25966483},
title = {{Extreme learning machine for multilayer perceptron}},
volume = {27},
year = {2015}
}
@article{Tang2018,
author = {Tang, Xiaofen and Chen, Li},
doi = {10.1007/s10586-018-1808-9},
file = {:C$\backslash$:/Users/arissetyawan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Chen - 2018 - Artificial bee colony optimization-based weighted extreme learning machine for imbalanced data learning.pdf:pdf},
issn = {1386-7857},
journal = {Cluster Computing},
keywords = {Artificial bee colony algorithm,Weighted extreme l,artificial bee colony algorithm,imbalanced data learning,layer feed-forward networks,single hidden,weighted extreme learning machine},
publisher = {Springer US},
title = {{Artificial bee colony optimization-based weighted extreme learning machine for imbalanced data learning}},
url = {http://link.springer.com/10.1007/s10586-018-1808-9},
year = {2018}
}
@article{Tang2018a,
author = {Tang, Xiaofen and Chen, Li},
doi = {10.1007/s13748-017-0136-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/tang2018.pdf:pdf},
issn = {2192-6352},
journal = {Progress in Artificial Intelligence},
keywords = {Weighted extreme learning machine,Imbalanced data,evolutionary algorithm,feed-forward networks,imbalanced data classification,single-hidden-layer,the self-adaptive differential,weighted extreme learning machine},
publisher = {Springer Berlin Heidelberg},
title = {{A self-adaptive evolutionary weighted extreme learning machine for binary imbalance learning}},
url = {http://link.springer.com/10.1007/s13748-017-0136-2},
year = {2018}
}
@article{Ucar2016,
abstract = {In this paper, a novel algorithm is proposed for facial expression recognition by integrating curvelet transform and online sequential extreme learning machine (OSELM) with radial basis function (RBF) hidden node having optimal network architecture. In the proposed algorithm, the curvelet transform is firstly applied to each region of the face image divided into local regions instead of whole face image to reduce the curvelet coefficients too huge to classify. Feature set is then generated by calculating the entropy, the standard deviation and the mean of curvelet coefficients of each region. Finally, spherical clustering (SC) method is employed to the feature set to automatically determine the optimal hidden node number and RBF hidden node parameters of OSELM by aim of increasing classification accuracy and reducing the required time to select the hidden node number. So, the learning machine is called as OSELM-SC. It is constructed two groups of experiments: The aim of the first one is to evaluate the classification performance of OSELM-SC on the benchmark datasets, i.e., image segment, satellite image and DNA. The second one is to test the performance of the proposed facial expression recognition algorithm on the Japanese Female Facial Expression database and the Cohn-Kanade database. The obtained experimental results are compared against the state-of-the-art methods. The results demonstrate that the proposed algorithm can produce effective facial expression features and exhibit good recognition accuracy and robustness. {\&}copy; 2014 Springer-Verlag London.},
author = {U{\c{c}}ar, Ayşeg{\"{u}}l and Demir, Yakup and G{\"{u}}zeliş, C{\"{u}}neyt},
doi = {10.1007/s00521-014-1569-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/uar2014.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Facial expression recognition,Local curvelet transform,Online sequential extreme learning machine,Spherical clustering},
number = {1},
pages = {131--142},
title = {{A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering}},
volume = {27},
year = {2016}
}
@article{Vong2015,
abstract = {Real-time face detection is an important research topic in computer vision and pattern recognition. One of the effective methods in face detection is model-based approach which employs neural network technique for the construction of classification model. Relevant techniques such as support vector machines are fast in training an accurate model which is, however, relatively slow in execution time. The reason is due to the large size of the constructed model. In this paper, the main contribution is to apply a new method called sparse Bayesian extreme learning machine (SBELM) for real-time face detection because SBELM can minimize the model size with nearly no compromise on the accuracy and have fast execution time. Several benchmark face datasets were employed for the evaluation of SBELM against other state-of-the-art techniques. Experimental results show that SBELM achieves fastest execution time with high accuracy over the benchmark face datasets. A MATLAB toolbox of SBELM is also available on our Web site.},
author = {Vong, Chi Man and Tai, Keng Iam and Pun, Chi Man and Wong, Pak Kin},
doi = {10.1007/s00521-014-1803-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/vong2014.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Face detection,Face recognition,Sparse Bayesian},
number = {5},
pages = {1149--1156},
title = {{Fast and accurate face detection by sparse Bayesian extreme learning machine}},
volume = {26},
year = {2015}
}
@article{Wan2017,
abstract = {Extreme learning machine (ELM) is an efficient and effective learning algorithm for pattern classification. For binary classification problem, traditional ELM learns only one hyperplane to separate different classes in the feature space. In this paper, we propose a novel twin extreme learning machine (TELM) to simultaneously train two ELMs with two nonparallel classification hyperplanes. Specifically, TELM first utilizes the random feature mapping mechanism to construct the feature space, and then two nonparallel separating hyperplanes are learned for the final classification. For each hyperplane, TELM jointly minimizes its distance to one class and requires it to be far away from the other class. TELM incorporates the idea of twin support vector machine (TSVM) into the basic framework of ELM, thus TELM could have the advantages of the both algorithms. Moreover, compared to TSVM, TELM has fewer optimization constraint variables but with better classification performance. We also introduce a successive over-relaxation technique to speed up the training of our algorithm. Comprehensive experimental results on a large number of datasets verify the effectiveness and efficiency of TELM.},
author = {Wan, Yihe and Song, Shiji and Huang, Gao and Li, Shuang},
doi = {10.1016/j.neucom.2017.04.036},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/wan2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Nonparallel separating hyperplane,Pattern classification,Twin extreme learning machine,Twin support vector machine},
pages = {235--244},
publisher = {Elsevier B.V.},
title = {{Twin extreme learning machines for pattern classification}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.04.036},
volume = {260},
year = {2017}
}
@article{Wang2015,
abstract = {Extreme Learning Machine (ELM), proposed by Huang et al. in 2004 for the first time, performs better than traditional learning machines such as BP networks and SVM in some applications. This paper attempts to give an oscillation bound of the generalization performance of ELM and a reason why ELM is not sensitive to the number of hidden nodes, which are essential open problems proposed by Huang et al. in 2011. The derivation of the bound is in the framework of statistical learning theory and under the assumption that the expectation of the ELM kernel exists. It turns out that our bound is consistent with the experimental results about ELM obtained before and predicts that overfitting can be avoided even when the number of hidden nodes approaches infinity. The prediction is confirmed by our experiments on 15 data sets using one kind of activation function with every parameter independently drawn from the same Guasssian distribution, which satisfies the assumption above. The experiments also showed that when the number of hidden nodes approaches infinity, the ELM kernel with the activation is insensitive to the kernel parameter.},
author = {Wang, Di and Wang, Ping and Ji, Yan},
doi = {10.1016/j.neucom.2014.10.006},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/wang2015.pdf:pdf},
isbn = {0925-2312},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Generalization performance,Infinite hidden nodes,Oscillation bound,Theoretical research},
number = {P2},
pages = {883--890},
publisher = {Elsevier},
title = {{An oscillation bound of the generalization performance of extreme learning machine and corresponding analysis}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.10.006},
volume = {151},
year = {2015}
}
@article{Wang2016,
author = {Wang, Gai-Ge and Lu, Mei and Dong, Yong-Quan and Zhao, Xiang-Jun},
doi = {10.1007/s00521-015-1874-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/wang2015.pdf:pdf},
isbn = {0941-0643},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {classification {\'{a}} self-adaptive {\'{a}},extreme,learning machine {\'{a}} back,propagation {\'{a}} general regression},
number = {2},
pages = {291--303},
title = {{Self-adaptive extreme learning machine}},
url = {http://link.springer.com/10.1007/s00521-015-1874-3},
volume = {27},
year = {2016}
}
@article{Wang2013,
abstract = {Predicting consumer sentiments revealed in online reviews is crucial to suppliers and potential consumers. We combine online sequential extreme learning machines (OS-ELMs) and intuitionistic fuzzy sets to predict consumer sentiments and propose a generalized ensemble learning scheme. The outputs of OS-ELMs are equivalently transformed into an intuitionistic fuzzy matrix. Then, predictions are made by fusing the degree of membership and non-membership concurrently. Moreover, we implement ELM, OS-ELM, and the proposed fusion scheme for Chinese reviews sentiment prediction. The experimental results have clearly shown the effectiveness of the proposed scheme and the strategy of weighting and order inducing.},
author = {Wang, Hai and Qian, Gang and Feng, Xiang Qian},
doi = {10.1007/s00521-012-0853-1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/wang2013.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Induced aggregation operator,Intuitionistic fuzzy set,OS-ELM,Sentiment prediction,ensemble learning},
number = {3-4},
pages = {479--489},
title = {{Predicting consumer sentiments using online sequential extreme learning machine and intuitionistic fuzzy sets}},
volume = {22},
year = {2013}
}
@article{Wang2017,
abstract = {Due to the fast learning speed, simplicity of implementation and minimal human intervention, extreme learning machine has received considerable attentions recently, mostly from the machine learning community. Generally, extreme learning machine and its various variants focus on classification and regression problems. Its potential application in analyzing censored time-to-event data is yet to be verified. In this study, we present an extreme learning machine ensemble to model right-censored survival data by combining the Buckley-James transformation and the random forest framework. According to experimental and statistical analysis results, we show that the proposed model outperforms popular survival models such as random survival forest, Cox proportional hazard models on well-known low-dimensional and high-dimensional benchmark datasets in terms of both prediction accuracy and time efficiency.},
author = {Wang, Hong and Wang, Jianxin and Zhou, Lifeng},
doi = {10.1007/s10489-017-1063-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/hongwang2017.pdf:pdf},
isbn = {1048901710634},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Buckley-James transformation,Censored data,Extreme learning machine,Survival ensemble,Time-to-event data},
pages = {1--13},
publisher = {Applied Intelligence},
title = {{A survival ensemble of extreme learning machine}},
year = {2017}
}
@article{Wang2017a,
abstract = {Semi-supervised learning has been applied in brain–computer interfaces (BCIs) to reduce calibration time for user. For example, a sequential updated self-training least squares support vector machine (SUST-LSSVM) was devised for online semi-supervised P300 speller. Despite its good performance, the computational complexity becomes too high after several updates, which hinders its practical online application. In this paper, we present a self-training regularized weighted online sequential extreme learning machine (ST-RWOS-ELM) for P300 speller. It achieves much lower complexity compared to SUST-LSSVM without affecting the spelling accuracy performance. The experimental results validate its effectiveness in the P300 system.},
author = {Wang, Junjie and Gu, Zhenghui and Yu, Zhuliang and Li, Yuanqing},
doi = {10.1016/j.neucom.2016.12.098},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/junjiewang2016.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Brain–computer interface,Extreme learning machine,Semi-supervised learning},
pages = {148--151},
publisher = {Elsevier B.V.},
title = {{An online semi-supervised P300 speller based on extreme learning machine}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.12.098},
volume = {269},
year = {2017}
}
@article{Wang2017b,
abstract = {This study proposes a new kernel extreme learning machine (KELM) parameter tuning strategy using a novel swarm intelligence algorithm called grey wolf optimization (GWO). GWO, which simulates the social hierarchy and hunting behavior of grey wolves in nature, is adopted to construct an effective KELM model for bankruptcy prediction. The derived model GWO-KELM is rigorously compared with three competitive KELM methods, which are typical in a comprehensive set of methods including particle swarm optimization-based KELM, genetic algorithm-based KELM, grid-search technique-based KELM, extreme learning machine, improved extreme learning machine, support vector machines and random forest, on two real-life datasets via 10-fold cross validation analysis. Results obtained clearly confirm the superiority of the developed model in terms of classification accuracy (training, validation, test), Type I error, Type II error, area under the receiver operating characteristic curve (AUC) criterion as well as computational time. Therefore, the proposed GWO-KELM prediction model is promising to serve as a powerful early warning tool with excellent performance for bankruptcy prediction.},
author = {Wang, Mingjing and Chen, Huiling and Li, Huaizhong and Cai, Zhennao and Zhao, Xuehua and Tong, Changfei and Li, Jun and Xu, Xin},
doi = {10.1016/j.engappai.2017.05.003},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/mingjingwang2017{\_}2.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Bankruptcy prediction,Grey wolf optimization,Kernel extreme learning machine,Parameter tuning},
pages = {54--68},
publisher = {Elsevier Ltd},
title = {{Grey wolf optimization evolving kernel extreme learning machine: Application to bankruptcy prediction}},
url = {http://dx.doi.org/10.1016/j.engappai.2017.05.003},
volume = {63},
year = {2017}
}
@article{Wang2017c,
abstract = {This study proposes a novel learning scheme for the kernel extreme learning machine (KELM) based on the chaotic moth-flame optimization (CMFO) strategy. In the proposed scheme, CMFO simultaneously performs parameter optimization and feature selection. The proposed methodology is rigorously compared to several other competitive KELM models that are based on the original moth-flame optimization, particle swarm optimization, and genetic algorithms. The comparison is made using the medical diagnosis problems of Parkinson's disease and breast cancer. And the proposed method has successfully been applied to practical medical diagnosis cases. The experimental results demonstrate that, compared to the alternative methods, the proposed method offers significantly better classification performance and also obtains a smaller feature subset. Promisingly, the proposed CMFOFS-KELM, can serve as an effective and efficient computer aided tool for medical diagnosis in the field of medical decision making.},
author = {Wang, Mingjing and Chen, Huiling and Yang, Bo and Zhao, Xuehua and Hu, Lufeng and Cai, Zhen Nao and Huang, Hui and Tong, Changfei},
doi = {10.1016/j.neucom.2017.04.060},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/mingjingwang2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Feature selection,Improved moth-flame optimization,Kernel extreme learning machine,Medical diagnosis,Parameter optimization},
pages = {69--84},
publisher = {Elsevier B.V.},
title = {{Toward an optimal kernel extreme learning machine using a chaotic moth-flame optimization strategy with applications in medical diagnoses}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.04.060},
volume = {267},
year = {2017}
}
@article{Wang2014,
abstract = {Novel constructive and destructive parsimonious extreme learning machines (CP- and DP-ELM) are proposed in this paper. By virtue of the proposed ELMs, parsimonious structure and excellent generalization of multiinput-multioutput single hidden-layer feedforward networks (SLFNs) are obtained. The proposed ELMs are developed by innovative decomposition of the recursive orthogonal least squares procedure into sequential partial orthogonalization (SPO). The salient features of the proposed approaches are as follows: 1) Initial hidden nodes are randomly generated by the ELM methodology and recursively orthogonalized into an upper triangular matrix with dramatic reduction in matrix size; 2) the constructive SPO in the CP-ELM focuses on the partial matrix with the subcolumn of the selected regressor including nonzeros as the first column while the destructive SPO in the DP-ELM operates on the partial matrix including elements determined by the removed regressor; 3) termination criteria for CP- and DP-ELM are simplified by the additional residual error reduction method; and 4) the output weights of the SLFN need not be solved in the model selection procedure and is derived from the final upper triangular equation by backward substitution. Both single- and multi-output real-world regression data sets are used to verify the effectiveness and superiority of the CP- and DP-ELM in terms of parsimonious architecture and generalization accuracy. Innovative applications to nonlinear time-series modeling demonstrate superior identification results.},
author = {Wang, Ning and Er, Meng Joo and Han, Min},
doi = {10.1109/TNNLS.2013.2296048},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/wang2014.pdf:pdf},
isbn = {2012329225},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Extreme learning machine (ELM),parsimonious model selection,recursive orthogonal least squares (ROLS),sequential partial orthogonalization (SPO),single hidden-layer feedforward network (SLFN).},
number = {10},
pages = {1828--1841},
pmid = {25291736},
title = {{Parsimonious extreme learning machine using recursive orthogonal least squares}},
volume = {25},
year = {2014}
}
@article{Wang2017d,
abstract = {A similarity or dissimilarity measure, such as the Euclidean distance, is crucial to discriminative clustering algorithms. These measures used to calculate pairwise similarities between samples rely on data representations in a feature space. However, discriminative clustering fails if the samples in a feature space are linearly inseparable. This problem can be solved by performing a nonlinear data transformation into a high dimensional feature space, which can increase the probability of the linear separability of the samples within the transformed feature space and simplify the associated data structure. Mercer kernels, which are constructed using such a nonlinear data transformation, have been widely used in clustering tasks. Extreme learning machine (ELM) is a new method that exhibits promising clustering performance owing to its universal approximation capability, easy parameter selection, explicit feature mapping process, and excellent feature representation capability. This study proposes an ELM based multi-view learning approach with different views generated by ELM random feature mapping with respect to different hidden-layer nodes, and exploits the properties of these views. Experiments show that better clustering results can be obtained by combining these views together compared with the corresponding ELM-based single-view clustering methods and the traditional algorithms which are performed in the feature space of the original data. Moreover, local kernel alignment property is widespread in these views. This alignment helps the clustering algorithm focus on closer sample pairs. This study also proposes an ELM based multiple kernel clustering algorithm with local kernel alignment maximization. The proposed algorithm is experimentally demonstrated on 10 single-view benchmark datasets and yields superior clustering performance when compared with the state-of-the-art multi-view clustering methods in recent literatures. Thus, the effectiveness and superiority of maximizing local kernel alignment on those views constructed by the proposed method are verified.},
author = {Wang, Qiang and Dou, Yong and Liu, Xinwang and Xia, Fei and Lv, Qi and Yang, Ke},
doi = {10.1016/j.neucom.2017.09.060},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/qianwang2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Local kernel alignment,Multi-view clustering},
title = {{Local kernel alignment based multi-view clustering using extreme learning machine}},
year = {2017}
}
@article{Wang2014a,
abstract = {In this paper, a novel recognition algorithm based on discriminant tensor subspace analysis (DTSA) and extreme learning machine (ELM) is introduced. DTSA treats a gray facial image as a second order tensor and adopts two-sided transformations to reduce dimensionality. One of the many advantages of DTSA is its ability to preserve the spatial structure information of the images. In order to deal with micro-expression video clips, we extend DTSA to a high-order tensor. Discriminative features are generated using DTSA to further enhance the classification performance of ELM classifier. Another notable contribution of the proposed method includes significant improvements in face and micro-expression recognition accuracy. The experimental results on the ORL, Yale, YaleB facial databases and CASME micro-expression database show the effectiveness of the proposed method. 2013 Springer Science+Business Media New York.},
author = {Wang, Su Jing and Chen, Hui Ling and Yan, Wen Jing and Chen, Yu Hsin and Fu, Xiaolan},
doi = {10.1007/s11063-013-9288-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/jinwang2013.pdf:pdf},
isbn = {1370-4621},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Discriminant information,Extreme learning machine,Face recognition,Locality preserving projection,Micro-expression recognition,Tensor subspace},
number = {1},
pages = {25--43},
title = {{Face recognition and micro-expression recognition based on discriminant tensor subspace analysis plus extreme learning machine}},
volume = {39},
year = {2014}
}
@article{Wang2017e,
abstract = {The random assignment strategy for input weights has brought extreme learning machine (ELM) many advantages such as fast learning speed, minimal manual intervention and so on. However, the Monte Carlo (MC) based random sampling method that is typically used to generate input weights of ELM has poor capability of sample structure preserving (SSP), which will degenerate the learning and generalization performance. For this reason, the Quasi-Monte Carlo (QMC) method is revisited and it is shown that the distortion error of QMC projection can obtain faster convergence rate than that of MC for relatively low-dimensional problems. Further, a unified random orthogonal (RO) projection method is proposed, and it is shown that such RO method can always provide the optimal transformation in terms of minimizing the loss of all the distances between samples. Experimental results on real-world benchmark data sets verify the rationality of theoretical analysis and indicate that by enhancing the SSP capability of input weights, QMC and RO projection methods tend to bring ELM algorithms better generalization performance.},
author = {Wang, Wenhui and Liu, Xueyi},
doi = {10.1016/j.neucom.2016.06.079},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/wenhuiwang2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Distance preserving,Extreme learning machine,Generalization performance,Monte Carlo sampling,Orthogonal projection,Quasi-Monte Carlo sequence},
pages = {28--36},
publisher = {Elsevier B.V.},
title = {{The selection of input weights of extreme learning machine: A sample structure preserving point of view}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.06.079},
volume = {261},
year = {2017}
}
@article{Wang2017f,
abstract = {Imbalanced data appear in many real-world applications, from biomedical application to network intrusion or fraud detection, etc. Existing methods for Parkinson's disease (PD) diagnosis are usually more concerned with overall accuracy (ACC), but ignore the classification performance of the minority class. To alleviate the bias against performance caused by imbalanced data, in this paper, an effective method named AABC-KWELM has been proposed for PD detection. First, based on a fast classifier extreme learning machine (ELM), weighted strategy is used for dealing with imbalanced data and non-linear mapping of kernel function is used for improving the extent of linear separation. Furthermore, both binary version and continuous version of an adaptive artificial bee colony (AABC) algorithm are used for performing feature selection and parameters optimization, respectively. Finally, PD data set is used for evaluating rigorously the effectiveness of the proposed method in accordance with specificity, sensitivity, ACC, G-mean and F-measure. Experimental results demonstrate that the proposed AABC-KWELM remarkably outperforms other approaches in the literature and obtains better classification performance via 5-fold cross-validation (CV), with specificity of 100{\%}, sensitivity of 98.62{\%}, ACC of 98.97{\%}, G-mean of 99.30{\%}, and F-measure of 99.30{\%}.},
author = {Wang, Yang and Wang, An Na and Ai, Qing and Sun, Hai Jing},
doi = {10.1016/j.bspc.2017.06.015},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/yangwang2017.pdf:pdf},
issn = {17468108},
journal = {Biomedical Signal Processing and Control},
keywords = {Artificial bee colony,Extreme learning machine,Feature selection,Imbalanced data,Parkinson's disease},
pages = {400--410},
publisher = {Elsevier Ltd},
title = {{An adaptive kernel-based weighted extreme learning machine approach for effective detection of Parkinson's disease}},
url = {http://dx.doi.org/10.1016/j.bspc.2017.06.015},
volume = {38},
year = {2017}
}
@article{Wang2017g,
author = {Wang, Yang and Wang, Anna and Ai, Qing and Sun, Haijing},
doi = {10.1007/s13748-016-0102-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/wang2016.pdf:pdf},
issn = {2192-6352},
journal = {Progress in Artificial Intelligence},
keywords = {Extreme learning machine,Neural networks,Artificia,artificial bee colony,chaotic local search,chaotic opposition-based learning,extreme learning machine,neural networks,self-adaptive search},
number = {1},
pages = {41--52},
publisher = {Springer Berlin Heidelberg},
title = {{A novel artificial bee colony optimization strategy-based extreme learning machine algorithm}},
url = {http://link.springer.com/10.1007/s13748-016-0102-4},
volume = {6},
year = {2017}
}
@article{Wang2011,
abstract = {Extreme learning machine (ELM), proposed by Huang et al., has been shown a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the input weights and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM. {\textcopyright} 2011.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.3924v1},
author = {Wang, Yuguang and Cao, Feilong and Yuan, Yubo},
doi = {10.1016/j.neucom.2010.11.030},
eprint = {arXiv:1409.3924v1},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/survey-papers/wang2011.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Effective extreme learning machine,Extreme learning machine,Feedforward neural networks},
number = {16},
pages = {2483--2490},
publisher = {Elsevier},
title = {{A study on effectiveness of extreme learning machine}},
url = {http://dx.doi.org/10.1016/j.neucom.2010.11.030},
volume = {74},
year = {2011}
}
@article{Wang2016a,
abstract = {Breast tumor detection is a most effective way to immunized against mammary cancer. It is known that the sort algorithm of extreme learning machine(ELM), in view of the feature model for breast X-ray image, is being applied in the computer aided detection of breast masses. On the basis of all these, it is raised in this paper that marking for the suspicious region in the double view mammography by the use of ELM, then classifying the result of double views marking by using the Simple Bias classifier and finally gaining the detection result. The experiment with 444 cases or 222 pair of X-ray mammography from Liao Ning Province Cancer Hospital shows that, the breast tumor detection in double views mammography based on Simple Bias is an available and effective way to detect breast tumor. Key Words: Extreme learning machine, Simple Bias, mammography, double views, tumor detection.},
author = {Wang, Zhiqiong and Qu, Qixun and Yu, Ge and Kang, Yan},
doi = {10.1007/s00521-014-1764-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/wang2014.pdf:pdf},
isbn = {9781467360128},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Computer-aided diagnosis (CAD),Extreme learning machine (ELM),Feature selection,Image processing,Mammograms},
number = {1},
pages = {227--240},
title = {{Breast tumor detection in double views mammography based on extreme learning machine}},
volume = {27},
year = {2016}
}
@article{Wang2017h,
abstract = {The Extreme Learning Machine (ELM) and its variants are effective in many machine learning applications such as Imbalanced Learning (IL) or Big Data (BD) learning. However, they are unable to solve both imbalanced and large-volume data learning problems. This study addresses the IL problem in BD applications. The Distributed and Weighted ELM (DW-ELM) algorithm is proposed, which is based on the MapReduce framework. To confirm the feasibility of parallel computation, first, the fact that matrix multiplication operators are decomposable is illustrated. Then, to further improve the computational efficiency, an Improved DW-ELM algorithm (IDW-ELM) is developed using only one MapReduce job. The successful operations of the proposed DW-ELM and IDW-ELM algorithms are finally validated through experiments.},
author = {Wang, Zhiqiong and Xin, Junchang and Yang, Hongxu and Tian, Shuo and Yu, Ge and Xu, Chenren and Yao, Yudong},
doi = {10.23919/TST.2017.7889638},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/wang2017.pdf:pdf},
issn = {10070214},
journal = {Tsinghua Science and Technology},
keywords = {weighted Extreme Learning Machine (ELM); imbalance},
number = {2},
pages = {160--173},
title = {{Distributed and weighted extreme learning machine for imbalanced big data learning}},
volume = {22},
year = {2017}
}
@article{Wen2017,
abstract = {In this paper, a hybrid structure-adaptive radial basis function-extreme learning machine (HSARBF-ELM) network classifier is presented. HSARBF-ELM consists of a structure-adaptive radial basis function (SARBF) network and an extreme learning machine (ELM) network of cascade, where the output of the SARBF network hidden layer is used as the input layer of the ELM network. In the HSARBF-ELM network classifier, the SARBF network is utilized to achieve adaptively localizing kernel mapping of input vectors, after that step, the ELM network is utilized to implement global classification of mapping samples in the kernel space. HSARBF-ELM indicates the combination of localized kernel mapping learning and the global nonlinear classification, which combines the advantages of the SARBF network and the ELM network. The quantitative conditions for the separability enhancement and the corresponding theoretical explanation for the HSARBF-ELM network are given, which demonstrate that when input vectors go through the SARBF network, adaptively adjusting the RBF kernel parameters can boost the separability of the original sample space. Thus, the classification performance of the HSARBF-ELM network can be guaranteed theoretically. An appropriate learning algorithm for the HSARBF-ELM network is subsequently presented, which effectively combines the methods of density clustering with a potential function, center-oriented unidirectional repulsive force and the existing ELM algorithm, and the optimized complementary HSARBF-ELM network can be constructed. The experimental results show that the classification performance of the HSARBF-ELM network clearly outperforms the ELM network, and outperforms other classifiers on most classification problems. INDEX TERMS Extreme learning machine (ELM), radial basis function (RBF), density clustering, hybrid, structure-adaptive, separability analysis, neural network.},
author = {Wen, Hui and Fan, Hongguang and Xie, Weixin and Pei, Jihong},
doi = {10.1109/ACCESS.2017.2740420},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/read-later-mixed/wen2017.pdf:pdf},
title = {{SPECIAL SECTION ON LEARNING SYSTEMS BASED CONTROL AND OPTIMIZATION OF COMPLEX NONLINEAR SYSTEMS Hybrid Structure-Adaptive RBF-ELM Network Classifier}},
year = {2017}
}
@article{Wong2017,
abstract = {An online extreme learning machine (ELM) based modeling and optimization approach for point-by-point engine calibration is proposed to improve the efficiency of conventional model-based calibration approach. Instead of building hundreds of local engine models for every engine operating point, only one ELM model is necessary for the whole process. This ELM model is firstly constructed for a starting operating point, and calibration of this starting point is conducted by determining the optimal parameters of the model. This ELM model is then re-used as a base model for a nearby target operating point, and optimization is performed on the model to search for its best parameters. With a design of experiment strategy on the best parameters obtained, new measurements from the target operating point can be collected and used to update the model. By repeating the optimization and model update procedures, the optimal parameters for the target point can be found after several iterations. By using the model of this target point as the base model for another nearby operating point and repeating the same process again, calibration for all the operating points can be done online efficiently. The contribution of the proposed method is to save the number of experiments in the calibration process. To verify the effectiveness of the proposed approach, experiments on a commercial engine simulation software have been conducted. Three variants of online ELM are utilized in the model update process for comparison. The results show that engine calibration can be carried out with much fewer measurements and time using the proposed approach, and the initial training free online ELM is the most efficient online modeling method for this application.},
author = {Wong, Pak Kin and Gao, Xiang Hui and Wong, Ka In and Vong, Chi Man},
doi = {10.1016/j.neucom.2017.02.104},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/wong2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Engine calibration,Engine modeling,Engine optimization,Initial-training-free online extreme learning mach},
publisher = {Elsevier B.V.},
title = {{Online extreme learning machine based modeling and optimization for point-by-point engine calibration}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.02.104},
year = {2017}
}
@article{Wong2016,
abstract = {Air-ratio is an important engine parameter that relates closely to engine emissions, power, and brake-specific fuel consumption. Model predictive controller (MPC) is a well-known technique for air-ratio control. This paper utilizes an advanced modelling technique, called online sequential extreme learning machine (OSELM), to develop an online sequential extreme learning machine MPC (OEMPC) for air-ratio regulation according to various engine loads. The proposed OEMPC was implemented on a real engine to verify its effectiveness. Its control performance is also compared with the latest MPC for engine air-ratio control, namely diagonal recurrent neural network MPC, and conventional proportional-integral-derivative (PID) controller. Experimental results show the superiority of the proposed OEMPC over the other two controllers, which can more effectively regulate the air-ratio to specific target values under external disturbance. Therefore, the proposed OEMPC is a promising scheme to replace conventional PID controller for engine air-ratio control.},
author = {Wong, Pak Kin and Wong, Hang Cheong and Vong, Chi Man and Xie, Zhengchao and Huang, Shaojia},
doi = {10.1007/s00521-014-1555-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/kinwong2014.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Air-ratio,Automotive engine,Nonlinear model predictive control,Online sequential extreme learning machine},
number = {1},
pages = {79--92},
title = {{Model predictive engine air-ratio control using online sequential extreme learning machine}},
volume = {27},
year = {2016}
}
@article{Wong2015,
author = {Wong, Shen Yuong and Yap, Keem Siah and Yap, Hwa Jen and Tan, Shing Chiang},
doi = {10.1007/s11063-014-9374-5},
file = {:C$\backslash$:/Users/arissetyawan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong et al. - 2015 - A Truly Online Learning Algorithm using Hybrid Fuzzy ARTMAP and Online Extreme Learning Machine for Pattern Classif.pdf:pdf;:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/wong2014.pdf:pdf},
isbn = {1106301493745},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Fuzzy ARTMAP (FAM),Online learning,Online sequential extreme learning machine (OSELM),Pattern classification},
number = {3},
pages = {585--602},
title = {{A Truly Online Learning Algorithm using Hybrid Fuzzy ARTMAP and Online Extreme Learning Machine for Pattern Classification}},
volume = {42},
year = {2015}
}
@article{Wu2017,
abstract = {{\textcopyright} 2017, Springer Science+Business Media New York.Extreme learning machine (ELM) is a learning algorithm for generalized single-hidden-layer feed-forward networks (SLFNs). In order to obtain a suitable network architecture, Incremental Extreme Learning Machine (I-ELM) is a sort of ELM constructing SLFNs by adding hidden nodes one by one. Although kinds of I-ELM-class algorithms were proposed to improve the convergence rate or to obtain minimal training error, they do not change the construction way of I-ELM or face the over-fitting risk. Making the testing error converge quickly and stably therefore becomes an important issue. In this paper, we proposed a new incremental ELM which is referred to as Length-Changeable Incremental Extreme Learning Machine (LCI-ELM). It allows more than one hidden node to be added to the network and the existing network will be regarded as a whole in output weights tuning. The output weights of newly added hidden nodes are determined using a partial error-minimizing method. We prove that an SLFN constructed using LCI-ELM has approximation capability on a universal compact input set as well as on a finite training set. Experimental results demonstrate that LCI-ELM achieves higher convergence rate as well as lower over-fitting risk than some competitive I-ELM-class algorithms.},
author = {Wu, You Xi and Liu, Dong and Jiang, He},
doi = {10.1007/s11390-017-1746-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/wu2017.pdf:pdf},
issn = {18604749},
journal = {Journal of Computer Science and Technology},
keywords = {convergence rate,incremental extreme learning machine (I-ELM),random hidden node,single-hidden-layer feed-forward network (SLFN),universal approximation},
number = {3},
pages = {630--643},
title = {{Length-Changeable Incremental Extreme Learning Machine}},
volume = {32},
year = {2017}
}
@article{Xiao-jian2017,
abstract = {The problem of choosing error penalty parameter C for optimization extreme learning machine (OELM) is that it can take any positive value for different applications and it is therefore hard to choose correctly. In this paper, we reformulated OELM to take a new regularization parameter $\nu$ ($\nu$-OELM) which is inspired by Sch{\"{o}}lkopf et al. The regularization in terms of $\nu$ is bounded between 0 and 1, and is easier to interpret as compared to C. This paper shows that: (1) $\nu$-OELM and $\nu$-SVM have similar dual optimization formulation, but $\nu$-OELM has less optimization constraints due to its special capability of class separation and (2) experiment results on both artificial and real binary classification problems show that $\nu$-OELM tends to achieve better generalization performance than $\nu$-SVM, OELM and other popular machine learning approaches, and it is computationally efficient on high dimension data sets. Additionally, the optimal parameter $\nu$ in $\nu$-OELM can be easily selected from few candidates.},
author = {Xiao-jian, Ding and Yuan, Lan and Zhi-feng, Zhang and Xin, Xu},
doi = {10.1016/j.neucom.2016.05.114},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/xiaojian2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Classification,Parameter selection,$\nu$-optimization extreme learning machine},
pages = {11--19},
publisher = {Elsevier B.V.},
title = {{Optimization extreme learning machine with $\nu$ regularization}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.05.114},
volume = {261},
year = {2017}
}
@article{Xie2016,
author = {Xie, Peidai and Liu, Xinwang and Yin, Jianping and Wang, Yongjun},
doi = {10.1007/s00521-014-1558-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xie2014.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Absent learning,Extreme learning machine,Malware,Packed executable identification},
number = {1},
pages = {93--100},
title = {{Absent extreme learning machine algorithm with application to packed executable identification}},
volume = {27},
year = {2016}
}
@article{Xin2014,
abstract = {Extreme Learning Machine (ELM) has been widely used in many fields such as text classification, image recognition and bioinformatics, as it provides good generalization performance at a extremely fast learning speed. However, as the data volume in real-world applications becomes larger and larger, the traditional centralized ELM cannot learn such massive data efficiently. Therefore, in this paper, we propose a novel Distributed Extreme Learning Machine based on MapReduce framework, named ELM¿¿¿, which can cover the shortage of traditional ELM whose learning ability is weak to huge dataset. Firstly, after adequately analyzing the property of traditional ELM, it can be found out that the most expensive computation part of the matrix Moore-Penrose generalized inverse operator in the output weight vector calculation is the matrix multiplication operator. Then, as the matrix multiplication operator is decomposable, a Distributed Extreme Learning Machine (ELM¿¿¿) based on MapReduce framework can be developed, which can first calculate the matrix multiplication effectively with MapReduce in parallel, and then calculate the corresponding output weight vector with centralized computing. Therefore, the learning of massive data can be made effectively. Finally, we conduct extensive experiments on synthetic data to verify the effectiveness and efficiency of our proposed ELM¿¿¿ in learning massive data with various experimental settings.},
author = {Xin, Junchang and Wang, Zhiqiong and Chen, Chen and Ding, Linlin and Wang, Guoren and Zhao, Yuhai},
doi = {10.1007/s11280-013-0236-2},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xin2013.pdf:pdf},
isbn = {1386-145X},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Cloud computing,Extreme learning machine,MapReduce,Massive data processing},
number = {5},
pages = {1189--1204},
title = {{ELM∗: distributed extreme learning machine with MapReduce}},
volume = {17},
year = {2014}
}
@article{Xing2013,
abstract = {In this paper, a regularized correntropy criterion (RCC) for extreme learning machine (ELM) is proposed to deal with the training set with noises or outliers. In RCC, the Gaussian kernel function is utilized to substitute Euclidean norm of the mean square error (MSE) criterion. Replacing MSE by RCC can enhance the anti-noise ability of ELM. Moreover, the optimal weights connecting the hidden and output layers together with the optimal bias terms can be promptly obtained by the half-quadratic (HQ) optimization technique with an iterative manner. Experimental results on the four synthetic data sets and the fourteen benchmark data sets demonstrate that the proposed method is superior to the traditional ELM and the regularized ELM both trained by the MSE criterion.},
author = {Xing, Hong Jie and Wang, Xin Mei},
doi = {10.1007/s00521-012-1184-y},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xing2012.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Correntropy,Extreme learning machine,Half-quadratic optimization,Regularization term},
number = {7-8},
pages = {1977--1986},
title = {{Training extreme learning machine via regularized correntropy criterion}},
volume = {23},
year = {2013}
}
@article{Xu2016,
abstract = {A novel multispectral palmprint recognition method is proposed based on multiclass projection extreme learning machine (MPELM) and digital shearlet transform. Extreme learning machine (ELM) is a novel and efficient learning machine based on the generalized single-hidden-layer feedforward networks, which performs well in classification applications. Many researchers' experimental results have shown the superiority of ELM with classical algorithm: support vector machine (SVM). To further improve the performance of multispectral palmprint recognition method, we propose a novel method based on MPELM in this paper. Firstly, all palmprint images are preprocessed by David Zhang's method. Then, we use image fusion method based on fast digital shearlet transform to fuse the multispectral palmprint images. At last, we use the proposed MPELM classifier to determine the final multispectral palmprint classification. The experimental results demonstrate the superiority of multispectral fusion to each single spectrum, and the proposed MPELM-based method outperforms the SVM-based and ELM-based methods. The proposed method is also suitable for other biometric applications and gets to be work well. {\textcopyright} 2014 Springer-Verlag London.},
author = {Xu, Xuebin and Lu, Longbin and Zhang, Xinman and Lu, Huimin and Deng, Wanyu},
doi = {10.1007/s00521-014-1570-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xu2014.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Digital shearlet transform,Extreme learning machine,Image fusion,Multispectral palmprint},
number = {1},
pages = {143--153},
title = {{Multispectral palmprint recognition using multiclass projection extreme learning machine and digital shearlet transform}},
volume = {27},
year = {2016}
}
@article{Xu2011,
abstract = {In recent years, computational intelligence and machine learning techniques have gained popularity to facilitate very fast dynamic security assessment for earlier detection of the risk of blackouts. However, many of the current state-of-the-art models usually suffer from excessive training time and complex parameters tuning problems, leading to inefficiency for real-time implementation and on-line model updating. In this study, a new transient stability assessment model using the increasingly prevalent extreme learning machine theory is developed. It has significantly improved the learning speed and can enable effective on-line updating. The proposed model is examined on the New England 39-bus test system, and compared with some state-of-the-art methods in terms of computation time and prediction accuracy. The simulation results show that the proposed model possesses significant superior computation speed and competitively high accuracy.},
author = {Xu, Y. and Dong, Z.Y. and Meng, K. and Zhang, R. and Wong, K.P.},
doi = {10.1049/iet-gtd.2010.0355},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/xu2011.pdf:pdf},
issn = {17518687},
journal = {IET Generation, Transmission {\&} Distribution},
number = {3},
pages = {314},
title = {{Real-time transient stability assessment model using extreme learning machine}},
url = {http://digital-library.theiet.org/content/journals/10.1049/iet-gtd.2010.0355},
volume = {5},
year = {2011}
}
@article{Xu2013,
abstract = {As a novel and promising learning technology, extreme learning machine (ELM) is featured by its much faster training speed and better generalization performance over traditional learning techniques. ELM has found applications in solving many real-world engineering problems, including those in electric power systems. Maintaining frequency stability is one of the essential requirements for secure and reliable operations of a power system. Conventionally, its assessment involves solving a large set of nonlinear differential-algebraic equations, which is very time-consuming and can be only carried out off-line. This paper firstly reviews the ELM's applications in power engineering and then develops an ELM-based predictor for real-time frequency stability assessment (FSA) of power systems. The inputs of the predictor are power system operational parameters, and the output is the frequency stability margin that measures the stability degree of the power system subject to a contingency. By off-line training with a frequency stability database, the predictor can be online applied for real-time FSA. Benefiting from the very fast speed of ELM, the predictor can be online updated for enhanced robustness and reliability. The developed predictor is examined on the New England 10-generator 39-bus test system, and the simulation results show that it can exactly (within acceptable errors) and rapidly (within very small computing time) predict the frequency stability. {\textcopyright} 2012 Springer-Verlag London Limited.},
author = {Xu, Yan and Dai, Yuanyu and Dong, Zhao Yang and Zhang, Rui and Meng, Ke},
doi = {10.1007/s00521-011-0803-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xu2012.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine (ELM),Frequency stability,Power system},
number = {3-4},
pages = {501--508},
title = {{Extreme learning machine-based predictor for real-time frequency stability assessment of electric power systems}},
volume = {22},
year = {2013}
}
@article{Xue2017,
author = {Xue, Xiaowei and Yao, Min and Wu, Zhaohui},
doi = {10.1007/s10115-017-1131-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xue2017.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {Feature selection,Genetic algorithm,Extreme learni,ensemble,extreme learning machine,feature selection,genetic algorithm},
publisher = {Springer London},
title = {{A novel ensemble-based wrapper method for feature selection using extreme learning machine and genetic algorithm}},
url = {http://link.springer.com/10.1007/s10115-017-1131-4},
year = {2017}
}
@article{Yan2017,
author = {Yan, Deqin and Chu, Yonghe and Li, Lina and Liu, Deshan},
doi = {10.1007/s11042-017-4494-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/yan2017.pdf:pdf},
isbn = {1104201744943},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {Extreme learning machine,Pattern recognition,Hyper,discrimination information,extreme learning machine,hyperspectral remote sensing,image,pattern recognition},
publisher = {Multimedia Tools and Applications},
title = {{Hyperspectral remote sensing image classification with information discriminative extreme learning machine}},
url = {http://link.springer.com/10.1007/s11042-017-4494-3},
year = {2017}
}
@article{Yang2016,
author = {Yang, Dakun and Li, Zhengxue and Wu, Wei},
doi = {10.1007/s00521-013-1519-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/yang2013.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Interval computation,Interval extreme learning machine,Interval neural network},
number = {1},
pages = {3--8},
title = {{Extreme learning machine for interval neural networks}},
volume = {27},
year = {2016}
}
@article{Yang2017,
abstract = {As real industrial processes have measurement samples with noises of different statistical characteristics and obtain the sample one by one usually, on-line sequential learning algorithms which can achieve better learning performance for systems with noises of various statistics are necessary. This paper proposes a new online Extreme Learning Machine (ELM, of Huang et al.) algorithm, namely recursive least mean p-power ELM (RLMP-ELM). In RLMP-ELM, a novel error criterion for cost function, namely the least mean p-power (LMP) error criterion, provides a mechanism to update the output weights sequentially. The LMP error criterion aims to minimize the mean p-power of the error that is the generalization of the mean square error criterion used in the ELM. The proposed on-line learning algorithm is able to provide on-line predictions of variables with noises of different statistics and obtains better performance than ELM and online sequential ELM (OS-ELM) while the non-Gaussian noises impact the processes. Simulations are reported to demonstrate the performance and effectiveness of the proposed methods.},
author = {Yang, Jing and Ye, Feng and Rong, Hai Jun and Chen, Badong},
doi = {10.1016/j.neunet.2017.04.001},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/yang2017{\_}1.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Alpha-stable noises,Extreme learning machine,Non-Gaussian noises,Online sequential learning,Recursive least mean p-power},
pages = {22--33},
publisher = {Elsevier Ltd},
title = {{Recursive least mean p-power Extreme Learning Machine}},
url = {http://dx.doi.org/10.1016/j.neunet.2017.04.001},
volume = {91},
year = {2017}
}
@article{Yang2013,
author = {Yang, Jucheng and Xie, Shanjuan and Yoon, Sook and Park, Dongsun and Fang, Zhijun and Yang, Shouyuan},
doi = {10.1007/s00521-011-0806-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/juchengyang2012.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Fingerprint matching,Invariant moments,Regularized},
number = {3-4},
pages = {435--445},
title = {{Fingerprint matching based on extreme learning machine}},
volume = {22},
year = {2013}
}
@article{Yang2018,
abstract = {{\textcopyright} 2016 The Natural Computing Applications Forum The interval extreme learning machine (IELM) (Yang et al. in Neural Comput Appl 27(1):3–8, 2016) is a newly proposed regression algorithm to deal with the data with interval-valued inputs and interval-valued output. In this paper, we firstly analyze the disadvantages of IELM and further point out that IELM is actually a slight variant of fuzzy regression analysis using neural networks (Ishibuchi and Tanaka in Fuzzy Sets Syst 50(3):257–265, 1992). Then, we propose a new interval-valued ELM (IVELM) model to handle the interval-valued data regression. IVELM does not require any iterative adjustment to network weights and thus has the extremely fast training speed. The experimental results on data sets used in (Yang et al. 2016) demonstrate the feasibility and effectiveness of IVELM which obtains the better predictive performance and faster learning speed than IELM.},
author = {fen Yang, Li and Liu, Chong and Long, Hao and Ashfaq, Rana Aamir Raza and lin He, Yu},
doi = {10.1007/s00521-016-2727-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/yang2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {ELM,Generalized inverse,Interval ELM,Interval-valued data},
number = {8},
pages = {311--318},
publisher = {Springer London},
title = {{Further improvements on extreme learning machine for interval neural network}},
volume = {29},
year = {2018}
}
@article{Yang2012,
abstract = {It is clear that the learning effectiveness and learning speed of neural networks are in general far slower than required, which has been a major bottleneck for many applications. Recently, a simple and efficient learning method, referred to as extreme learning machine (ELM), was proposed by Huang , which has shown that, compared to some conventional methods, the training time of neural networks can be reduced by a thousand times. However, one of the open problems in ELM research is whether the number of hidden nodes can be further reduced without affecting learning effectiveness. This brief proposes a new learning algorithm, called bidirectional extreme learning machine (B-ELM), in which some hidden nodes are not randomly selected. In theory, this algorithm tends to reduce network output error to 0 at an extremely early learning stage. Furthermore, we find a relationship between the network output error and the network output weights in the proposed B-ELM. Simulation results demonstrate that the proposed method can be tens to hundreds of times faster than other incremental ELM algorithms.},
author = {Yang, Yimin and Wang, Yaonan and Yuan, Xiaofang},
doi = {10.1109/TNNLS.2012.2202289},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/yang2012.pdf:pdf},
isbn = {2162-237X},
issn = {2162237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Feedforward neural network,learning effectiveness,number of hidden nodes,universal approximation},
number = {9},
pages = {1498--1505},
pmid = {24807932},
title = {{Bidirectional extreme learning machine for regression problem and its learning effectiveness}},
volume = {23},
year = {2012}
}
@article{Yang2013a,
author = {Yang, Yimin and Wang, Yaonan and Yuan, Xiaofang},
doi = {10.1007/s11063-012-9246-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/yang2012.pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Chaos optimization algorithm,Convergence rate,Extreme learning machine,Random hidden nodes},
number = {3},
pages = {277--301},
title = {{Parallel chaos search based incremental extreme learning machine}},
volume = {37},
year = {2013}
}
@article{Yang2016a,
abstract = {As demonstrated earlier, the learning effectiveness and learning speed of single-hidden-layer feedforward neural networks are in general far slower than required, which has been a major bottleneck for many applications. Huang et al. proposed extreme learning machine (ELM) which improves the training speed by hundreds of times as compared to its predecessor learning techniques. This paper offers an ELM-based learning method that can grow subnetwork hidden nodes by pulling back residual network error to the hidden layer. Furthermore, the proposed method provides a similar or better generalization performance with remarkably fewer hidden nodes as compared to other ELM methods employing huge number of hidden nodes. Thus, the learning speed of the proposed technique is hundred times faster compared to other ELMs as well as to back propagation and support vector machines. The experimental validations for all methods are carried out on 32 data sets.},
author = {Yang, Yimin and Wu, Q. M.Jonathan},
doi = {10.1109/TCYB.2015.2492468},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/yang2015.pdf:pdf},
isbn = {2168-2267},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Extreme learning machine (ELM),feedforward neural network (FNN),image recognition,learning effectiveness,universal approximation},
number = {12},
pages = {2885--2898},
pmid = {26552104},
title = {{Extreme Learning Machine with Subnetwork Hidden Nodes for Regression and Classification}},
volume = {46},
year = {2016}
}
@article{Yang2017a,
abstract = {Recently, Extreme learning machine (ELM), an efficient training algorithm for single-hidden-layer feedforward neural networks (SLFN), has gained increasing popularity in machine learning communities. In this paper the ELM based Area Under the ROC Curve (AUC) optimization algorithms are studied so as to further improve the performance of ELM for imbalanced datasets. For binary class problems, a novel ELM algorithm is proposed based on an efficient least square method. For multi-class problems, the following works are done in this paper: First of all, theoretical comparison analysis is proposed for the potential multi-class extensions of AUC; Secondly, a unified objective function for multi-class AUC optimization is proposed following the theoretical analysis; Subsequently, two ELM based multi-class AUC optimization algorithms called ELMMAUC and ELMmacroAUC respectively are proposed followed with complexity analyses; Finally, the generalization analysis is established for ELMMAUC in search of theoretical supports. Empirical study on a variety of real-world datasets show the effectiveness of our proposed algorithms.},
author = {Yang, Zhiyong and Zhang, Taohong and Lu, Jingcheng and Zhang, Dezheng and Kalui, Dorothy},
doi = {10.1016/j.knosys.2017.05.013},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/yang2017.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Area under the ROC curve (AUC),Extreme learning machine (ELM),Imbalanced datasets,Multi-class AUC optimization},
pages = {74--89},
publisher = {Elsevier B.V.},
title = {{Optimizing area under the ROC curve via extreme learning machines}},
url = {http://dx.doi.org/10.1016/j.knosys.2017.05.013},
volume = {130},
year = {2017}
}
@article{Yao2016,
abstract = {In this paper, we describe a compact low-power, high performance hardware implementation of the extreme learning machine (ELM) for machine learning applications. Mismatch in current mirrors are used to perform the vector-matrix multiplication that forms the first stage of this classifier and is the most computationally intensive. Both regression and classification (on UCI data sets) are demonstrated and a design space trade-off between speed, power and accuracy is explored. Our results indicate that for a wide set of problems, {\$}\backslashsigma V{\_}T{\$} in the range of {\$}15-25{\$}mV gives optimal results. An input weight matrix rotation method to extend the input dimension and hidden layer size beyond the physical limits imposed by the chip is also described. This allows us to overcome a major limit imposed on most hardware machine learners. The chip is implemented in a {\$}0.35 \backslashmu{\$}m CMOS process and occupies a die area of around 5 mm {\$}\backslashtimes{\$} 5 mm. Operating from a {\$}1{\$} V power supply, it achieves an energy efficiency of {\$}0.47{\$} pJ/MAC at a classification rate of {\$}31.6{\$} kHz.},
archivePrefix = {arXiv},
arxivId = {1605.00740},
author = {Yao, Enyi and Basu, Arindam},
doi = {10.1109/TVLSI.2016.2558842},
eprint = {1605.00740},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/yao2016.pdf:pdf},
issn = {1063-8210},
journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
pages = {1--15},
title = {{VLSI Extreme Learning Machine: A Design Space Exploration}},
url = {http://arxiv.org/abs/1605.00740{\%}5Cnhttp://ieeexplore.ieee.org/document/7470473/},
year = {2016}
}
@article{Yin2018,
abstract = {The electroencephalography (EEG) based machine-learning model for mental fatigue recognition can evaluate the reliability of the human operator performance. The task-generic model is particularly important since the time cost for preparing the task-specific training EEG dataset is avoid. This study develops a novel mental fatigue classifier, dynamical deep extreme learning machine (DD-ELM), to adapt the variation of the EEG feature distributions across two mental tasks. Different from the static deep learning approaches, DD-ELM iteratively updates the shallow weights at multiple time steps during the testing stage. The proposed method incorporates the both of the merits from the deep network for EEG feature abstraction and the ELM autoencoder for fast weight recompuation. The feasibility of the DD-ELM is validated by investigating EEG datasets recorded under two paradigms of AutoCAMS human-machine tasks. The accuracy comparison indicates the new classifier significantly outperforms several state-of-the-art mental fatigue estimators. By examining the CPU time, the computational burden of the DD-ELM is also acceptable for high-dimensional EEG features.},
author = {Yin, Zhong and Zhang, Jianhua},
doi = {10.1016/j.neucom.2017.12.062},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/yin2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep learning,Electroencephalography,Extreme learning machine,Human-machine system,Mental fatigue},
pages = {266--281},
publisher = {Elsevier B.V.},
title = {{Task-generic mental fatigue recognition based on neurophysiological signals and dynamical deep extreme learning machine}},
url = {https://doi.org/10.1016/j.neucom.2017.12.062},
volume = {283},
year = {2018}
}
@article{Ying2016,
abstract = {Single-hidden-layer feedforward networks with randomly generated additive or radial basis function hidden nodes have been theoretically proved that they can approximate any continuous function. Meanwhile, an incremental algorithm referred to as incremental extreme learning machine (I-ELM) was proposed which outperforms many popular learning algorithms. However, I-ELM may produce redundant nodes which increase the network architecture complexity and reduce the convergence rate of I-ELM. Moreover, the output weight vector obtained by I-ELM is not the least squares solution of equation H$\beta$ = T. In order to settle these problems, this paper proposes an orthogonal incremental extreme learning machine (OI-ELM) and gives the rigorous proofs in theory. OI-ELM avoids redundant nodes and obtains the least squares solution of equation H$\beta$ = T through incorporating the Gram–Schmidt orthogonalization method into I-ELM. Simulation results on nonlinear dynamic system identification and some benchmark real-world problems verify that OI-ELM learns much faster and obtains much more compact neural networks than ELM, I-ELM, convex I-ELM and enhanced I-ELM while keeping competitive performance.},
author = {Ying, Li},
doi = {10.1007/s00521-014-1567-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/ying2014.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Convergence rate,Feedforward neural networks,Gram???Schmidt orthogonalization method,Incremental extreme learning machine,Least squares solution},
number = {1},
pages = {111--120},
title = {{Orthogonal incremental extreme learning machine for regression and multiclass classification}},
volume = {27},
year = {2016}
}
@article{Yu2015,
abstract = {Identification and classification of graph data is a hot research issue in pattern recognition. The conventional methods of graph classification usually convert the graph data to the vector representation and then using SVM to be a classifier. These methods ignore the sparsity of graph data, and with the increase of the input sample, the storage and computation of the kernel matrix will cost a lot of memory and time. In this paper, we propose a new graph classification algorithm called graph classification based on sparse graph feature selection and extreme learning machine. The key of our method is using the lasso to select features because of the sparsity of graph data, and extreme learning machine (ELM) is introduced to the following classification task due to its good performance. Extensive experimental results on a series of benchmark graph data sets validate the effectiveness of the proposed methods.},
author = {Yu, Yajun and Pan, Zhisong and Hu, Guyu and Ren, Huifeng},
doi = {10.1016/j.neucom.2016.03.110},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/yajunyu2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Graph classification,Graph kernel,Lasso},
publisher = {Elsevier B.V.},
title = {{Graph classification based on sparse graph feature selection and extreme learning machine}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.03.110},
year = {2015}
}
@article{Yu2017,
abstract = {As one of supervised learning algorithms, extreme learning machine (ELM) has been proposed for training single-hidden-layer feedforward neural networks and shown great generalization performance. ELM randomly assigns the weights and biases between input and hidden layers and only learns the weights between hidden and output layers. Physiological research has shown that neurons at the same layer are laterally inhibited to each other such that outputs of each layer are sparse. However, it is difficult for ELM to accommodate the lateral inhibition by directly using random feature mapping. Therefore, this paper proposes a sparse coding ELM (ScELM) algorithm, which can map the input feature vector into a sparse representation. In this proposed ScELM algorithm, an unsupervised way is used for sparse coding and dictionary is randomly assigned rather than learned. Gradient projection based method is used for the sparse coding. The output weights are trained through the same supervised way as ELM. Experimental results on the benchmark datasets have shown that this proposed ScELM algorithm can outperform other state-of-the-art methods in terms of classification accuracy.},
author = {Yu, Yuanlong and Sun, Zhenzhen},
doi = {10.1016/j.neucom.2016.06.078},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/yu2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Gradient projection,Sparse coding},
pages = {50--56},
publisher = {Elsevier B.V.},
title = {{Sparse coding extreme learning machine for classification}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.06.078},
volume = {261},
year = {2017}
}
@article{Zeng2017,
abstract = {In this paper, a hybrid learning approach, which combines the extreme learning machine (ELM) with a new switching delayed PSO (SDPSO) algorithm, is proposed for the problem of the short-term load forecasting (STLF). In particular, the input weights and biases of ELM are optimized by a new developed SDPSO algorithm, where the delayed information of locally best particle and globally best particle are exploited to update the velocity of particle. By testing the proposed SDPSO-ELM in a comprehensive manner on a tanh function, this approach obtain better generalization performance and can also avoid adding unnecessary hidden nodes and overtraining problems. Moreover, it has shown outstanding performance than other state-of-the-art ELMs. Finally, the proposed SDPSO-ELM algorithm is successfully applied to the STLF of power system. Experiment results demonstrate that the proposed learning algorithm can get better forecasting results in comparison with the radial basis function neural network (RBFNN) algorithm.},
author = {Zeng, Nianyin and Zhang, Hong and Liu, Weibo and Liang, Jinling and Alsaadi, Fuad E.},
doi = {10.1016/j.neucom.2017.01.090},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/zeng2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Neural network,Short-term load forecasting,Switching delayed particle swarm optimization (SDP,Time-delay},
pages = {175--182},
publisher = {Elsevier B.V.},
title = {{A switching delayed PSO optimized extreme learning machine for short-term load forecasting}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.01.090},
volume = {240},
year = {2017}
}
@article{Zhang2016,
abstract = {The shape feature of an object represents the geometrical information which plays an important role in the image understanding and image retrieval. How to get an excellent shape feature that has rotation, scaling and translation (RST) invariance is a problem in this field. This paper proposed a novel local extreme learning machine (LELM) classification algorithm to extract the shape features. LELM finds nearest neighbors of the testing set from the original training set and trains a local classification model. The shape feature is represented by an analytic decision function with a radial basis function (RBF) kernel obtained by LELM. Our method has the following advantages: (1) LELM not only keeps the local structure of the samples, but also solves the imbalance problem between variance and bias. (2) Features obtained by the RBF kernel are RST invariant. (3) LELM is more robust against the noise and fragmentation compared to other methods. We also demonstrate the performance of the proposed method in image retrieval.},
author = {Zhang, Jing and Feng, Lin and Wu, Bin},
doi = {10.1007/s00521-015-2008-7},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zhang2015.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine (ELM),Feature point,Image retrieval,Local extreme learning machine (LELM)},
number = {7},
pages = {2095--2105},
publisher = {Springer London},
title = {{Local extreme learning machine: local classification model for shape feature extraction}},
volume = {27},
year = {2016}
}
@article{Zhang2017,
abstract = {Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aims at exploring the capability of extreme learning machine on high-level deep features of images. Additionally, motivated by the biological learning mechanism of ELM, in this paper, an adaptive extreme learning machine (AELM) method is proposed for handling cross-task (domain) learning problems, without loss of its nature of randomization and high efficiency. The proposed AELM is an extension of ELM from single task to cross task learning, by introducing a new error term and Laplacian graph based manifold regularization term in objective function. We have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt 4 benchmark object recognition datasets from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on ImageNet. Experiments demonstrate that the proposed AELM is comparable and effective in single and multiple domains based recognition tasks.},
author = {Zhang, Lei and He, Zhenwei and Liu, Yan},
doi = {10.1016/j.neucom.2017.02.016},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/leizhang2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep learning,Extreme learning machine,Image classification,Object recognition,Support vector machine},
pages = {194--203},
publisher = {Elsevier B.V.},
title = {{Deep object recognition across domains based on adaptive extreme learning machine}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.02.016},
volume = {239},
year = {2017}
}
@article{Zhang2016a,
abstract = {— We address the problem of visual knowledge adap-tation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the 2,1 -norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.},
author = {Zhang, Lei and Zhang, David},
doi = {10.1109/TIP.2016.2598679},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/zhang2016.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Domain adaptation,Extreme learning machine,cross-domain learning,knowledge adaptation},
number = {10},
pages = {4959--4973},
title = {{Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation}},
volume = {25},
year = {2016}
}
@article{Zhang2017a,
archivePrefix = {arXiv},
arxivId = {1505.04373},
author = {Zhang, Lei and Zhang, David},
doi = {10.1109/TNNLS.2016.2607757},
eprint = {1505.04373},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/zhang2017.pdf:pdf},
isbn = {2162-237X VO - PP},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {12},
pages = {3045--3060},
pmid = {27740499},
title = {{Evolutionary Cost-Sensitive Extreme Learning Machine}},
url = {http://ieeexplore.ieee.org/document/7588107/},
volume = {28},
year = {2017}
}
@article{Zhang2018,
author = {Zhang, Liwei and Zhai, Jian},
doi = {10.1007/s10586-018-1804-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/liweizhang2017.pdf:pdf},
issn = {1386-7857},
journal = {Cluster Computing},
keywords = {Power transformers,Fault diagnosis,Dissolved gas a,dissolved gas analysis,extreme learning machine,fault diagnosis,majority voting,power transformers},
number = {1},
publisher = {Springer US},
title = {{Fault diagnosis for oil-filled transformers using voting based extreme learning machine}},
url = {http://link.springer.com/10.1007/s10586-018-1804-0},
year = {2018}
}
@article{Zhang2013,
author = {Zhang, W.B. and Ji, H.B.},
doi = {10.1049/el.2012.3642},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/zhang2013.pdf:pdf},
isbn = {0013-5194},
issn = {0013-5194},
journal = {Electronics Letters},
number = {7},
pages = {448--450},
title = {{Fuzzy extreme learning machine for classification}},
url = {http://digital-library.theiet.org/content/journals/10.1049/el.2012.3642},
volume = {49},
year = {2013}
}
@article{Zhang2017b,
author = {Zhang, Wei and Xu, Aiqiang and Ping, Dianfa and Gao, Mingzhe},
doi = {10.1007/s00521-017-3096-3},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zhang2017.pdf:pdf},
isbn = {1601111088},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Adaptive regularization,Extreme learning machine,Fixed budget,Online modeling,Sparsity measures,Time series prediction},
pages = {1--16},
publisher = {Springer London},
title = {{An improved kernel-based incremental extreme learning machine with fixed budget for nonstationary time series prediction}},
year = {2017}
}
@article{Zhang2016b,
abstract = {Extreme learning machine for single-hidden-layer feedforward neural networks has been extensively applied in imbalanced data learning due to its fast learning capability. Ensemble approach can effectively improve the classification performance by combining several weak learners according to a certain rule. In this paper, a novel ensemble approach on weighted extreme learning machine for imbalanced data classification problem is proposed. The weight of each base learner in the ensemble is optimized by differential evolution algorithm. Experimental results on 12 datasets show that the proposed method could achieve more classification performance compared with the simple vote-based ensemble method and non-ensemble method.},
author = {Zhang, Yong and Liu, Bo and Cai, Jing and Zhang, Suhua},
doi = {10.1007/s00521-016-2342-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zhang2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Class imbalanced learning,Differential evolution,Ensemble,Extreme learning machine},
pages = {1--9},
publisher = {Springer London},
title = {{Ensemble weighted extreme learning machine for imbalanced data classification based on differential evolution}},
year = {2016}
}
@article{Zhang2017c,
abstract = {{\textcopyright} 2017 Springer Science+Business Media, LLC Pathological brain detection is an automated computer-aided diagnosis for brain images. This study provides a novel method to achieve this goal.We first used synthetic minority oversampling to balance the dataset. Then, our system was based on three components: wavelet packet Tsallis entropy, extreme learning machine, and Jaya algorithm. The 10 repetitions of K-fold cross validation showed our method achieved perfect classification on two small datasets, and achieved a sensitivity of 99.64 ± 0.52{\%}, a specificity of 99.14 ± 1.93{\%}, and an accuracy of 99.57 ± 0.57{\%} over a 255-image dataset. Our method performs better than six state-of-the-art approaches. Besides, Jaya algorithm performs better than genetic algorithm, particle swarm optimization, and bat algorithm as ELM training method.},
author = {Zhang, Yu Dong and Zhao, Guihu and Sun, Junding and Wu, Xiaosheng and Wang, Zhi Heng and Liu, Hong Min and Govindaraj, Vishnu Varthanan and Zhan, Tianmin and Li, Jianwu},
doi = {10.1007/s11042-017-5023-0},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/dongzhang2017.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Extreme learning machine,Jaya algorithm,Pathological brain detection,Synthetic minority oversampling},
pages = {1--20},
publisher = {Multimedia Tools and Applications},
title = {{Smart pathological brain detection by synthetic minority oversampling technique, extreme learning machine, and Jaya algorithm}},
year = {2017}
}
@article{Zhao2014,
abstract = {Extreme learning machine (ELM) is widely used in training single-hidden layer feedforward neural networks (SLFNs) because of its good generalization and fast speed. However, most improved ELMs usually discuss the approximation problem for sample data with output noises, not for sample data with noises both in input and output values, i.e., error-in-variable (EIV) model. In this paper, a novel algorithm, called (regularized) TLS-ELM, is proposed to approximate the EIV model based on ELM and total least squares (TLS) method. The proposed TLS-ELM uses the idea of ELM to choose the hidden weights, and applies TLS method to determine the output weights. Furthermore, the perturbation quantities of hidden output matrix and observed values are given simultaneously. Comparison experiments of our proposed TLS-ELM with least square method, TLS method and ELM show that our proposed TLS-ELM has better accuracy and less training time.},
author = {Zhao, Jianwei and Wang, Zhihui and Cao, Feilong},
doi = {10.1007/s11280-013-0220-x},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/jianweizhao2013.pdf:pdf},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Errors-in-variables model,Extreme learning machine,Regularization,Total least square},
number = {5},
pages = {1205--1216},
pmid = {1655718327},
title = {{Extreme learning machine with errors in variables}},
volume = {17},
year = {2014}
}
@article{Zhao2014a,
author = {Zhao, Jianwei and Zhou, Zhenghua and Cao, Feilong},
doi = {10.1007/s00521-013-1356-4},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/weizhao2013.pdf:pdf},
isbn = {0941-0643},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {2-Dimension principal component analysis,Ensemble of polyharmonic extreme learning machine,Fast discrete curvelet transform,Human face recognition},
number = {6},
pages = {1317--1326},
title = {{Human face recognition based on ensemble of polyharmonic extreme learning machine}},
volume = {24},
year = {2014}
}
@article{Zhao2014b,
abstract = {This paper presents a novel solution based on Extreme Learning Machine (ELM) for multiclass XML documents classification. ELM is a generalized Single-hidden Layer Feedforward Network (SLFN) with extremely fast learning capacity. An improved vector model DSVM (Distribution based Structured Vector Model) is proposed to represent XML documents with more structural information and more precise semantic information. The XML documents classifiers are conducted based on PV-ELM (Probablity based Voting ELM) with a postprocessing method $\epsilon$-RCC ($\epsilon$ - Revoting of Confusing Classes) to refine the voting results. To evaluate the overall performance of this solution, a series of experiments are conducted on two real datasets of news feeds online. The experimental results show that DSVM represents the XML documents more effectively and PV-ELM with $\epsilon$-RCC achieves a higher accuracy than original ELM algorithm for multiclass classification. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Zhao, Xiangguo and Bi, Xin and Qiao, Baiyou},
doi = {10.1007/s11280-013-0230-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zhao2013.pdf:pdf},
isbn = {1386-145X},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Decomposition methods,Extreme learning machine,Multiclass classification,Voting-ELM,XML representation model},
number = {5},
pages = {1217--1231},
title = {{Probability based voting extreme learning machine for multiclass XML documents classification}},
volume = {17},
year = {2014}
}
@article{Zheng2013,
abstract = {This article proposes a novel approach for text categorization based on a regularization extreme learning machine (RELM) in which its weights can be obtained analytically, and a bias-variance trade-off could be achieved by adding a regularization term into the linear system of single-hidden layer feedforward neural networks. To fit the input scale of RELM, the latent semantic analysis was used to represent text for dimensionality reduction. Moreover, a classification algorithm based on RELM was developed including the uni-label (i.e., a document can only be assigned to a unique category) and multi-label (i.e., a document can be assigned to multiple categories simultaneously) situations. The experimental results in two benchmarks show that the proposed method can produce good performance in most cases, and it could learn faster than popular methods such as feedforward neural networks or support vector machine. {\textcopyright} 2012 Springer-Verlag London Limited.},
author = {Zheng, Wenbin and Qian, Yuntao and Lu, Huijuan},
doi = {10.1007/s00521-011-0808-y},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zheng2012.pdf:pdf},
isbn = {09410643 (ISSN)},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Latent semantic analysis,Regularization,Support vector machine,Text categorization},
number = {3-4},
pages = {447--456},
title = {{Text categorization based on regularization extreme learning machine}},
volume = {22},
year = {2013}
}
@article{Zheng2013a,
abstract = {The bloom of Internet has made fast text categorization very essential. Generally, the popular methods have good classification accuracy but slow speed, and vice versa. This paper proposes a novel approach for fast text categorization, in which a collaborative work framework based on a linear classifier and an extreme learning machine (ELM) is constructed. The linear classifier, obtained by a modified non-negative matrix factorization algorithm, maps all documents from the original term space into the class space directly such that it performs classification very fast. The ELM, with good classification accuracy via some nonlinear and linear transformations, classifies a few of documents according to some given criteria to improve the classification quality of the total system. Experimental results show that the proposed method not only achieves good accuracy but also performs classification very fast, which improves the averaged speed by 180 {\%} compared with its corresponding method. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Zheng, Wenbin and Tang, Hong and Qian, Yuntao},
doi = {10.1007/s11280-013-0225-5},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zheng2013.pdf:pdf},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Extreme learning machine,Fast text categorization,Linear transformation,Non-negative matrix factorization},
number = {2},
pages = {235--252},
title = {{Collaborative work with linear classifier and extreme learning machine for fast text categorization}},
volume = {18},
year = {2013}
}
@article{Zhou2016,
abstract = {It is expensive in time or resources to obtain adequate labeled data for a new remote sensing image to be categorized. The cost of manual interpretation can be reduced if labeled samples collected fromprevious temporal images can be reused to classify a new image over the same investigated area. However, it is rea- sonable to consider that the distributions of the target data and the historical data are usually not identical. Therefore, the ef- ficient strategy of transferring the beneficial information from historical images to the target image hits a bottleneck. In order to reuse sufficient historical samples to classify a given image with scarce labeled samples, this letter presents a novel transfer learning algorithm for remote sensing image classification based on extreme learning machine with weighted least square. This algorithm adds a transferring item to an objective function and adjusts historical and target training data with different weight strategies. Experiments on two sets of remote sensing images show that the presented algorithm reduces the requirement for target training samples and improves classification accuracy, timeliness, and integrity. Index},
author = {Zhou, Yang and Lian, Jie and Han, Min},
doi = {10.1109/LGRS.2016.2568263},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/zhou2016.pdf:pdf},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Extreme learning machine (ELM),Weighted least square,image classification,remote sensing,transfer learning},
number = {10},
pages = {1405--1409},
title = {{Remote Sensing Image Transfer Classification Based on Weighted Extreme Learning Machine}},
volume = {13},
year = {2016}
}
@article{Zhou2013,
author = {Zhou, Zheng Hua and Zhao, Jian Wei and Cao, Fei Long},
doi = {10.1007/s00521-012-0891-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zhou2012.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Extreme learning machine,Feedforward neural networks,Polyharmonic extreme learning machine,Surface reconstruction},
number = {2},
pages = {283--292},
title = {{Surface reconstruction based on extreme learning machine}},
volume = {23},
year = {2013}
}
@article{Zhu2016,
abstract = {Extreme learning machine (ELM) is a single-hidden layer feed-forward neural network with an efficient learning algorithm. Conventionally an ELM is trained using all the data based on the least square solution, and thus it may suffer from overfitting. In this paper, we present a new method of data and feature mixed ensemble based extreme learning machine (DFEN-ELM). DFEN-ELM combines data ensemble and feature subspace ensemble to tackle the overfitting problem and it takes advantage of the fast speed of ELM when building ensembles of classifiers. Both one-class and two-class ensemble based ELM have been studied. Experiments were conducted on computed tomography (CT) data for liver tumor detection and segmentation as well as magnetic resonance imaging (MRI) data for rodent brain segmentation. To improve the ensembles with new training data, sequential kernel learning is adopted further in the experiments on CT data for speedy retraining and iteratively enhancing the image segmentation performance. Experiment results on different testing cases and various testing datasets demonstrate that DFEN-ELM is a robust and efficient algorithm for medical object detection and segmentation.},
author = {Zhu, Wanzheng and Huang, Weimin and Lin, Zhiping and Yang, Yongzhong and Huang, Su and Zhou, Jiayin},
doi = {10.1007/s11042-015-2582-9},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zhu2015.pdf:pdf},
isbn = {1573-7721},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Classifier,Ensemble learning,Extreme learning machine (ELM),Iterative learning,Medical object detection and segmentation,Overfitting},
number = {5},
pages = {2815--2837},
title = {{Data and feature mixed ensemble based extreme learning machine for medical object detection and segmentation}},
volume = {75},
year = {2016}
}
@article{Zhu2017,
abstract = {Urban haze pollution is becoming increasingly serious, which is considered very harmful for humans by World Health Organization (WHO). Haze forecasts can be used to protect human health. In this paper, a Selective ENsemble based on an Extreme Learning Machine (ELM) and Improved Discrete Artificial Fish swarm algorithm (IDAFSEN) is proposed, which overcomes the drawback that a single ELM is unstable in terms of its classification. First, the initial pool of base ELMs is generated by using bootstrap sampling, which is then pre-pruned by calculating the pair-wise diversity measure of each base ELM. Second, partial-based ELMs among the initial pool after pre-pruning with higher precision and with greater diversity are selected by using an Improved Discrete Artificial Fish Swarm Algorithm (IDAFSA). Finally, the selected base ELMs are integrated through majority voting. The Experimental results on 16 datasets from the UCI Machine Learning Repository demonstrate that IDAFSEN can achieve better classification accuracy than other previously reported methods. After a performance evaluation of the proposed approach, this paper looks at how this can be used in haze forecasting in China to protect human health.},
author = {Zhu, Xuhui and Ni, Zhiwei and Cheng, Meiying and Jin, Feifei and Li, Jingming and Weckman, Gary},
doi = {10.1007/s10489-017-1027-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/xuhuizhu2017.pdf:pdf},
isbn = {0924-669X},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Artificial fish swarm algorithm,Extreme learning machine,Haze forecast,Selective ensemble},
pages = {1--19},
publisher = {Applied Intelligence},
title = {{Selective ensemble based on extreme learning machine and improved discrete artificial fish swarm algorithm for haze forecast}},
year = {2017}
}
@article{Zong2013,
abstract = {Relevance ranking has been a popular and interesting topic over the years, which has a large variety of applications. A number of machine learning techniques were success- fully applied as the learning algorithms for relevance ranking, including neural network, regularized least square, support vector machine and so on. From machine learning point of view, extreme learning machine actually provides a unified framework where the afore- mentioned algorithms can be considered as special cases. In this paper, pointwise ELM and pairwise ELM are proposed to learn relevance ranking problems for the first time. In par- ticular, ELM type of linear random node is newly proposed together with kernel version of ELM to be linear as well. The famous publicly available dataset collection LETOR is tested to compare ELM-based ranking algorithms with state-of-art linear ranking algorithms.},
author = {Zong, Weiwei and Huang, Guangbin},
doi = {10.1007/s11063-013-9295-8},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/link.springer/zong2013.pdf:pdf},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {ELM,Kernal,Learning,Linear},
number = {2},
pages = {155--166},
title = {{Learning to Rank with Extreme Learning Machine}},
url = {http://link.springer.com/10.1007/s11063-013-9295-8{\%}0Apapers3://publication/doi/10.1007/s11063-013-9295-8},
volume = {39},
year = {2013}
}
@article{Zou2016,
abstract = {Indoor Positioning System (IPS) has become one of the most attractive research fields due to the increasing demands on Location Based Services (LBSs) in indoor environments. Various IPSs have been developed under different circumstances, and most of them adopt the fingerprinting technique to mitigate pervasive indoor multipath effects. However, the performance of the fingerprinting technique severely suffers from device heterogeneity existing across commercial off-the-shelf mobile devices (e.g. smart phones, tablet computers, etc.) and indoor environmental changes (e.g. the number, distribution and activities of people, the placement of furniture, etc.). In this paper, we transform the Received Signal Strength (RSS) to a standardized location fingerprint based on the Procrustes analysis, and introduce a similarity metric, termed Signal Tendency Index (STI), for matching standardized fingerprints. An analysis on the capability of the proposed STI in handling device heterogeneity and environmental changes is presented. We further develop a robust and precise IPS by integrating the merits of both the STI and Weighted Extreme Learning Machine (WELM). Finally, extensive experiments are carried out and a performance comparison with existing solutions verifies the superiority of the proposed IPS in terms of robustness to device heterogeneity.},
author = {Zou, Han and Huang, Baoqi and Lu, Xiaoxuan and Jiang, Hao and Xie, Lihua},
doi = {10.1109/TWC.2015.2487963},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/ieeexplorer/zou2016.pdf:pdf},
isbn = {1536-1276 VO  - 15},
issn = {15361276},
journal = {IEEE Transactions on Wireless Communications},
keywords = {Device Heterogeneity,Indoor Positioning System (IPS),Procrustes Analysis,Weighted Extreme Learning Machine},
number = {2},
pages = {1252--1266},
title = {{A Robust Indoor Positioning System Based on the Procrustes Analysis and Weighted Extreme Learning Machine}},
volume = {15},
year = {2016}
}
@article{Zou2018,
abstract = {In online learning, the contribution of old samples to a model decreases as time passes, and old samples gradually become invalid. Although the Online Sequential Extreme Learning Machine (OS-ELM) can avoid the repetitive training of old samples, invalid samples are still used, which goes against improving the accuracy of an OS-ELM model. The Online Sequence Extreme Learning Machine with Forgetting Mechanism (FOS-ELM) timely discards invalid samples, but it does not consider the differences among valid samples and then has the limitation on boosting the accuracy and generalization. To solve this issue, the Memory Degradation Based OS-ELM (MDOS-ELM) is proposed in this paper. The MDOS-ELM adjusts the weights of the old and new samples in real time by a self-adaptive memory factor, and simultaneously discards invalid samples. The self-adaptive memory factor is determined by two elements. One is the similarity between the new and old samples, and the other is the prediction errors of the current training samples on the previous model. The performance of the proposed MDOS-ELM is validated on both regression and classification datasets which include an artificial dataset and twenty-two real-world dataset. The results demonstrate that the MDOS-ELM model outperforms the OS-ELM and the FOS-ELM models on the accuracy and generalization.},
author = {Zou, Quan Yi and Wang, Xiao Jun and Zhou, Chang Jun and Zhang, Qiang},
doi = {10.1016/j.neucom.2017.11.030},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/zhou2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Extreme learning machine,Memory factor,Online learning,Similarity},
pages = {2864--2879},
publisher = {Elsevier B.V.},
title = {{The memory degradation based online sequential extreme learning machine}},
url = {https://doi.org/10.1016/j.neucom.2017.11.030},
volume = {275},
year = {2018}
}
@article{Zou2017,
abstract = {Predictions regarding the solar greenhouse temperature and humidity are important because they play a critical role in greenhouse cultivation. On account of this, it is important to set up a predictive model of temperature and humidity that would precisely predict the temperature and humidity, reducing potential financial losses. This paper presents a novel temperature and humidity prediction model based on convex bidirectional extreme learning machine (CB-ELM). Simulation results show that the convergence rate of the bidirectional extreme learning machine (B-ELM) can further be improved while retaining the same simplicity, by simply recalculating the output weights of the existing nodes based on a convex optimization method when a new hidden node is randomly added. The performance of the CB-ELM model is compared with other modeling approaches by applying it to predict solar greenhouse temperature and humidity. The experiment results show that the CB-ELM model predictions are more accurate than those of the B-ELM, Back Propagation Neural Network (BPNN), Support Vector Machine (SVM), and Radial Basis Function (RBF). Therefore, it can be considered as a suitable and effective method for predicting the solar greenhouse temperature and humidity.},
author = {Zou, Weidong and Yao, Fenxi and Zhang, Baihai and He, Chaoxing and Guan, Zixiao},
doi = {10.1016/j.neucom.2017.03.023},
file = {:D$\backslash$:/{\_}{\_}PASCA{\_}{\_}/{\_}{\_}THESIS{\_}{\_}{\_}/ELM/references/elsevier/zou2017.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convex bidirectional extreme learning machine,Radial basis function,Solar greenhouse,Support vector machine},
pages = {72--85},
publisher = {Elsevier B.V.},
title = {{Verification and predicting temperature and humidity in a solar greenhouse based on convex bidirectional extreme learning machine algorithm}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.03.023},
volume = {249},
year = {2017}
}
