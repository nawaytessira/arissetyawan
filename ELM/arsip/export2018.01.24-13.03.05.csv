Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,MeSH Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Real-time transient stability assessment model using extreme learning machine,Y. Xu; Z. Y. Dong; K. Meng; R. Zhang; K. P. Wong,"Dept. of Electr. Eng., Hong Kong Polytech. Univ., Kowloon, China","IET Generation, Transmission & Distribution",20110217,2011,5,3,314,322,"In recent years, computational intelligence and machine learning techniques have gained popularity to facilitate very fast dynamic security assessment for earlier detection of the risk of blackouts. However, many of the current state-of-the-art models usually suffer from excessive training time and complex parameters tuning problems, leading to inefficiency for real-time implementation and on-line model updating. In this study, a new transient stability assessment model using the increasingly prevalent extreme learning machine theory is developed. It has significantly improved the learning speed and can enable effective on-line updating. The proposed model is examined on the New England 39-bus test system, and compared with some state-of-the-art methods in terms of computation time and prediction accuracy. The simulation results show that the proposed model possesses significant superior computation speed and competitively high accuracy.",1751-8687;17518687,,10.1049/iet-gtd.2010.0355,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714771,,,learning (artificial intelligence);power engineering computing;power system transient stability,39-bus test system;England;computational intelligence;extreme learning machine;fast dynamic security assessment;machine learning techniques;parameters tuning problems;power systems;real-time transient stability assessment model,,22,,,,,,Mar-11,,IET,IET Journals & Magazines
Remote Sensing Image Transfer Classification Based on Weighted Extreme Learning Machine,Y. Zhou; J. Lian; M. Han,"Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China",IEEE Geoscience and Remote Sensing Letters,20170519,2016,13,10,1405,1409,"It is expensive in time or resources to obtain adequate labeled data for a new remote sensing image to be categorized. The cost of manual interpretation can be reduced if labeled samples collected from previous temporal images can be reused to classify a new image over the same investigated area. However, it is reasonable to consider that the distributions of the target data and the historical data are usually not identical. Therefore, the efficient strategy of transferring the beneficial information from historical images to the target image hits a bottleneck. In order to reuse sufficient historical samples to classify a given image with scarce labeled samples, this letter presents a novel transfer learning algorithm for remote sensing image classification based on extreme learning machine with weighted least square. This algorithm adds a transferring item to an objective function and adjusts historical and target training data with different weight strategies. Experiments on two sets of remote sensing images show that the presented algorithm reduces the requirement for target training samples and improves classification accuracy, timeliness, and integrity.",1545-598X;1545598X,,10.1109/LGRS.2016.2568263,Special Fund for Basic Research on Scientific Instruments of the National Natural Science Foundation of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7527666,Extreme learning machine (ELM);image classification;remote sensing;transfer learning;weighted least square,Algorithm design and analysis;Classification algorithms;Data models;Linear programming;Remote sensing;Training;Training data,geophysical image processing;image classification;remote sensing,historical image information;historical training data;remote sensing image transfer classification;target training data;transfer learning algorithm;weighted extreme learning machine;weighted least square,,2,,,,,20160801,Oct. 2016,,IEEE,IEEE Journals & Magazines
Bidirectional Extreme Learning Machine for Regression Problem and Its Learning Effectiveness,Y. Yang; Y. Wang; X. Yuan,"College of Electrical and Information Engineering, Hunan University, Changsha, China",IEEE Transactions on Neural Networks and Learning Systems,20120801,2012,23,9,1498,1505,"It is clear that the learning effectiveness and learning speed of neural networks are in general far slower than required, which has been a major bottleneck for many applications. Recently, a simple and efficient learning method, referred to as extreme learning machine (ELM), was proposed by Huang , which has shown that, compared to some conventional methods, the training time of neural networks can be reduced by a thousand times. However, one of the open problems in ELM research is whether the number of hidden nodes can be further reduced without affecting learning effectiveness. This brief proposes a new learning algorithm, called bidirectional extreme learning machine (B-ELM), in which some hidden nodes are not randomly selected. In theory, this algorithm tends to reduce network output error to 0 at an extremely early learning stage. Furthermore, we find a relationship between the network output error and the network output weights in the proposed B-ELM. Simulation results demonstrate that the proposed method can be tens to hundreds of times faster than other incremental ELM algorithms.",2162-237X;2162237X,,10.1109/TNNLS.2012.2202289,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6222007,Feedforward neural network;learning effectiveness;number of hidden nodes;universal approximation,Computer architecture;Equations;Helium;Learning systems;Machine learning;Testing;Training,learning (artificial intelligence);neural nets;regression analysis,B-ELM;bidirectional extreme learning machine;learning effectiveness;neural networks;regression problem,1,51,,13,,,20120620,Sept. 2012,,IEEE,IEEE Journals & Magazines
BELM: Bayesian Extreme Learning Machine,E. Soria-Olivas; J. Gomez-Sanchis; J. D. Martin; J. Vila-Frances; M. Martinez; J. R. Magdalena; A. J. Serrano,"Digital Signal Processing Group, Department of Electronic Engineering, ETSE, University of Valencia, Burjassot, Spain",IEEE Transactions on Neural Networks,20110228,2011,22,3,505,509,"The theory of extreme learning machine (ELM) has become very popular on the last few years. ELM is a new approach for learning the parameters of the hidden layers of a multilayer neural network (as the multilayer perceptron or the radial basis function neural network). Its main advantage is the lower computational cost, which is especially relevant when dealing with many patterns defined in a high-dimensional space. This brief proposes a Bayesian approach to ELM, which presents some advantages over other approaches: it allows the introduction of a priori knowledge; obtains the confidence intervals (CIs) without the need of applying methods that are computationally intensive, e.g., bootstrap; and presents high generalization capabilities. Bayesian ELM is benchmarked against classical ELM in several artificial and real datasets that are widely used for the evaluation of machine learning algorithms. Achieved results show that the proposed approach produces a competitive accuracy with some additional advantages, namely, automatic production of CIs, reduction of probability of model overfitting, and use of a priori knowledge.",1045-9227;10459227,,10.1109/TNN.2010.2103956,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692833,Bayesian;extreme learning machine;multilayer perceptron;radial basis function,Artificial neural networks;Bayesian methods;Computational modeling;Machine learning;Mathematical model;Optimization;Training,belief networks;learning (artificial intelligence);multilayer perceptrons;radial basis function networks,Bayesian extreme learning machine;confidence interval;multilayer neural network;multilayer perceptron;radial basis function,"Algorithms;Artificial Intelligence;Bayes Theorem;Computer Simulation;Neural Networks (Computer);Pattern Recognition, Automated",57,,18,,,20110120,Mar-11,,IEEE,IEEE Journals & Magazines
A Robust Indoor Positioning System Based on the Procrustes Analysis and Weighted Extreme Learning Machine,H. Zou; B. Huang; X. Lu; H. Jiang; L. Xie,"School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Wireless Communications,20160208,2016,15,2,1252,1266,"Indoor positioning system (IPS) has become one of the most attractive research fields due to the increasing demands on location-based services (LBSs) in indoor environments. Various IPSs have been developed under different circumstances, and most of them adopt the fingerprinting technique to mitigate pervasive indoor multipath effects. However, the performance of the fingerprinting technique severely suffers from device heterogeneity existing across commercial off-the-shelf mobile devices (e.g., smart phones, tablet computers, etc.) and indoor environmental changes (e.g., the number, distribution and activities of people, the placement of furniture, etc.). In this paper, we transform the received signal strength (RSS) to a standardized location fingerprint based on the Procrustes analysis, and introduce a similarity metric, termed signal tendency index (STI), for matching standardized fingerprints. An analysis of the capability of the proposed STI to handle device heterogeneity and environmental changes is presented. We further develop a robust and precise IPS by integrating the merits of both the STI and weighted extreme learning machine (WELM). Finally, extensive experiments are carried out and a performance comparison with existing solutions verifies the superiority of the proposed IPS in terms of robustness to device heterogeneity.",1536-1276;15361276,,10.1109/TWC.2015.2487963,Berkeley Education Alliance for Research in Singapore (BEARS); Berkeley as a Center for Intellectual Excellence in Research and Education in Singapore; Singapore-Berkeley Building Efficiency and Sustainability in the Tropics; 10.13039/501100001381 - Republic of Singapore NRF; 10.13039/501100001381 - Republic of Singapore National Research Foundation (NRF); 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004763 - Natural Science Foundation of Inner Mongolia Autonomous Region of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293674,Device Heterogeneity;Indoor Positioning System (IPS);Indoor positioning system (IPS);Procrustes Analysis;Procrustes analysis;Weighted Extreme Learning Machine;device heterogeneity;weighted extreme learning machine (WELM),Accuracy;Databases;IEEE 802.11 Standard;Mobile handsets;Robustness;Shape;Training,RSSI;indoor navigation;learning (artificial intelligence);radionavigation;telecommunication computing,IPS;LBSs;RSS;STI;WELM;commercial off-the-shelf mobile devices;fingerprinting technique;indoor environments;location-based services;pervasive indoor multipath effect mitigation;procrustes analysis;received signal strength;robust indoor positioning system;signal tendency index;standardized fingerprint matching;standardized location fingerprint;weighted extreme learning machine,,22,,44,,,20151007,Feb. 2016,,IEEE,IEEE Journals & Magazines
Dimension Reduction With Extreme Learning Machine,L. L. C. Kasun; Y. Yang; G. B. Huang; Z. Zhang,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Image Processing,20160624,2016,25,8,3906,3918,"Data may often contain noise or irrelevant information, which negatively affect the generalization capability of machine learning algorithms. The objective of dimension reduction algorithms, such as principal component analysis (PCA), non-negative matrix factorization (NMF), random projection (RP), and auto-encoder (AE), is to reduce the noise or irrelevant information of the data. The features of PCA (eigenvectors) and linear AE are not able to represent data as parts (e.g. nose in a face image). On the other hand, NMF and non-linear AE are maimed by slow learning speed and RP only represents a subspace of original data. This paper introduces a dimension reduction framework which to some extend represents data as parts, has fast learning speed, and learns the between-class scatter subspace. To this end, this paper investigates a linear and non-linear dimension reduction framework referred to as extreme learning machine AE (ELM-AE) and sparse ELM-AE (SELM-AE). In contrast to tied weight AE, the hidden neurons in ELM-AE and SELM-AE need not be tuned, and their parameters (e.g, input weights in additive neurons) are initialized using orthogonal and sparse random weights, respectively. Experimental results on USPS handwritten digit recognition data set, CIFAR-10 object recognition, and NORB object recognition data set show the efficacy of linear and non-linear ELM-AE and SELM-AE in terms of discriminative capability, sparsity, training time, and normalized mean square error.",1057-7149;10577149,,10.1109/TIP.2016.2570569,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471467,Dimension reduction;Extreme Learning Machine (ELM);Extreme learning machine (ELM);Non-negative Matrix Factorization (NMF);Principal Component Analysis (PCA);auto-encoder (AE);dimension reduction;non-negative matrix factorization (NMF);principal component analysis (PCA);random projection (RP),Machine learning;Machine learning algorithms;Mathematical model;Principal component analysis;Support vector machines,,,,7,,55,,,20160518,Aug. 2016,,IEEE,IEEE Journals & Magazines
Electricity Price Forecasting With Extreme Learning Machine and Bootstrapping,X. Chen; Z. Y. Dong; K. Meng; Y. Xu; K. P. Wong; H. W. Ngan,"Ergon Energy, Brisbane, Australia",IEEE Transactions on Power Systems,20121018,2012,27,4,2055,2062,"Artificial neural networks (ANNs) have been widely applied in electricity price forecasts due to their nonlinear modeling capabilities. However, it is well known that in general, traditional training methods for ANNs such as back-propagation (BP) approach are normally slow and it could be trapped into local optima. In this paper, a fast electricity market price forecast method is proposed based on a recently emerged learning method for single hidden layer feed-forward neural networks, the extreme learning machine (ELM), to overcome these drawbacks. The new approach also has improved price intervals forecast accuracy by incorporating bootstrapping method for uncertainty estimations. Case studies based on chaos time series and Australian National Electricity Market price series show that the proposed method can effectively capture the nonlinearity from the highly volatile price data series with much less computation time compared with other methods. The results show the great potential of this proposed approach for online accurate price forecasting for the spot market prices analysis.",0885-8950;08858950,,10.1109/TPWRS.2012.2190627,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184354,Bootstrapping;extreme learning machine;interval forecast;price forecast,Electricity supply industry;Forecasting;Learning systems;Machine learning;Predictive models;Time series analysis,chaos;feedforward neural nets;learning (artificial intelligence);load forecasting;power engineering computing;power markets;time series;training,ANN;Australian National Electricity market price series;ELM;artificial neural networks;chaos time series;electricity market price forecast method;extreme learning machine;incorporating bootstrapping method;nonlinear modeling capabilities;price interval forecasting;single hidden layer feedforward neural networks;spot market prices analysis;training methods;uncertainty estimations;volatile price data series,,68,,40,,,20120416,Nov. 2012,,IEEE,IEEE Journals & Magazines
Weighted Tanimoto Extreme Learning Machine with Case Study in Drug Discovery,W. M. Czarnecki,"Faculty of Mathematics and Computer Science, Jagiellonian University, Krakow, Poland",IEEE Computational Intelligence Magazine,20150716,2015,10,3,19,29,"Machine learning methods are becoming more and more popular in the field of computer-aided drug design. The specific data characteristic, including sparse, binary representation as well as noisy, imbalanced datasets, presents a challenging binary classification problem. Currently, two of the most successful models in such tasks are the Support Vector Machine (SVM) and Random Forest (RF). In this paper, we introduce a Weighted Tanimoto Extreme Learning Machine (T-WELM), an extremely simple and fast method for predicting chemical compound biological activity and possibly other data with discrete, binary representation. We show some theoretical properties of the proposed model including the ability to learn arbitrary sets of examples. Further analysis shows numerous advantages of T-WELM over SVMs, RFs and traditional Extreme Learning Machines (ELM) in this particular task. Experiments performed on 40 large datasets of thousands of chemical compounds show that T-WELMs achieve much better classification results and are at the same time faster in terms of both training time and further classification than both ELM models and other state-of-the-art methods in the field.",1556-603X;1556603X,,10.1109/MCI.2015.2437312,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160842,,Biological system modeling;Compounds;Computational modeling;Design automation;Drugs;Fingerprint recognition;Machine learning,drug delivery systems;learning (artificial intelligence);medical computing;pattern classification;support vector machines,RF;SVM;T-WELM;binary classification problem;chemical compound biological activity prediction;computer-aided drug design;data characteristic;drug discovery;random forest;support vector machine;weighted Tanimoto extreme learning machine method,,10,,45,,,,Aug. 2015,,IEEE,IEEE Journals & Magazines
Robust Extreme Learning Machine With its Application to Indoor Positioning,X. Lu; H. Zou; H. Zhou; L. Xie; G. B. Huang,"School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Cybernetics,20170520,2016,46,1,194,205,"The increasing demands of location-based services have spurred the rapid development of indoor positioning system and indoor localization system interchangeably (IPSs). However, the performance of IPSs suffers from noisy measurements. In this paper, two kinds of robust extreme learning machines (RELMs), corresponding to the close-to-mean constraint, and the small-residual constraint, have been proposed to address the issue of noisy measurements in IPSs. Based on whether the feature mapping in extreme learning machine is explicit, we respectively provide random-hidden-nodes and kernelized formulations of RELMs by second order cone programming. Furthermore, the computation of the covariance in feature space is discussed. Simulations and real-world indoor localization experiments are extensively carried out and the results demonstrate that the proposed algorithms can not only improve the accuracy and repeatability, but also reduce the deviation and worst case error of IPSs compared with other baseline algorithms.",2168-2267;21682267,,10.1109/TCYB.2015.2399420,National Research Foundation of Singapore; Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047752,Indoor positioning system (IPS);robust extreme learning machine (RELM);second order cone programming (SOCP),Approximation methods;IEEE 802.11 Standards;Kernel;Robustness;Support vector machines;Training;Vectors,convex programming;indoor navigation;learning (artificial intelligence);wireless LAN,IPS;RELM;SOCP;feature mapping;indoor localization system;indoor positioning system;kernelized formulation;location-based service;noisy measurement;random-hidden-node;robust extreme learning machine;second order cone programming,,11,,37,,,20150224,Jan. 2016,,IEEE,IEEE Journals & Magazines
Hierarchical Extreme Learning Machine-Polynomial Based Low Valued Capacitance Measurement Using Frequency Synthesizer–Vector Voltmeter,G. Sarkar; A. Chatterjee; A. Rakshit; K. Bhattacharya,"Electrical Engineering Department, Jadavpur University, Kolkata, India",IEEE Transactions on Instrumentation and Measurement,20140807,2014,63,9,2180,2187,"This present paper describes the development of a capacitance measurement system in the picofarad region. The system uses an universal serial bus port-based arrangement in conjunction with an indigenously developed Programmable Intelligent Computer microcontroller-based frequency synthesizer-vector voltmeter that can be used to measure the voltage in vector form and the capacitance can be determined using circuit solution technique. An intelligent two-layered, hierarchical reinforcement-based instrumentation scheme is proposed that can be integrated along with the original measurements to significantly improve the system performance. In layer 1, an extreme learning machine-based supervised phase reinforcement scheme is employed to improve the accuracy of the voltage measurement. Subsequently, in layer 2, local polynomial-based reinforcements are employed to improve both the resistive and reactive part measurements in the unknown capacitance. Three variants of ELM-based reinforcements are implemented for capacitance measurements in the range 100-10 000 pF and the utility of the hybrid ELM-polynomial-based reinforcements for such measurements is aptly demonstrated.",0018-9456;00189456,,10.1109/TIM.2014.2307991,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6766742,Capacitance measurement;extreme learning machine (ELM);frequency synthesizer-vector voltmeter (FSVV);polynomial;polynomial.,Capacitance;Capacitance measurement;Frequency measurement;Impedance measurement;Instruments;Vectors;Voltage measurement,capacitance measurement;computerised instrumentation;electric resistance measurement;frequency synthesizers;learning (artificial intelligence);microcontrollers;polynomials;voltage measurement;voltmeters,ELM-based reinforcement;capacitance 100 pF to 10000 pF;circuit solution technique;extreme learning machine-based supervised phase reinforcement scheme;frequency synthesizer-vector voltmeter;hierarchical extreme learning machine-polynomial;intelligent two-layered hierarchical reinforcement-based instrumentation scheme;low valued capacitance measurement;programmable intelligent computer microcontroller;reactive part measurement;resistive part measurement;universal serial bus port-based arrangement;voltage measurement,,3,,27,,,20140313,Sept. 2014,,IEEE,IEEE Journals & Magazines
VLSI Extreme Learning Machine: A Design Space Exploration,E. Yao; A. Basu,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,20161226,2017,25,1,60,74,"In this paper, we describe a compact low-power high-performance hardware implementation of extreme learning machine for machine learning applications. Mismatches in current mirrors are used to perform the vector-matrix multiplication that forms the first stage of this classifier and is the most computationally intensive. Both regression and classification (on UCI data sets) are demonstrated and a design space tradeoff between speed, power, and accuracy is explored. Our results indicate that for a wide set of problems, σ V<sub>T</sub> in the range of 15-25 mV gives optimal results. An input weight matrix rotation method to extend the input dimension and hidden layer size beyond the physical limits imposed by the chip is also described. This allows us to overcome a major limit imposed on most hardware machine learners. The chip is implemented in a 0.35-μm CMOS process and occupies a die area of around 5 mm × 5 mm. Operating from a 1 V power supply, it achieves an energy efficiency of 0.47 pJ/MAC at a classification rate of 31.6 kHz.",1063-8210;10638210,,10.1109/TVLSI.2016.2558842,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470473,Classifier;extreme learning machine (ELM);low power;machine learning;neural networks,Hardware;Machine learning algorithms;Mirrors;Neurons;Radiation detectors;Transistors;Very large scale integration,CMOS integrated circuits;VLSI;current mirrors;energy conservation;integrated circuit design;learning (artificial intelligence);low-power electronics;matrix multiplication;neural nets;pattern classification;power aware computing;regression analysis;vectors,CMOS process;VLSI extreme learning machine;classifier;current mirrors;design space exploration;design space tradeoff;energy efficiency;hidden layer size;input weight matrix rotation method;low-power high-performance hardware implementation;regression analysis;size 0.35 mum;vector-matrix multiplication;voltage 1 V,,3,,,,,20160517,Jan. 2017,,IEEE,IEEE Journals & Magazines
Remote Sensing Image Classification Based on Ensemble Extreme Learning Machine With Stacked Autoencoder,F. Lv; M. Han; T. Qiu,"Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China",IEEE Access,20170619,2017,5,,9021,9031,"Classification is one of the most popular topics in remote sensing. Consider the problems that the remote sensing data are complicated and few labeled training samples limit the performance and efficiency in the classification of remote sensing image. For these problems, a huge number of methods were proposed in the last two decades. However, most of them do not yield good performance. In this paper, a remote sensing image classification algorithm based on the ensemble of extreme learning machine (ELM) neural network, namely, stacked autoencoder (SAE)-ELM, is proposed. First, due to improve the ensemble classification accuracy, we adopt feature segmentation and SAE in the sample data to create high diversity among the base classifiers. Furthermore, ELM neural network is chosen as a base classifier to improve the learning speed of the algorithm. Finally, to determine the final ensemble-based classifier, Q-statistics is adopted. The experiment compares the proposed algorithm with Bagging, Adaboost, Random Forest et al., which results show that the proposed algorithm not only gets high classification accuracy on low resolution, medium resolution, high resolution and hyperspectral remote sensing images, but also has strong stability and generalization on UCI data.",2169-3536;21693536,,10.1109/ACCESS.2017.2706363,National Basic Research Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932442,Q-statistics;Remote sensing classification;ensemble algorithm;extreme learning machine;feature extraction,Feature extraction;Hyperspectral imaging;Image resolution;Neural networks;Training,feedforward neural nets;geophysical image processing;hyperspectral imaging;image classification;image coding;image resolution;learning (artificial intelligence);remote sensing;statistics,ELM neural network;Q-statistics;SAE-ELM;UCI data;ensemble classification;ensemble extreme learning machine;feature segmentation;high resolution remote sensing images;hyperspectral remote sensing images;learning speed;low resolution remote sensing images;medium resolution remote sensing images;remote sensing image classification;stacked autoencoder,,,,,,OAPA,20170523,2017,,IEEE,IEEE Journals & Magazines
Blind Domain Adaptation With Augmented Extreme Learning Machine Features,M. Uzair; A. Mian,"School of Computer Science and Software Engineering, University of Western Australia, Crawley, WA, Australia",IEEE Transactions on Cybernetics,20170520,2017,47,3,651,660,"In practical applications, the test data often have different distribution from the training data leading to suboptimal visual classification performance. Domain adaptation (DA) addresses this problem by designing classifiers that are robust to mismatched distributions. Existing DA algorithms use the unlabeled test data from target domain during training time in addition to the source domain data. However, target domain data may not always be available for training. We propose a blind DA algorithm that does not require target domain samples for training. For this purpose, we learn a global nonlinear extreme learning machine (ELM) model from the source domain data in an unsupervised fashion. The global ELM model is then used to initialize and learn class specific ELM models from the source domain data. During testing, the target domain features are augmented with the reconstructed features from the global ELM model. The resulting enriched features are then classified using the class specific ELM models based on minimum reconstruction error. Extensive experiments on 16 standard datasets show that despite blind learning, our method outperforms six existing state-of-the-art methods in cross domain visual recognition.",2168-2267;21682267,,10.1109/TCYB.2016.2523538,10.13039/501100000923 - Australian Research Council Discovery; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404267,Blind domain adaptation (DA);extreme learning machines (ELMs);object recognition;visual classification,Adaptation models;Cybernetics;Data models;Joining processes;Training;Training data;Visualization,image classification;image reconstruction;learning (artificial intelligence);object recognition,AELM;augmented extreme learning machine;blind DA;blind domain adaptation;feature reconstruction;source domain data;unsupervised fashion;visual classification performance;visual recognition,,2,,,,,20160211,Mar-17,,IEEE,IEEE Journals & Magazines
Deep Learning of Semisupervised Process Data With Hierarchical Extreme Learning Machine and Soft Sensor Application,L. Yao; Z. Ge,"Department of Control Science and Engineering, Zhejiang University, Hangzhou, China",IEEE Transactions on Industrial Electronics,20171211,2018,65,2,1490,1498,"Data-driven soft sensors have been widely utilized in industrial processes to estimate the critical quality variables which are intractable to directly measure online through physical devices. Due to the low sampling rate of quality variables, most of the soft sensors are developed on small number of labeled samples and the large number of unlabeled process data is discarded. The loss of information greatly limits the improvement of quality prediction accuracy. One of the main issues of data-driven soft sensor is to furthest exploit the information contained in all available process data. This paper proposes a semisupervised deep learning model for soft sensor development based on the hierarchical extreme learning machine (HELM). First, the deep network structure of autoencoders is implemented for unsupervised feature extraction with all the process samples. Then, extreme learning machine is utilized for regression through appending the quality variable. Meanwhile, the manifold regularization method is introduced for semisupervised model training. The new method can not only deeply extract the information that the data contains, but learn more from the extra unlabeled samples as well. The proposed semisupervised HELM method is applied in a high–low transformer to estimate the carbon monoxide content, which shows a significant improvement of the prediction accuracy, compared to traditional methods.",0278-0046;02780046,,10.1109/TIE.2017.2733448,10.13039/501100001809 - National Natural Science Foundation of China (NSFC); ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002611,Deep learning (DL);extreme learning machine (ELM);manifold regularization;semisupervised learning;soft sensor,Artificial neural networks;Biological neural networks;Feature extraction;Machine learning;Neurons;Process control;Training,,,,,,,,Traditional,20170804,Feb. 2018,,IEEE,IEEE Journals & Magazines
Regularized Weighted Circular Complex-Valued Extreme Learning Machine for Imbalanced Learning,S. Shukla; R. N. Yadav,"Department of Computer Science and Engineering, Maulana Azad National Institute of Technology, Bhopal, India",IEEE Access,20160112,2015,3,,3048,3057,"Extreme learning machine (ELM) is emerged as an effective, fast, and simple solution for real-valued classification problems. Various variants of ELM were recently proposed to enhance the performance of ELM. Circular complex-valued extreme learning machine (CC-ELM), a variant of ELM, exploits the capabilities of complex-valued neuron to achieve better performance. Another variant of ELM, weighted ELM (WELM) handles the class imbalance problem by minimizing a weighted least squares error along with regularization. In this paper, a regularized weighted CC-ELM (RWCC-ELM) is proposed, which incorporates the strength of both CC-ELM and WELM. Proposed RWCC-ELM is evaluated using imbalanced data sets taken from Keel repository. RWCC-ELM outperforms CC-ELM and WELM for most of the evaluated data sets.",2169-3536;21693536,,10.1109/ACCESS.2015.2506601,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349136,Class imbalance problem;Complex valued neural network;Extreme Learning Machine;Real valued classification;Regularization;Weighted least squares error;class imbalance problem;complex valued neural network;extreme learning machine;regularization;weighted least squares error,Algorithm design and analysis;Biological neural networks;Classification;Learning systems;Least squares methods;Neurons;Signal processing algorithms,learning (artificial intelligence);least squares approximations;pattern classification,Keel repository;complex-valued neuron;imbalanced learning;real-valued classification problems;regularized weighted CC-ELM;regularized weighted circular complex-valued extreme learning machine;weighted ELM;weighted least squares error,,2,,39,,OAPA,20151208,2015,,IEEE,IEEE Journals & Magazines
Sequential Nonlinear Learning for Distributed Multiagent Systems via Extreme Learning Machines,N. D. Vanli; M. O. Sayin; I. Delibalta; S. S. Kozat,"Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey",IEEE Transactions on Neural Networks and Learning Systems,20170520,2017,28,3,546,558,"We study online nonlinear learning over distributed multiagent systems, where each agent employs a single hidden layer feedforward neural network (SLFN) structure to sequentially minimize arbitrary loss functions. In particular, each agent trains its own SLFN using only the data that is revealed to itself. On the other hand, the aim of the multiagent system is to train the SLFN at each agent as well as the optimal centralized batch SLFN that has access to all the data, by exchanging information between neighboring agents. We address this problem by introducing a distributed subgradient-based extreme learning machine algorithm. The proposed algorithm provides guaranteed upper bounds on the performance of the SLFN at each agent and shows that each of these individual SLFNs asymptotically achieves the performance of the optimal centralized batch SLFN. Our performance guarantees explicitly distinguish the effects of data- and network-dependent parameters on the convergence rate of the proposed algorithm. The experimental results illustrate that the proposed algorithm achieves the oracle performance significantly faster than the state-of-the-art methods in the machine learning and signal processing literature. Hence, the proposed method is highly appealing for the applications involving big data.",2162-237X;2162237X,,10.1109/TNNLS.2016.2536649,Turkish Academy of Sciences Outstanding Researcher Programme within the Scientific and Technological Research Council of Turkey; T¿rk Telekom Laboratories; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7431997,Distributed systems;extreme learning machine (ELM);multiagent optimization;sequential learning;single hidden layer feedforward neural networks (SLFNs),Convergence;Cost function;Learning systems;Machine learning algorithms;Signal processing;Signal processing algorithms,learning (artificial intelligence);multi-agent systems;neural nets,Big Data;SLFN;distributed multiagent systems;distributed subgradient-based extreme learning machine algorithm;neighboring agents;sequential nonlinear learning;single hidden layer feedforward neural network,,,,,,,20160311,Mar-17,,IEEE,IEEE Journals & Magazines
Fuzzy extreme learning machine for classification,W. B. Zhang; H. B. Ji,"School of Electronic Engineering, Xidian University, PO Box 229, Xi'an, 710071, People's Republic of China",Electronics Letters,20130422,2013,49,7,448,450,"Compared to traditional classifiers, such as SVM, the extreme learning machine (ELM) achieves similar performance for classification and runs at a much faster learning speed. However, in many real applications, the different input points may not be exactly assigned to one of the classes, such as the imbalance problems and the weighted classification problems. The traditional ELM lacks the ability to solve those problems. Proposed is a fuzzy ELM, which introduces a fuzzy membership to the traditional ELM method. Then, the inputs with different fuzzy matrix can make different contributions to the learning of the output weights. For the weighted classification problems, FELM can provide a more logical result than that of ELM.",0013-5194;00135194,,10.1049/el.2012.3642,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504956,,,fuzzy set theory;learning (artificial intelligence);matrix algebra;pattern classification,FELM;fuzzy ELM;fuzzy extreme learning machine;fuzzy matrix;fuzzy membership;imbalance problem;weighted classification problem,,2,,,,,,March 28 2013,,IET,IET Journals & Magazines
Short-term load forecasting of Australian National Electricity Market by an ensemble model of extreme learning machine,R. Zhang; Z. Y. Dong; Y. Xu; K. Meng; K. P. Wong,"Centre for Intelligent Electricity Networks, University of Newcastle, Newcastle, Australia","IET Generation, Transmission & Distribution",20130617,2013,7,4,391,397,"Artificial Neural Network (ANN) has been recognized as a powerful method for short-term load forecasting (STLF) of power systems. However, traditional ANNs are mostly trained by gradient-based learning algorithms which usually suffer from excessive training and tuning burden as well as unsatisfactory generalization performance. Based on the ensemble learning strategy, this paper develops an ensemble model of a promising novel learning technology called extreme learning machine (ELM) for high-quality STLF of Australian National Electricity Market (NEM). The model consists of a series of single ELMs. During the training, the ensemble model generalizes the randomness of single ELMs by selecting not only random input parameters but also random hidden nodes within a pre-defined range. The forecast result is taken as the median value the single ELM outputs. Owing to the very fast training/tuning speed of ELM, the model can be efficiently updated to on-line track the variation trend of the electricity load and maintain the accuracy. The developed model is tested with the NEM historical load data and its performance is compared with some state-of-the-art learning algorithms. The results show that the training efficiency and the forecasting accuracy of the developed model are superior over the competitive algorithms.",1751-8687;17518687,,10.1049/iet-gtd.2012.0541,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530985,,,gradient methods;learning (artificial intelligence);load forecasting;neural nets;power engineering computing;power markets,ANN;Australian NEM;Australian National Electricity Market;ELM learning technique;NEM historical load data;STLF problem;artificial neural network;electricity load variation;ensemble model;extreme learning machine;forecasting accuracy;gradient-based learning algorithm;power system operations;pre-defined range;short-term load forecasting;stability problem;training speed;training-tuning speed,,16,,,,,,Apr-13,,IET,IET Journals & Magazines
Stacked Extreme Learning Machines,H. Zhou; G. B. Huang; Z. Lin; H. Wang; Y. C. Soh,"School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang, Singapore",IEEE Transactions on Cybernetics,20170520,2015,45,9,2013,2025,"Extreme learning machine (ELM) has recently attracted many researchers' interest due to its very fast learning speed, good generalization ability, and ease of implementation. It provides a unified solution that can be used directly to solve regression, binary, and multiclass classification problems. In this paper, we propose a stacked ELMs (S-ELMs) that is specially designed for solving large and complex data problems. The S-ELMs divides a single large ELM network into multiple stacked small ELMs which are serially connected. The S-ELMs can approximate a very large ELM network with small memory requirement. To further improve the testing accuracy on big data problems, the ELM autoencoder can be implemented during each iteration of the S-ELMs algorithm. The simulation results show that the S-ELMs even with random hidden nodes can achieve similar testing accuracy to support vector machine (SVM) while having low memory requirements. With the help of ELM autoencoder, the S-ELMs can achieve much better testing accuracy than SVM and slightly better accuracy than deep belief network (DBN) with much faster training speed.",2168-2267;21682267,,10.1109/TCYB.2014.2363492,Singapore Academic Research Fund (AcRF) Tier 1; Singapore¿s National Research Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6937189,Deep learning;eigenvalue;extreme learning machine (ELM);feature mapping;principal component analysis (PCA);support vector machines (SVMs),Accuracy;Covariance matrices;Eigenvalues and eigenfunctions;Principal component analysis;Support vector machines;Testing;Training,Big Data;feedforward neural nets,ELM autoencoder;ELM network;S-ELM;big data problems;extreme learning machine;generalized single-hidden layer feed-forward networks;random hidden nodes;stacked ELM,,14,,43,,,20141028,Sept. 2015,,IEEE,IEEE Journals & Magazines
Low-Discrepancy Points for Deterministic Assignment of Hidden Weights in Extreme Learning Machines,C. Cervellera; D. Macciò,"National Research Council, Institute of Intelligent Systems for Automation, Genoa, Italy",IEEE Transactions on Neural Networks and Learning Systems,20160315,2016,27,4,891,896,"The traditional extreme learning machine (ELM) approach is based on a random assignment of the hidden weight values, while the linear coefficients of the output layer are determined analytically. This brief presents an analysis based on geometric properties of the sampling points used to assign the weight values, investigating the replacement of random generation of such values with low-discrepancy sequences (LDSs). Such sequences are a family of sampling methods commonly employed for numerical integration, yielding a more efficient covering of multidimensional sets with respect to random sequences, without the need for any computationally intensive procedure. In particular, we prove that the universal approximation property of the ELM is guaranteed when LDSs are employed, and how an efficient covering affects the convergence positively. Furthermore, since LDSs are generated deterministically, the results do not have a probabilistic nature. Simulation results confirm, in practice, the good theoretical properties given by the combination of ELM with LDSs.",2162-237X;2162237X,,10.1109/TNNLS.2015.2424999,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103315,Discrepancy;extreme learning machines (ELMs);low-discrepancy sequences (LDSs);universal approximation;universal approximation.,Approximation algorithms;Approximation methods;Convergence;Learning systems;Probabilistic logic;Standards;Training,approximation theory;learning (artificial intelligence);numerical analysis,ELM;LDS;deterministic assignment;extreme learning machines;geometric properties;hidden weight values;hidden weights;low discrepancy points;low-discrepancy sequences;sampling methods;sampling points;universal approximation property,,5,,20,,,20150507,Apr-16,,IEEE,IEEE Journals & Magazines
Depth-Based Human Fall Detection via Shape Features and Improved Extreme Learning Machine,X. Ma; H. Wang; B. Xue; M. Zhou; B. Ji; Y. Li,"School of Control Science and Engineering, Shandong University, Jinan, China",IEEE Journal of Biomedical and Health Informatics,20170520,2014,18,6,1915,1922,"Falls are one of the major causes leading to injury of elderly people. Using wearable devices for fall detection has a high cost and may cause inconvenience to the daily lives of the elderly. In this paper, we present an automated fall detection approach that requires only a low-cost depth camera. Our approach combines two computer vision techniques-shape-based fall characterization and a learning-based classifier to distinguish falls from other daily actions. Given a fall video clip, we extract curvature scale space (CSS) features of human silhouettes at each frame and represent the action by a bag of CSS words (BoCSS). Then, we utilize the extreme learning machine (ELM) classifier to identify the BoCSS representation of a fall from those of other actions. In order to eliminate the sensitivity of ELM to its hyperparameters, we present a variable-length particle swarm optimization algorithm to optimize the number of hidden neurons, corresponding input weights, and biases of ELM. Using a low-cost Kinect depth camera, we build an action dataset that consists of six types of actions (falling, bending, sitting, squatting, walking, and lying) from ten subjects. Experimenting with the dataset shows that our approach can achieve up to 91.15% sensitivity, 77.14% specificity, and 86.83% accuracy. On a public dataset, our approach performs comparably to state-of-the-art fall detection methods that need multiple cameras.",2168-2194;21682194,,10.1109/JBHI.2014.2304357,; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730899,Curvature scale space (CSS);extreme learning machine (ELM);fall detection;particle swarm optimization;shape contour,Accuracy;Algorithm design and analysis;Cameras;Computer vision;Feature extraction;Particle swarm optimization,bending;biomedical optical imaging;cameras;feature extraction;gait analysis;geriatrics;image classification;image representation;learning (artificial intelligence);mechanoception;medical image processing;particle swarm optimisation,BoCSS representation;Kinect depth camera;bending;computer vision technique-shape-based fall characterization;curvature scale space feature extraction;depth-based human fall detection;elderly people;extreme learning machine classifier;fall video clip;injury;lying;sitting;squatting;variable-length particle swarm optimization algorithm;walking;wearable devices,"0;Accidental Falls;Algorithms;Artificial Intelligence;Female;Humans;Image Processing, Computer-Assisted;Male;Models, Statistical;Monitoring, Ambulatory;Posture;Sensitivity and Specificity;Walking",37,,37,,OAPA,20140203,Nov. 2014,,IEEE,IEEE Journals & Magazines
Dynamic Delay Predictions for Large-Scale Railway Networks: Deep and Shallow Extreme Learning Machines Tuned via Thresholdout,L. Oneto; E. Fumeo; G. Clerico; R. Canepa; F. Papa; C. Dambra; N. Mazzino; D. Anguita,"Department of Informatics, BioEngineering, Robotics, and Systems Engineering, University of Genoa, Genoa, Italy","IEEE Transactions on Systems, Man, and Cybernetics: Systems",20170914,2017,47,10,2754,2767,"Current train delay (TD) prediction systems do not take advantage of state-of-the-art tools and techniques for handling and extracting useful and actionable information from the large amount of endogenous (i.e., generated by the railway system itself) and exogenous (i.e., related to railway operation but generated by external phenomena) data available. Additionally, they are not designed in order to deal with the intrinsic time varying nature of the problem (e.g., regular changes in the nominal timetable, etc.). The purpose of this paper is to build a dynamic data-driven TD prediction system that exploits the most recent tools and techniques in the field of time varying big data analysis. In particular, we map the TD prediction problem into a time varying multivariate regression problem that allows exploiting both historical data about the train movements and exogenous data about the weather provided by the national weather services. The performance of these methods have been tuned through the state-of-the-art thresholdout technique, a very powerful procedure which relies on the differential privacy theory. Finally, the performance of two efficient implementations of shallow and deep extreme learning machines that fully exploit the recent in-memory large-scale data processing technologies have been compared with the current state-of-the-art TD prediction systems. Results on real-world data coming from the Italian railway network show that the proposal of this paper is able to remarkably improve the state-of-the-art systems.",2168-2216;21682216,,10.1109/TSMC.2017.2693209,European Union through the Projects Capacity for Rail&#8212;C4R; Innovative Intelligent Rail&#8212;In2Rail; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917288,Apache Spark;big data;deep extreme learning machine (DELM);delay prediction;dynamic varying systems;in-memory computing;intelligent transportation systems;model selection (MS);railway;shallow extreme learning machine (SELM);thresholdout,Big Data;Data models;Delays;Meteorology;Predictive models;Rail transportation;Tools,learning (artificial intelligence);railways;regression analysis,differential privacy theory;dynamic data-driven TD prediction system;dynamic delay predictions;extreme learning machines;large-scale railway networks;time varying big data analysis;time varying multivariate regression problem,,,,,,OAPA,20170502,Oct. 2017,,IEEE,IEEE Journals & Magazines
Cost-Sensitive AdaBoost Algorithm for Ordinal Regression Based on Extreme Learning Machine,A. Riccardi; F. Fernández-Navarro; S. Carloni,"Advanced Concepts Team, European Space Research and Technology Centre, European Space Agency, Noordwijk, The Netherlands",IEEE Transactions on Cybernetics,20140912,2014,44,10,1898,1909,"In this paper, the well known stagewise additive modeling using a multiclass exponential (SAMME) boosting algorithm is extended to address problems where there exists a natural order in the targets using a cost-sensitive approach. The proposed ensemble model uses an extreme learning machine (ELM) model as a base classifier (with the Gaussian kernel and the additional regularization parameter). The closed form of the derived weighted least squares problem is provided, and it is employed to estimate analytically the parameters connecting the hidden layer to the output layer at each iteration of the boosting algorithm. Compared to the state-of-the-art boosting algorithms, in particular those using ELM as base classifier, the suggested technique does not require the generation of a new training dataset at each iteration. The adoption of the weighted least squares formulation of the problem has been presented as an unbiased and alternative approach to the already existing ELM boosting techniques. Moreover, the addition of a cost model for weighting the patterns, according to the order of the targets, enables the classifier to tackle ordinal regression problems further. The proposed method has been validated by an experimental study by comparing it with already existing ensemble methods and ELM techniques for ordinal regression, showing competitive results.",2168-2267;21682267,,10.1109/TCYB.2014.2299291,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6719563,Boosting;SAMME algorithm;extreme learning machine;neural networks;ordinal regression,Artificial neural networks;Boosting;Kernel;Prediction algorithms;Training;Vectors,learning (artificial intelligence);least squares approximations;regression analysis,ELM model;Gaussian kernel;SAMME boosting algorithm;cost model;cost-sensitive AdaBoost algorithm;ensemble model;extreme learning machine;ordinal regression;regularization parameter;stagewise additive modeling using a multiclass exponential;weighted least squares problem,0,21,,63,,,20140122,Oct. 2014,,IEEE,IEEE Journals & Magazines
Extreme Learning Machine for Multilayer Perceptron,J. Tang; C. Deng; G. B. Huang,"School of Information and Electronics, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Neural Networks and Learning Systems,20170520,2016,27,4,809,821,"Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ<sub>1</sub> constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.",2162-237X;2162237X,,10.1109/TNNLS.2015.2424995,Excellent Young Scholars Research Fund of Beijing Institute of Technology; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103337,Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping;random feature mapping.,Artificial neural networks;Feature extraction;Least squares approximations;Nonhomogeneous media;Optimization;Training,feedforward neural nets;learning (artificial intelligence);multilayer perceptrons;pattern classification,DL;ELM random feature mapping;ELM-based hierarchical learning framework;ELM-based sparse autoencoder;computer vision;decision making;deep learning;extreme learning machine;feature learning;feature representations;generalized single hidden layer feedforward neural networks;greedy layerwise training;hierarchical learning methods;hierarchically encoded outputs;learning algorithm;learning speed;multilayer perceptron;random initialized hidden weights;self-taught feature extraction;supervised feature classification;unsupervised multilayer encoding,,105,,33,,,20150507,Apr-16,,IEEE,IEEE Journals & Magazines
An Energy-Efficient Nonvolatile In-Memory Computing Architecture for Extreme Learning Machine by Domain-Wall Nanowire Devices,Y. Wang; H. Yu; L. Ni; G. B. Huang; M. Yan; C. Weng; W. Yang; J. Zhao,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Nanotechnology,20151109,2015,14,6,998,1012,"The data-oriented applications have introduced increased demands on memory capacity and bandwidth, which raises the need to rethink the architecture of the current computing platforms. The logic-in-memory architecture is highly promising as future logic-memory integration paradigm for high throughput data-driven applications. From memory technology aspect, as one recently introduced nonvolatile memory device, domain-wall nanowire (or race-track) not only shows potential as future power efficient memory, but also computing capacity by its unique physics of spintronics. This paper explores a novel distributed in-memory computing architecture where most logic functions are executed within the memory, which significantly alleviates the bandwidth congestion issue and improves the energy efficiency. The proposed distributed in-memory computing architecture is purely built by domain-wall nanowire, i.e., both memory and logic are implemented by domain-wall nanowire devices. As a case study, neural network-based image resolution enhancement algorithm, called DW-NN, is examined within the proposed architecture. We show that all operations involved in machine learning on neural network can be mapped to a logic-in-memory architecture by nonvolatile domain-wall nanowire. Domain-wall nanowire-based logic is customized for in machine learning within image data storage. As such, both neural network training and processing can be performed locally within the memory. The experimental results show that the domain-wall memory can reduce 92% leakage power and 16% dynamic power compared to main memory implemented by DRAM; and domain-wall logic can reduce 31% both dynamic and 65% leakage power under the similar performance compared to CMOS transistor-based logic. And system throughput in DW-NN is improved by 11.6x and the energy efficiency is improved by 56x when compared to conventional image processing system.",1536-125X;1536125X,,10.1109/TNANO.2015.2447531,A*STAR PSF; Huawei Shannon Research Lab; MOE Tier-2; Singapore NRF-CRP; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7128727,Domain wall;extreme learning machine;in-memory computing;nonvolatile memory,Magnetic tunneling;Memory architecture;Nanoscale devices;Nanowires;Nonvolatile memory;Random access memory,electric domain walls;learning (artificial intelligence);nanowires;neural nets;random-access storage,DW-NN;bandwidth congestion issue;computing capacity;data-oriented applications;distributed in-memory computing architecture;domain-wall nanowire devices;domain-wall nanowire-based logic;energy efficiency;future logic-memory integration paradigm;image data storage;logic functions;logic-in-memory architecture;machine learning;neural network training;neural network-based image resolution enhancement algorithm;nonvolatile domain-wall nanowire;nonvolatile memory device,,20,,45,,,20150619,Nov. 2015,,IEEE,IEEE Journals & Magazines
Online Spatiotemporal Extreme Learning Machine for Complex Time-Varying Distributed Parameter Systems,X. Lu; F. Yin; C. Liu; M. Huang,"State Key Laboratory of High Performance, Complex Manufacturing, School of Mechanical and Electrical Engineering, Central South University, Changsha, China",IEEE Transactions on Industrial Informatics,20170803,2017,13,4,1753,1762,"Many industrial processes are complex nonlinear distributed parameter systems (DPSs) with time-varying spatiotemporal dynamics. However, time-varying spatiotemporal dynamics and the nonlinear relationships between spatial points are currently not given much consideration in the existing data-driven modeling methods. Thus, accurately modeling a nonlinear DPS with time-varying spatiotemporal dynamics using these current methods is challenging. Here, we propose a spatiotemporal extreme learning machine (ELM) to accurately model time-varying and nonlinear DPSs. First, we develop the nonlinear spatial activation function to describe the nonlinear relationships between spatial points. As a result, in contrast to the traditional ELM method which is only used to model the temporal dynamics, the spatiotemporal ELM inherently takes the spatial information into consideration. Next, an online time coefficient model is developed, which accounts for the time-varying temporal dynamics of the DPS. After the integration of the spatial activation function with the time coefficient model, this modeling method is able to adapt to real-time spatiotemporal variation. Unlike the existing data-driven DPS modeling approaches, the proposed method has the capability to accurately represent the nonlinear relationships between spatial points and has the adaptive ability for modeling time-varying dynamics. Finally, through application on practical curing experiments, the proposed method can improve the modeling precision for an unknown, time-varying, and nonlinear DPS due to smaller modeling error as compared to the several commonly used DPS modeling methods.",1551-3203;15513203,,10.1109/TII.2017.2666841,Fundamental Research Funds for the Central Universities of Central South University; Hunan Province Science and Technology Plan; Hunan Provincial Natural Science Foundation of China; Project of Innovation-Driven Plan; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100002822 - Central South University; 10.13039/501100004602 - Program for New Century Excellent Talents in University; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849212,Distributed parameter system (DPS);extreme learning machine (ELM);modeling;nonlinear process;time-varying,Adaptation models;Data models;Heating;Mathematical model;Power system dynamics;Spatiotemporal phenomena;Time-varying systems,curing;distributed parameter systems;large-scale systems;learning systems;nonlinear systems;process control;time-varying systems,DPS;complex nonlinear distributed parameter systems;complex time-varying distributed parameter systems;data-driven modeling method;industrial process;modeling error;nonlinear relationship;nonlinear spatial activation function;online spatiotemporal extreme learning machine;online time coefficient model;practical curing experiment;real-time spatiotemporal variation;spatial points;spatiotemporal ELM;time-varying spatiotemporal dynamics,,1,,,,,20170209,Aug. 2017,,IEEE,IEEE Journals & Magazines
Dynamic Extreme Learning Machine and Its Approximation Capability,R. Zhang; Y. Lan; G. B. Huang; Z. B. Xu; Y. C. Soh,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Cybernetics,20131119,2013,43,6,2054,2065,"Extreme learning machines (ELMs) have been proposed for generalized single-hidden-layer feedforward networks which need not be neuron alike and perform well in both regression and classification applications. The problem of determining the suitable network architectures is recognized to be crucial in the successful application of ELMs. This paper first proposes a dynamic ELM (D-ELM) where the hidden nodes can be recruited or deleted dynamically according to their significance to network performance, so that not only the parameters can be adjusted but also the architecture can be self-adapted simultaneously. Then, this paper proves in theory that such D-ELM using Lebesgue p-integrable hidden activation functions can approximate any Lebesgue p-integrable function on a compact input set. Simulation results obtained over various test problems demonstrate and verify that the proposed D-ELM does a good job reducing the network size while preserving good generalization performance.",2168-2267;21682267,,10.1109/TCYB.2013.2239987,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459569,Dynamic learning;extreme learning machine (ELM);feedforward neural networks;universal approximation,Approximation methods;Computer architecture;Cybernetics;Educational institutions;Feedforward neural networks;Linear systems;Machine learning,learning (artificial intelligence);pattern classification;recurrent neural nets;regression analysis,D-ELM;Lebesgue p-integrable hidden activation functions;approximation capability;classification applications;dynamic ELM;dynamic extreme learning machine;generalized single-hidden-layer feedforward networks;network architectures;regression applications,0,27,,36,,,20130211,Dec. 2013,,IEEE,IEEE Journals & Magazines
Online Sequential Extreme Learning Machine With Kernels,S. Scardapane; D. Comminiello; M. Scarpiniti; A. Uncini,"Department of Information EngineeringElectronics and Telecommunications, Sapienza University of Rome, Rome, Italy",IEEE Transactions on Neural Networks and Learning Systems,20150817,2015,26,9,2214,2220,"The extreme learning machine (ELM) was recently proposed as a unifying framework for different families of learning algorithms. The classical ELM model consists of a linear combination of a fixed number of nonlinear expansions of the input vector. Learning in ELM is hence equivalent to finding the optimal weights that minimize the error on a dataset. The update works in batch mode, either with explicit feature mappings or with implicit mappings defined by kernels. Although an online version has been proposed for the former, no work has been done up to this point for the latter, and whether an efficient learning algorithm for online kernel-based ELM exists remains an open problem. By explicating some connections between nonlinear adaptive filtering and ELM theory, in this brief, we present an algorithm for this task. In particular, we propose a straightforward extension of the well-known kernel recursive least-squares, belonging to the kernel adaptive filtering (KAF) family, to the ELM framework. We call the resulting algorithm the kernel online sequential ELM (KOS-ELM). Moreover, we consider two different criteria used in the KAF field to obtain sparse filters and extend them to our context. We show that KOS-ELM, with their integration, can result in a highly efficient algorithm, both in terms of obtained generalization error and training time. Empirical evaluations demonstrate interesting results on some benchmarking datasets.",2162-237X;2162237X,,10.1109/TNNLS.2014.2382094,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000606,Extreme learning machine (ELM);kernel;online learning;recursive least square (RLS).,Accuracy;Computational modeling;Context;Kernel;Support vector machines;Training;Vectors,,,,23,,30,,,20141231,Sept. 2015,,IEEE,IEEE Journals & Magazines
"<formula formulatype=""inline""><tex Notation=""TeX"">${{rm E}^{2}}{rm LMs}$</tex> </formula>: Ensemble Extreme Learning Machines for Hyperspectral Image Classification",A. Samat; P. Du; S. Liu; J. Li; L. Cheng,"Key Laboratory for Satellite Mapping Technology and Applications of State Administration of Surveying, Mapping, and Geoinformation of China, Nanjing University, Nanjing, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,20140422,2014,7,4,1060,1069,"Extreme learning machine (ELM) has attracted attentions in pattern recognition field due to its remarkable advantages such as fast operation, straightforward solution, and strong generalization. However, the performance of ELM for high-dimensional data, such as hyperspectral image, is still an open problem. Therefore, in this paper, we introduce ELM for hyperspectral image classification. Furthermore, in order to overcome the drawbacks of ELM caused by the randomness of input weights and bias, two new algorithms of ensemble extreme learning machines (Bagging-based and AdaBoost-based ELMs) are proposed for the classification task. In order to illustrate the performance of the proposed algorithms, support vector machines (SVMs) are used for evaluation and comparison. Experimental results with real hyperspectral images collected by reflective optics spectrographic image system (ROSIS) and airborne visible/infrared imaging spectrometer (AVIRIS) indicate that the proposed ensemble algorithms produce excellent classification performance in different scenarios with respect to spectral and spectral-spatial feature sets.",1939-1404;19391404,,10.1109/JSTARS.2014.2301775,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732910,"Bagging-based ensemble extreme learning machines (BagELMs);boostELMs;classification;ensemble extreme learning machines (<formula formulatype=""inline""><tex Notation=""TeX"">${{bf E}^{2}}{bf LMs}$</tex> </formula>);ensemble learning (EL);extreme learning machine (ELM);hyperspectral remote sensing",Educational institutions;Hyperspectral imaging;Neurons;Support vector machines;Training,geophysical image processing;hyperspectral imaging;image classification;infrared imaging;infrared spectra;learning (artificial intelligence);support vector machines;visible spectra,AVIRIS;AdaBoost-based ELM;E<sup>2</sup>LM;ROSIS;SVM;airborne visible-infrared imaging spectrometer;bagging-based ELM;ensemble extreme learning machine;hyperspectral image classification;pattern recognition;reflective optics spectrographic image system;spectral-spatial feature set;support vector machine,,61,,40,,,20140205,Apr-14,,IEEE,IEEE Journals & Magazines
GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data,C. Chen; K. Li; A. Ouyang; Z. Tang; K. Li,"College of Information Science and Engineering, Hunan University, Changsha, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems",20170914,2017,47,10,2740,2753,"The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.",2168-2216;21682216,,10.1109/TSMC.2017.2690673,International Science and Technology Cooperation Program of China; Key Technology Research and Development Programs of Guangdong Province; National High-Tech Research and Development Program of China; 10.13039/501100001809 - International (Regional) Cooperation and Exchange Program of National Natural Science Foundation of China; 10.13039/501100001809 - Key Program of National Natural Science Foundation of China; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100001809 - National Outstanding Youth Science Program of National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7908958,Big data;GPGPU;deep learning (DL);flink;hierarchical extreme learning machine (H-ELM);parallel,Acceleration;Approximation algorithms;Big Data;Clustering algorithms;Libraries;Machine learning algorithms;Training,Big Data;graphics processing units;learning (artificial intelligence);parallel algorithms,Big Data;Flink;GPU-accelerated parallel hierarchical extreme learning machine;Java virtual machine;graphics processing units;in-memory cluster computing platforms;multilayer perceptron;parallel H-ELM algorithm;single hidden layer feedforward networks,,,,,,OAPA,20170424,Oct. 2017,,IEEE,IEEE Journals & Magazines
Dynamic Adjustment of Hidden Node Parameters for Extreme Learning Machine,G. Feng; Y. Lan; X. Zhang; Z. Qian,"School of Communication and Information Engineering, Shanghai University, Shanghai, China",IEEE Transactions on Cybernetics,20170520,2015,45,2,279,288,"Extreme learning machine (ELM), proposed by Huang et al., was developed for generalized single hidden layer feedforward networks with a wide variety of hidden nodes. ELMs have been proved very fast and effective especially for solving function approximation problems with a predetermined network structure. However, it may contain insignificant hidden nodes. In this paper, we propose dynamic adjustment ELM (DA-ELM) that can further tune the input parameters of insignificant hidden nodes in order to reduce the residual error. It is proved in this paper that the energy error can be effectively reduced by applying recursive expectation-minimization theorem. In DA-ELM, the input parameters of insignificant hidden node are updated in the decreasing direction of the energy error in each step. The detailed theoretical foundation of DA-ELM is presented in this paper. Experimental results show that the proposed DA-ELM is more efficient than the state-of-art algorithms such as Bayesian ELM, optimally-pruned ELM, two-stage ELM, Levenberg-Marquardt, sensitivity-based linear learning method as well as the preliminary ELM.",2168-2267;21682267,,10.1109/TCYB.2014.2325594,Innovation Program of Shanghai Municipal Education Commission; Qualified Personnel Foundation of Taiyuan University of Technology; Shanghai Rising-Star Program; Taiyuan University of Technology Group Fund; 10.13039/100007219 - Natural Science Foundation of Shanghai; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826504,Adjustment of hidden node parameters;error minimized approximation;extreme learning machine;least squares method,Approximation algorithms;Function approximation;Heuristic algorithms;Support vector machines;Training;Vectors,expectation-maximisation algorithm;feedforward neural nets;function approximation;learning (artificial intelligence),DA-ELM;dynamic adjustment ELM;energy error;extreme learning machine;function approximation problems;generalized single hidden layer feedforward networks;hidden node parameter dynamic adjustment;recursive expectation-minimization theorem,,13,,30,,,20140605,Feb. 2015,,IEEE,IEEE Journals & Magazines
Physical-Layer Authentication Based on Extreme Learning Machine,N. Wang; T. Jiang; S. Lv; L. Xiao,"Key Laboratory of Universal Wireless Communication Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Communications Letters,20170711,2017,21,7,1557,1560,"Most physical-layer authentication techniques use hypothesis tests to compare the radio channel information with the channel record of Alice to detect spoofer Eve in wireless networks. However, the test threshold in the hypothesis test is not always available, especially in dynamic networks. In this letter, we propose a physical-layer authentication scheme based on extreme learning machine that exploit multi-dimensional characters of radio channels and use the training data generated from the spoofing model to improve the spoofing detection accuracy. Simulation results show that our proposed technique can significantly improve the authentication accuracy compared with the state-of-the-art method.",1089-7798;10897798,,10.1109/LCOMM.2017.2690437,CCF-Venustech Hongyan Research Initiative (2016-010); 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891506,Physical layer authentication;extreme learning machine;wireless security,Authentication;Correlation;Data models;Measurement;Programmable logic arrays;Training data;Wireless communication,cognitive radio;learning (artificial intelligence);telecommunication channels;telecommunication computing;telecommunication security,dynamic networks;extreme learning machine;multidimensional characters;physical-layer authentication scheme;physical-layer authentication techniques;radio channel information,,,,,,,20170403,Jul-17,,IEEE,IEEE Journals & Magazines
Experimental Study on Extreme Learning Machine Applications for Speech Enhancement,T. Hussain; S. M. Siniscalchi; C. C. Lee; S. S. Wang; Y. Tsao; W. H. Liao,"Taiwan International Graduate Program, Social Network and Human Centered Computing Program, Institute of Information Science, Academia Sinica, Taipei, Taiwan",IEEE Access,20171207,2017,5,,25542,25554,"In wireless telephony and audio data mining applications, it is desirable that noise suppression can be made robust against changing noise conditions and operates in real time (or faster). The learning effectiveness and speed of artificial neural networks are therefore critical factors in applications for speech enhancement tasks. To address these issues, we present an extreme learning machine (ELM) framework, aimed at the effective and fast removal of background noise from a single-channel speech signal, based on a set of randomly chosen hidden units and analytically determined output weights. Because feature learning with shallow ELM may not be effective for natural signals, such as speech, even with a large number of hidden nodes, hierarchical ELM (H-ELM) architectures are deployed by leveraging sparse autoencoders. In this manner, we not only keep all the advantages of deep models in approximating complicated functions and maintaining strong regression capabilities, but we also overcome the cumbersome and time-consuming features of both greedy layer-wise pre-training and back-propagation (BP)-based fine tuning schemes, which are typically adopted for training deep neural architectures. The proposed ELM framework was evaluated on the Aurora-4 speech database. The Aurora-4 task provides relatively limited training data, and test speech data corrupted with both additive noise and convolutive distortions for matched and mismatched channels and signal-to-noise ratio (SNR) conditions. In addition, the task includes a subset of testing data involving noise types and SNR levels that are not seen in the training data. The experimental results indicate that when the amount of training data is limited, both ELMand H-ELM-based speech enhancement techniques consistently outperform the conventional BP-based shallow and deep learning algorithms, in terms of standardized objective evaluations, under various testing conditions.",,,10.1109/ACCESS.2017.2766675,National Science Council of Taiwan; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085130,Speech enhancement;artificial neural networks;extreme learning machine;hierarchical extreme learning machines,Noise measurement;Noise reduction;Signal to noise ratio;Speech;Speech enhancement;Training data,backpropagation;data mining;feedforward neural nets;learning (artificial intelligence);regression analysis;signal denoising;speech enhancement,Aurora-4 speech database;ELM framework;ELMand H-ELM;additive noise;artificial neural networks;audio data mining applications;background noise;complicated functions;critical factors;deep learning algorithms;deep models;deep neural architectures;extreme learning machine applications;fine tuning schemes;greedy layer-wise pre-training;hidden nodes;hierarchical ELM architectures;learning effectiveness;mismatched channels;natural signals;noise conditions;noise suppression;noise types;output weights;randomly chosen hidden units;regression capabilities;shallow ELM;single-channel speech signal;sparse autoencoders;speech enhancement tasks;test speech data;testing conditions;testing data;training data;wireless telephony,,,,,,,20171026,2017,,IEEE,IEEE Journals & Magazines
Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II),S. Lin; X. Liu; J. Fang; Z. Xu,"College of Mathematics and Information Science, Wenzhou University, Wenzhou, China",IEEE Transactions on Neural Networks and Learning Systems,20170520,2015,26,1,21,34,"An extreme learning machine (ELM) can be regarded as a two-stage feed-forward neural network (FNN) learning system that randomly assigns the connections with and within hidden neurons in the first stage and tunes the connections with output neurons in the second stage. Therefore, ELM training is essentially a linear learning problem, which significantly reduces the computational burden. Numerous applications show that such a computation burden reduction does not degrade the generalization capability. It has, however, been open that whether this is true in theory. The aim of this paper is to study the theoretical feasibility of ELM by analyzing the pros and cons of ELM. In the previous part of this topic, we pointed out that via appropriately selected activation functions, ELM does not degrade the generalization capability in the sense of expectation. In this paper, we launch the study in a different direction and show that the randomness of ELM also leads to certain negative consequences. On one hand, we find that the randomness causes an additional uncertainty problem of ELM, both in approximation and learning. On the other hand, we theoretically justify that there also exist activation functions such that the corresponding ELM degrades the generalization capability. In particular, we prove that the generalization capability of ELM with Gaussian kernel is essentially worse than that of FNN with Gaussian kernel. To facilitate the use of ELM, we also provide a remedy to such a degradation. We find that the well-developed coefficient regularization technique can essentially improve the generalization capability. The obtained results reveal the essential characteristic of ELM in a certain sense and give theoretical guidance concerning how to use ELM.",2162-237X;2162237X,,10.1109/TNNLS.2014.2336665,Key Program of National Natural Science Foundation of China; National 973 Program; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6862852,Extreme learning machine (ELM);Gaussian kernel;generalization capability;neural networks,Approximation error;Degradation;Kernel;Learning systems;Neurons;Uncertainty,Gaussian distribution;approximation theory;feedforward neural nets;learning (artificial intelligence),ELM approximation;FNN learning system;Gaussian kernel;coefficient regularization technique;extreme learning machine;feed-forward neural network;generalization capability;uncertainty problem,"0;Algorithms;Artificial Intelligence;Computer Simulation;Humans;Learning;Models, Neurological;Neural Networks (Computer);Neurons;Probability",33,,37,,,20140723,Jan. 2015,,IEEE,IEEE Journals & Magazines
Predicting of Job Failure in Compute Cloud Based on Online Extreme Learning Machine: A Comparative Study,C. Liu; J. Han; Y. Shang; C. Liu; B. Cheng; J. Chen,"State Key laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Access,20170619,2017,5,,9359,9368,"Early prediction of job failures and specific disposal steps in advance could significantly improve the efficiency of resource utilization in large-scale data center. The existing machine learning-based prediction methods commonly adopt offline working pattern, which cannot be used for online prediction in practical operations, in which data arrive sequentially. To solve this problem, a new method based on online sequential extreme learning machine (OS-ELM) is proposed in this paper to predict online job termination status. With this method, real-time data are collected according to the sequence of job arriving, the job status could be predicted and the operation model is thus updated based on these data. The method with online incremental learning strategy has fast learning speed and good generalization. Comparative study using Google trace data shows that prediction accuracy of the proposed method is 93% with updating model in 0.01 s. Compared with some state-of-the-art methods, such, as support vector machine (SVM), ELM, and OS-SVM, the method developed in this paper has many advantages, such as less time-consuming in establishing and updating the model, higher prediction accuracy and precision, and better false negative performance.",2169-3536;21693536,,10.1109/ACCESS.2017.2706740,Key Science and Technology Program of Henan Province; National Grand Fundamental Research 973 Program of China; National Science-Technology Support Plan Projects; Natural Science Research Projects of the Department of Education of Henan Province; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932064,Cloud cluster;OS-ELM model;jobs termination status;online predicting;the Google cluster traces,Computational modeling;Data models;Google;Predictive models;Real-time systems;Support vector machines,cloud computing;computer centres;learning (artificial intelligence);resource allocation;system recovery,Google trace data;OS-ELM;compute cloud;disposal steps;job failure prediction;large-scale data center;machine learning-based prediction methods;offline working pattern;online incremental learning strategy;online job termination status prediction;online sequential extreme learning machine;operation model;resource utilization;time 0.01 s,,,,,,OAPA,20170519,2017,,IEEE,IEEE Journals & Magazines
Evolutionary Cost-Sensitive Extreme Learning Machine,L. Zhang; D. Zhang,"College of Communication Engineering, Chongqing University, Chongqing, China",IEEE Transactions on Neural Networks and Learning Systems,20171116,2017,28,12,3045,3060,"Conventional extreme learning machines (ELMs) solve a Moore-Penrose generalized inverse of hidden layer activated matrix and analytically determine the output weights to achieve generalized performance, by assuming the same loss from different types of misclassification. The assumption may not hold in cost-sensitive recognition tasks, such as face recognition-based access control system, where misclassifying a stranger as a family member may result in more serious disaster than misclassifying a family member as a stranger. Though recent cost-sensitive learning can reduce the total loss with a given cost matrix that quantifies how severe one type of mistake against another, in many realistic cases, the cost matrix is unknown to users. Motivated by these concerns, this paper proposes an evolutionary cost-sensitive ELM, with the following merits: 1) to the best of our knowledge, it is the first proposal of ELM in evolutionary cost-sensitive classification scenario; 2) it well addresses the open issue of how to define the cost matrix in cost-sensitive learning tasks; and 3) an evolutionary backtracking search algorithm is induced for adaptive cost matrix optimization. Experiments in a variety of cost-sensitive tasks well demonstrate the effectiveness of the proposed approaches, with about 5%-10% improvements.",2162-237X;2162237X,,10.1109/TNNLS.2016.2607757,Research Fund for Central Universities; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588107,Classification;cost matrix;cost-sensitive learning;extreme learning machine (ELM),Access control;Face;Face recognition;Kernel;Learning systems;Neurons;Training,evolutionary computation;learning (artificial intelligence);matrix algebra;pattern classification;search problems,Moore-Penrose inverse;access control system;adaptive cost matrix optimization;conventional extreme learning machines;cost-sensitive learning;cost-sensitive learning tasks;cost-sensitive recognition tasks;cost-sensitive tasks;evolutionary backtracking search algorithm;evolutionary cost-sensitive ELM;evolutionary cost-sensitive classification scenario;evolutionary cost-sensitive extreme learning machine;face recognition;generalized performance;hidden layer activated matrix,,2,,,,,20161011,Dec. 2017,,IEEE,IEEE Journals & Magazines
Efficient Digital Implementation of Extreme Learning Machines for Classification,S. Decherchi; P. Gastaldo; A. Leoncini; R. Zunino,"Department Drug Discovery and Development, Fondazione Istituto Italiano di Tecnologia (IIT), Genova, Italy",IEEE Transactions on Circuits and Systems II: Express Briefs,20120810,2012,59,8,496,500,"The availability of compact fast circuitry for the support of artificial neural systems is a long-standing and critical requirement for many important applications. This brief addresses the implementation of the powerful extreme learning machine (ELM) model on reconfigurable digital hardware (HW). The design strategy first provides a training procedure for ELMs, which effectively trades off prediction accuracy and network complexity. This, in turn, facilitates the optimization of HW resources. Finally, this brief describes and analyzes two implementation approaches: one involving field-programmable gate array devices and one embedding low-cost low-performance devices such as complex programmable logic devices. Experimental results show that, in both cases, the design approach yields efficient digital architectures with satisfactory performances and limited costs.",1549-7747;15497747,,10.1109/TCSII.2012.2204112,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6236105,Complex programmable logic device (CPLD);extreme learning machine (ELM);field-programmable gate array (FPGA);hardware (HW) neural networks (NNs),Computer architecture;Cost function;Feeds;Field programmable gate arrays;Neurons;Training;Vectors,,,,31,,18,,,20120710,Aug. 2012,,IEEE,IEEE Journals & Magazines
A New Multivariate Approach for Prognostics Based on Extreme Learning Machine and Fuzzy Clustering,K. Javed; R. Gouriveau; N. Zerhouni,"Automatic Control and Micro-Mechatronic Systems Department, FEMTO-ST Institute, Besan&#x00E7;on, France",IEEE Transactions on Cybernetics,20170520,2015,45,12,2626,2639,"Prognostics is a core process of prognostics and health management (PHM) discipline, that estimates the remaining useful life (RUL) of a degrading machinery to optimize its service delivery potential. However, machinery operates in a dynamic environment and the acquired condition monitoring data are usually noisy and subject to a high level of uncertainty/unpredictability, which complicates prognostics. The complexity further increases, when there is absence of prior knowledge about ground truth (or failure definition). For such issues, data-driven prognostics can be a valuable solution without deep understanding of system physics. This paper contributes a new data-driven prognostics approach namely, an “enhanced multivariate degradation modeling,” which enables modeling degrading states of machinery without assuming a homogeneous pattern. In brief, a predictability scheme is introduced to reduce the dimensionality of the data. Following that, the proposed prognostics model is achieved by integrating two new algorithms namely, the summation wavelet-extreme learning machine and subtractive-maximum entropy fuzzy clustering to show evolution of machine degradation by simultaneous predictions and discrete state estimation. The prognostics model is equipped with a dynamic failure threshold assignment procedure to estimate RUL in a realistic manner. To validate the proposition, a case study is performed on turbofan engines data from PHM challenge 2008 (NASA), and results are compared with recent publications.",2168-2267;21682267,,10.1109/TCYB.2014.2378056,Laboratory of Excellence ACTION funded by the French Government through the program ?Investments for the future? managed by the National Agency for Research (ANR-11-LABX-01-01); ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021915,Data-driven;extreme learning machine (ELM);fuzzy clustering;prognostics;remaining useful life (RUL),Clustering algorithms;Data models;Degradation;Machinery;Monitoring;Predictive models;Prognostics and health management,condition monitoring;jet engines;learning (artificial intelligence);machinery;mechanical engineering computing;remaining life assessment;statistical analysis;wavelet transforms,RUL;data-driven prognostics;degrading machinery;discrete state estimation;dynamic failure threshold assignment;enhanced multivariate degradation modeling;health management;machine degradation;multivariate approach;predictability scheme;remaining useful life;subtractive-maximum entropy fuzzy clustering;summation wavelet-extreme learning machine;turbofan engine,"1;Algorithms;Cluster Analysis;Engineering;Fuzzy Logic;Machine Learning;Models, Theoretical;Multivariate Analysis",16,,44,,,20150126,Dec. 2015,,IEEE,IEEE Journals & Magazines
MST-GEN: An Efficient Parameter Selection Method for One-Class Extreme Learning Machine,S. Wang; Q. Liu; E. Zhu; J. Yin; W. Zhao,"College of Computer, National University of Defense Technology, Changsha, China",IEEE Transactions on Cybernetics,20170906,2017,47,10,3266,3279,"One-class classification (OCC) models a set of target data from one class to detect outliers. OCC approaches like one-class support vector machine (OCSVM) and support vector data description (SVDD) have wide practical applications. Recently, one-class extreme learning machine (OCELM), which inherits the fast learning speed of original ELM and achieves equivalent or higher data description performance than OCSVM and SVDD, is proposed as a promising alternative. However, OCELM faces the same thorny parameter selection problem as OCSVM and SVDD. It significantly affects the performance of OCELM and remains under-explored. This paper proposes minimal spanning tree (MST)-GEN, an automatic way to select proper parameters for OCELM. Specifically, we first build a n-round MST to model the structure and distribution of the given target set. With information from n-round MST, a controllable number of pseudo outliers are generated by edge pattern detection and a novel “repelling” process, which readily overcomes two fundamental problems in previous outlier generation methods: where and how many pseudo outliers should be generated. Unlike previous methods that only generate pseudo outliers, we further exploit n-round MST to generate pseudo target data, so as to avoid the time-consuming cross-validation process and accelerate the parameter selection. Extensive experiments on various datasets suggest that the proposed method can select parameters for OCELM in a highly efficient and accurate manner when compared with existing methods, which enables OCELM to achieve better OCC performance in OCC applications. Furthermore, our experiments show that MST-GEN can also be favorably applied to other prevalent OCC methods like OCSVM and SVDD.",2168-2267;21682267,,10.1109/TCYB.2017.2707463,10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938706,Extreme learning machine (ELM);one-class classification (OCC);parameter selection,Data models;Image edge detection;Image reconstruction;Kernel;Shape;Support vector machines;Training,edge detection;pattern classification;support vector machines,MST-GEN;OCC applications;OCC methods;OCC models;OCC performance;OCELM;OCSVM;SVDD;data description performance;edge pattern detection;minimal spanning tree;n-round MST;one-class classification models;one-class extreme learning machine;one-class support vector machine;outlier generation methods;parameter selection method;pseudo outliers;pseudo target data;repelling process;support vector data description;thorny parameter selection problem;time-consuming cross-validation process,,,,,,,20170605,Oct. 2017,,IEEE,IEEE Journals & Magazines
Machine Learning Based Power Grid Outage Prediction in Response to Extreme Events,R. Eskandarpour; A. Khodaei,"Department of Electrical and Computer Engineering, University of Denver, Denver, CO, USA",IEEE Transactions on Power Systems,20170620,2017,32,4,3315,3316,"A machine learning based prediction method is proposed in this paper to determine the potential outage of power grid components in response to an imminent hurricane. The decision boundary, which partitions the components' states into two sets of damaged and operational, is obtained via logistic regression by using a second-order function and proper parameter fitting. Two metrics are examined to validate the performance of the obtained decision boundary in efficiently predicting component outages.",0885-8950;08858950,,10.1109/TPWRS.2016.2631895,US National Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752978,Extreme events;machine learning;power system resilience,Hurricanes;Logistics;Neural networks;Power grids;Predictive models;Regression tree analysis;Wind speed,learning (artificial intelligence);power engineering computing;power grids;power system reliability;regression analysis,decision boundary;logistic regression;machine learning based power grid outage prediction;parameter fitting;second-order function,,,,,,,20161122,Jul-17,,IEEE,IEEE Journals & Magazines
Local Receptive Fields Based Extreme Learning Machine,G. B. Huang; Z. Bai; L. L. C. Kasun; C. M. Vong,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Computational Intelligence Magazine,20150409,2015,10,2,18,29,"Extreme learning machine (ELM), which was originally proposed for ""generalized"" single-hidden layer feedforward neural networks (SLFNs), provides efficient unified learning solutions for the applications of feature learning, clustering, regression and classification. Different from the common understanding and tenet that hidden neurons of neural networks need to be iteratively adjusted during training stage, ELM theories show that hidden neurons are important but need not be iteratively tuned. In fact, all the parameters of hidden nodes can be independent of training samples and randomly generated according to any continuous probability distribution. And the obtained ELM networks satisfy universal approximation and classification capability. The fully connected ELM architecture has been extensively studied. However, ELM with local connections has not attracted much research attention yet. This paper studies the general architecture of locally connected ELM, showing that: 1) ELM theories are naturally valid for local connections, thus introducing local receptive fields to the input layer; 2) each hidden node in ELM can be a combination of several hidden nodes (a subnetwork), which is also consistent with ELM theories. ELM theories may shed a light on the research of different local receptive fields including true biological receptive fields of which the exact shapes and formula may be unknown to human beings. As a specific example of such general architectures, random convolutional nodes and a pooling structure are implemented in this paper. Experimental results on the NORB dataset, a benchmark for object recognition, show that compared with conventional deep learning solutions, the proposed local receptive fields based ELM (ELM-LRF) reduces the error rate from 6.5% to 2.7% and increases the learning speed up to 200 times.",1556-603X;1556603X,,10.1109/MCI.2015.2405316,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083684,,Approximation methods;Feature extraction;Feedforward neural networks;Iterative methods;Learning systems;Network architecture;Neural networks;Probability distribution;Regression analysis;Training,approximation theory;feedforward neural nets;learning (artificial intelligence);object recognition,ELM networks;ELM-LRF;NORB dataset;SLFN;continuous probability distribution;feature clustering;feature learning;generalized single-hidden layer feedforward neural networks;local receptive field based extreme learning machine;object recognition;unified learning solutions;universal approximation;universal classification capability,,79,,66,,,,May-15,,IEEE,IEEE Journals & Magazines
Firefly-Algorithm-Inspired Framework With Band Selection and Extreme Learning Machine for Hyperspectral Image Classification,H. Su; Y. Cai; Q. Du,"School of Earth Sciences and Engineering, Hohai University, Nanjing, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,20170520,2017,10,1,309,320,"A firefly algorithm (FA) inspired band selection and optimized extreme learning machine (ELM) for hyperspectral image classification is proposed. In this framework, FA is to select a subset of original bands to reduce the complexity of the ELM network. It is also adapted to optimize the parameters in ELM (i.e., regularization coefficient C, Gaussian kernel σ, and hidden number of neurons L). Due to very low complexity of ELM, its classification accuracy can be used as the objective function of FA during band selection and parameter optimization. In the experiments, two hyperspectral image datasets acquired by HYDICE and HYMAP are used, and the experiment results indicate that the proposed method can offer better performance, compared with particle swarm optimization and other related band selection algorithms.",1939-1404;19391404,,10.1109/JSTARS.2016.2591004,Fundamental Research Funds for the Central Universities; Institute of Remote Sensing and Digital Earth; Open Research Fund of Key Laboratory of Digital Earth Science; Priority Academic Program Development of Jiangsu Higher Education Institutions; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100002367 - Chinese Academy of Sciences; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7536149,Band selection;extreme learning machine (ELM);firefly algorithm (FA);hyperspectral image classification,Brightness;Hyperspectral imaging;Linear programming;Neurons;Optimization;Training,geophysical image processing;geophysical techniques;image classification,Gaussian kernel;HYDICE;HYMAP;band selection;classification accuracy;firefly-algorithm-inspired framework;hyperspectral image classification;optimized extreme learning machine,,7,,,,,20160808,Jan. 2017,,IEEE,IEEE Journals & Magazines
A 128-Channel Extreme Learning Machine-Based Neural Decoder for Brain Machine Interfaces,Y. Chen; E. Yao; A. Basu,"Centre of Excellence in IC Design (VIRTUS), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Biomedical Circuits and Systems,20160304,2016,10,3,679,692,"Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning coprocessor in 0.35- μm CMOS for the motor intention decoding in the brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 3.45 pJ/MAC at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3% for movement type. The same coprocessor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5% with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by ≈ 2X.",1932-4545;19324545,,10.1109/TBCAS.2015.2483618,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348721,Brain-machine interfaces;extreme learning machine;implant;machine learning;motor intention;neural decoding;neural network;portable;very large scale integration (VLSI),Decoding;Digital signal processing;Implants;Machine learning algorithms;Neurons;Sorting;Training,CMOS integrated circuits;biomechanics;biomedical electronics;biomedical equipment;brain-computer interfaces;learning (artificial intelligence);low-power electronics;medical computing;neurophysiology,128-channel extreme learning machine algorithm;CMOS;PC;asynchronous neural spikes;brain-machine interfaces;classification accuracy;core analog processor;digitally stored coefficients;frequency 50 Hz;low-power analog processing;machine learning coprocessor;monkey finger movements;neural decoder;pJ-MAC;sparsity promoting training scheme;state-of-the-art motor intention decoding algorithms;time-delayed feature dimension enhancement,,13,,43,,,20151207,Jun-16,,IEEE,IEEE Journals & Magazines
Unsupervised Feature Learning Classification With Radial Basis Function Extreme Learning Machine Using Graphic Processors,D. Lam; D. Wunsch,"Department of Electrical and Computer Engineering, Applied Computational Intelligence Laboratory, Missouri University of Science and Technology, Rolla, MO, USA",IEEE Transactions on Cybernetics,20170520,2017,47,1,224,231,"Ever-increasing size and complexity of data sets create challenges and potential tradeoffs of accuracy and speed in learning algorithms. This paper offers progress on both fronts. It presents a mechanism to train the unsupervised learning features learned from only one layer to improve performance in both speed and accuracy. The features are learned by an unsupervised feature learning (UFL) algorithm. Then, those features are trained by a fast radial basis function (RBF) extreme learning machine (ELM). By exploiting the massive parallel computing attribute of modern graphics processing unit, a customized compute unified device architecture (CUDA) kernel is developed to further speed up the computing of the RBF kernel in the ELM. Results tested on Canadian Institute for Advanced Research and Mixed National Institute of Standards and Technology data sets confirm the UFL RBF ELM achieves high accuracy, and the CUDA implementation is up to 20 times faster than CPU and the naive parallel approach.",2168-2267;21682267,,10.1109/TCYB.2015.2511149,Mary K. Finley Missouri Endowment; Missouri S&T Intelligent Systems Center; 10.13039/100000001 - National Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373625,Compute unified device architecture (CUDA);extreme learning machine (ELM);neural network;radial basis function (RBF);support vector machine (SVM),Graphics processing units;Instruction sets;Kernel;Machine learning;Support vector machines;Training,parallel architectures;parallel processing;radial basis function networks;unsupervised learning,RBF ELM;UFL algorithm;customized compute unified device architecture;graphic processors;massive parallel computing;modern graphics processing unit;radial basis function extreme learning machine;unsupervised feature learning classification algorithm,,,,,,,20160106,Jan. 2017,,IEEE,IEEE Journals & Magazines
Inverse-Free Extreme Learning Machine With Optimal Information Updating,S. Li; Z. H. You; H. Guo; X. Luo; Z. Q. Zhao,"Department Computing, Hong Kong Polytechnic University, Hong Kong",IEEE Transactions on Cybernetics,20170520,2016,46,5,1229,1241,"The extreme learning machine (ELM) has drawn insensitive research attentions due to its effectiveness in solving many machine learning problems. However, the matrix inversion operation involved in the algorithm is computational prohibitive and limits the wide applications of ELM in many scenarios. To overcome this problem, in this paper, we propose an inverse-free ELM to incrementally increase the number of hidden nodes, and update the connection weights progressively and optimally. Theoretical analysis proves the monotonic decrease of the training error with the proposed updating procedure and also proves the optimality in every updating step. Extensive numerical experiments show the effectiveness and accuracy of the proposed algorithm.",2168-2267;21682267,,10.1109/TCYB.2015.2434841,Young Scientist Foundation of Chongqing; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7115113,Extreme learning machine (ELM);inverse-free;neural networks;optimal updates,Accuracy;Approximation algorithms;Approximation error;Least squares approximations;Neural networks;Training,approximation theory;learning (artificial intelligence);matrix algebra,ELM;inverse-free extreme learning machine;machine learning problems;matrix inversion operation;neural networks;optimal information updating,,21,,64,,,20150601,May-16,,IEEE,IEEE Journals & Magazines
Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part I),X. Liu; S. Lin; J. Fang; Z. Xu,"School of Mathematics and Statistics, Institute for Information and System Sciences, Xi&#8217;an Jiaotong University, Xi&#x2019;an, China",IEEE Transactions on Neural Networks and Learning Systems,20170520,2015,26,1,7,20,"An extreme learning machine (ELM) is a feedforward neural network (FNN) like learning system whose connections with output neurons are adjustable, while the connections with and within hidden neurons are randomly fixed. Numerous applications have demonstrated the feasibility and high efficiency of ELM-like systems. It has, however, been open if this is true for any general applications. In this two-part paper, we conduct a comprehensive feasibility analysis of ELM. In Part I, we provide an answer to the question by theoretically justifying the following: 1) for some suitable activation functions, such as polynomials, Nadaraya-Watson and sigmoid functions, the ELM-like systems can attain the theoretical generalization bound of the FNNs with all connections adjusted, i.e., they do not degrade the generalization capability of the FNNs even when the connections with and within hidden neurons are randomly fixed; 2) the number of hidden neurons needed for an ELM-like system to achieve the theoretical bound can be estimated; and 3) whenever the activation function is taken as polynomial, the deduced hidden layer output matrix is of full column-rank, therefore the generalized inverse technique can be efficiently applied to yield the solution of an ELM-like system, and, furthermore, for the nonpolynomial case, the Tikhonov regularization can be applied to guarantee the weak regularity while not sacrificing the generalization capability. In Part II, however, we reveal a different aspect of the feasibility of ELM: there also exists some activation functions, which makes the corresponding ELM degrade the generalization capability. The obtained results underlie the feasibility and efficiency of ELM-like systems, and yield various generalizations and improvements of the systems as well.",2162-237X;2162237X,,10.1109/TNNLS.2014.2335212,National 973 Program of China; National Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6861448,Extreme learning machine (ELM);feasibility;generalization capability;neural networks;neural networks.,Biological neural networks;Estimation;Kernel;Learning systems;Neurons;Polynomials;Training,feedforward neural nets;generalisation (artificial intelligence);learning (artificial intelligence);transfer functions,ELM-like systems;FNN;Nadaraya-Watson functions;Tikhonov regularization;activation functions;extreme learning machine;feedforward neural network;full column-rank;generalization capability;generalized inverse technique;hidden layer output matrix;hidden neurons;learning system;sigmoid functions,"0;Algorithms;Artificial Intelligence;Computer Simulation;Generalization (Psychology);Humans;Learning;Models, Neurological;Nerve Net;Neural Networks (Computer);Neurons",46,,42,,,20140721,Jan. 2015,,IEEE,IEEE Journals & Magazines
A Fingerprint Method for Indoor Localization Using Autoencoder Based Deep Extreme Learning Machine,Z. E. Khatab; A. Hajihoseini; S. A. Ghorashi,"Department of Electrical Engineering, Cognitive Telecommunication Research Group, Shahid Beheshti University, Tehran, Iran",IEEE Sensors Letters,20180123,2018,2,1,1,4,"By growing the demand for location based services in indoor environments in recent years, fingerprint based indoor localization has attracted much research interest. The fingerprint localization method works based on received signal strength (RSS) in wireless sensor networks. This method uses RSS measurements from available transmitter sensors, which are collected by a smart phone with internal sensors. In this article, we propose a novel algorithm that takes advantage of deep learning, extreme learning machines, and high level extracted features by autoencoder to improve the localization performance in the feature extraction and the classification. Furthermore, as the fingerprint database needs to be updated (due to the dynamic nature of environment), we also increase the number of training data, in order to improve the localization performance, gradually. Simulation results indicate that the proposed method provides a significant improvement in localization performance by using high level extracted features by autoencoder and increasing the number of training data.",,,10.1109/LSENS.2017.2787651,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240634,Sensor applications;autoencoder;deep extreme learning machine;fingerprint;indoor localization;wireless sensor network,Decoding;Feature extraction;Machine learning;Neurons;Sensors;Training;Wireless sensor networks,,,,,,,,,20171227,Mar-18,,IEEE,IEEE Journals & Magazines
Extreme Learning Machine With Subnetwork Hidden Nodes for Regression and Classification,Y. Yang; Q. M. J. Wu,"Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada",IEEE Transactions on Cybernetics,20170520,2016,46,12,2885,2898,"As demonstrated earlier, the learning effectiveness and learning speed of single-hidden-layer feedforward neural networks are in general far slower than required, which has been a major bottleneck for many applications. Huang et al. proposed extreme learning machine (ELM) which improves the training speed by hundreds of times as compared to its predecessor learning techniques. This paper offers an ELM-based learning method that can grow subnetwork hidden nodes by pulling back residual network error to the hidden layer. Furthermore, the proposed method provides a similar or better generalization performance with remarkably fewer hidden nodes as compared to other ELM methods employing huge number of hidden nodes. Thus, the learning speed of the proposed technique is hundred times faster compared to other ELMs as well as to back propagation and support vector machines. The experimental validations for all methods are carried out on 32 data sets.",2168-2267;21682267,,10.1109/TCYB.2015.2492468,10.13039/501100000038 - Natural Sciences and Engineering Research Council of Canada; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314912,Extreme learning machine (ELM);feedforward neural network (FNN);image recognition;learning effectiveness;universal approximation,Artificial neural networks;Learning systems;Mathematical model;Neurons;Standards;Support vector machines;Training,feedforward neural nets;image classification;learning (artificial intelligence);regression analysis;support vector machines,ELM;back propagation;classification;extreme learning machine;feedforward neural networks;image recognition;regression;subnetwork hidden nodes;support vector machines,,4,,,,,20151102,Dec. 2016,,IEEE,IEEE Journals & Magazines
Extreme learning machine for estimating blocking probability of bufferless OBS/OPS networks,H. C. Leung; C. S. Leung; E. W. M. Wong; S. Li,"Department of Electronic Engineering, City University of Hong Kong, Kowloon, Hong Kong",IEEE/OSA Journal of Optical Communications and Networking,20170809,2017,9,8,682,692,"Recently, the neural network approach for the blocking probability evaluation on optical networks was proposed, in which the inputs of the neural network were the optical network parameters and the output was the blocking probability of the optical network. The numerical results showed that its evaluation speed of the blocking probability was thousands of times faster than that of the discrete event simulator. However, the existing approach had two drawbacks. First, when the blocking probability was small, there was a significant approximation error due to the high dynamic range of the blocking probability. Second, the single-hidden-layer feedforward network model was used, which needed some time-consuming training algorithms to learn the parameters of hidden nodes, such as backpropagation. To solve these problems, this paper proposes to use the mean squared error of the log blocking probability as the training objective and use the extreme learning machine (ELM) framework for the training. Our numerical results show that the blocking probability estimated by our training objective is much more accurate than that of the existing approach, and it is obtained efficiently due to the greatly simplified training procedure offered by the ELM.",1943-0620;19430620,,10.1364/JOCN.9.000682,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005559,Artificial neural network; Blocking probability; Network performance evaluation; Optical network,Adaptive optics;Approximation algorithms;Neural networks;Optical buffering;Optical fiber networks;Optical packet switching;Training,backpropagation;discrete event simulation;feedforward neural nets;mean square error methods;optical burst switching;probability,ELM;approximation error;backpropagation;bufferless OBS-OPS networks;discrete event simulator;extreme learning machine framework;hidden nodes;log blocking probability;mean squared error;neural network approach;optical network parameters;probability evaluation;single-hidden-layer feedforward network model;time-consuming training algorithms;training objective;training procedure,,,,,,,,Aug. 2017,,IEEE,IEEE Journals & Magazines
SAR Image Change Detection Based on Correlation Kernel and Multistage Extreme Learning Machine,L. Jia; M. Li; P. Zhang; Y. Wu,"National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi&#x0027;an, China",IEEE Transactions on Geoscience and Remote Sensing,20160811,2016,54,10,5993,6006,"Designing a kernel function with good discriminating ability and a highly application-adaptive kernelized classifier is the key of many kernel methods. However, not many kernel functions combining directly the bitemporal images' information are designed specifically for change detection tasks. In addition, extreme learning machine (ELM) has not found wide applications in change detection tasks, even though it is a potential kernel method possessing outstanding approximation and generalization capabilities as well as great classification accuracy and efficiency. Therefore, an approach relying on a difference correlation kernel (DCK) and a multistage ELM (MS-ELM) is proposed in this paper for synthetic aperture radar (SAR) image change detection. First, a DCK function is constructed specifically for change detection by measuring the “distance” between any two pixels. The DCK function depicts the cross-time similarities between couples of bitemporal image patches at any cyclic shifts with a kernel correlation operation and the high-order spatial distances between two differently located pixels with an algebraic subtraction. The DCK function possesses strong noise immunity and good identification of changed areas simultaneously. Second, an MS-ELM classifier is constructed for obtaining the change detection result. In MS-ELM, the hidden nodes and weights between the hidden and output layers are updated stage by stage by improving the kernel functions that compose them. Each stage of the MS-ELM is a standard kernel-ELM, and the DCK function is utilized in the first stage. The regenerative kernel functions incorporate the output spatial-neighborhood information of the previous stage for enhancing remarkably the MS-ELM's discriminating ability and noise resistance. The converged result at the last stage of MS-ELM is the final change detection result. Experiments on real SAR image change detection demonstrate the effectiveness of the - CK function and the MS-ELM algorithm, particularly its good identification of changed areas and strong robustness against noise in SAR images.",0196-2892;01962892,,10.1109/TGRS.2016.2578438,National Ministries and Research Foundation; Natural Science Basic Research Plan in Shaanxi Province of China; Specialized Research Fund for the Doctoral Program of Higher Education; 10.13039/501100001809 - Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498649,Change detection;difference correlation kernel (DCK) function;multistage extreme learning machine (ELM) (MS-ELM);spatial-neighborhood information;synthetic aperture radar (SAR) image,Change detection algorithms;Correlation;Feature extraction;Kernel;Standards;Support vector machines;Synthetic aperture radar,image classification;radar imaging;remote sensing by radar;synthetic aperture radar,DCK function;MS-ELM classifier;SAR image change detection;algebraic subtraction;application-adaptive kernelized classifier;bitemporal image patches;bitemporal images information;change detection tasks;correlation kernel;kernel function;kernel functions;kernel methods;multistage extreme learning machine;noise immunity;noise resistance;output spatial-neighborhood information;real SAR image change detection;standard kernel-ELM;synthetic aperture radar,,1,,,,,20160623,Oct. 2016,,IEEE,IEEE Journals & Magazines
Pruning Extreme Learning Machines Using the Successive Projections Algorithm,D. Parente Mesquita; J. Gomes; L. Ramos Rodrigues; R. Kawakami Galvao,"Univ. Fed. do Ceara, Fortaleza, Brazil",IEEE Latin America Transactions,20160211,2015,13,12,3974,3979,"Extreme Learning Machine (ELM) is a recently proposed machine learning method with successful applications in many domains. The key strengths of ELM are its simple formulation and the reduced number of hyper-parameters. Among these hyper-parameters, the number of hidden nodes has significant impact on ELM performance since too few/many hidden nodes may lead to underfitting/overfitting. In this work, we propose a pruning strategy for ELM using the Successive Projections Algorithm (SPA) as an approach to automatically find the number of hidden nodes. SPA was originally proposed for variable selection. In this work, it was adapted in order to be used to prune ELMs. The proposed method was compared to the Optimally Pruned Extreme Learning Machine algorithm (OP-ELM), which is considered as a state of the art method. Real world datasets were used to assess the performance of the proposed method for regression and classification problems. The application of the proposed model resulted in much simpler models with similar performance compared to the OP-ELM. For some classification instances, the performance of the proposed method outperformed the OP-ELM method.",1548-0992;15480992,,10.1109/TLA.2015.7404935,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404935,exterme learning machines;neural networks;prunning techniques,Classification algorithms;Feedforward neural networks;Input variables;Learning systems;Predictive models;Presses;Projection algorithms,data handling;learning (artificial intelligence),ELM;OP-ELM;SPA;hyper-parameters;optimally pruned extreme learning machine algorithm;pruning extreme learning machines;real world datasets;successive projections algorithm;variable selection,,1,,,,,,Dec. 2015,,IEEE,IEEE Journals & Magazines
Graph Embedded Extreme Learning Machine,A. Iosifidis; A. Tefas; I. Pitas,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece",IEEE Transactions on Cybernetics,20170520,2016,46,1,311,324,"In this paper, we propose a novel extension of the extreme learning machine (ELM) algorithm for single-hidden layer feedforward neural network training that is able to incorporate subspace learning (SL) criteria on the optimization process followed for the calculation of the network's output weights. The proposed graph embedded ELM (GEELM) algorithm is able to naturally exploit both intrinsic and penalty SL criteria that have been (or will be) designed under the graph embedding framework. In addition, we extend the proposed GEELM algorithm in order to be able to exploit SL criteria in arbitrary (even infinite) dimensional ELM spaces. We evaluate the proposed approach on eight standard classification problems and nine publicly available datasets designed for three problems related to human behavior analysis, i.e., the recognition of human face, facial expression, and activity. Experimental results denote the effectiveness of the proposed approach, since it outperforms other ELM-based classification schemes in all the cases.",2168-2267;21682267,,10.1109/TCYB.2015.2401973,European Union Seventh Framework Programme (FP7/2007¿2013) (IMPART); ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052327,Extreme learning machine (ELM);facial image classification;graph embedding;human action recognition,Kernel;Laplace equations;Matrix decomposition;Optimization;Training;Training data;Vectors,face recognition;feedforward neural nets;graph theory;image classification;learning (artificial intelligence);optimisation,ELM-based classification schemes;GEELM algorithm;SL criteria;facial expression;graph embedded ELM algorithm;graph embedded extreme learning machine;graph embedding framework;human behavior analysis;human face recognition;network output weights;optimization process;single-hidden layer feedforward neural network training;subspace learning criteria,"0;Face;Female;Human Activities;Humans;Image Processing, Computer-Assisted;Machine Learning;Male;Pattern Recognition, Automated;Video Recording",31,,58,,,20150302,Jan. 2016,,IEEE,IEEE Journals & Magazines
Sparse Bayesian Extreme Learning Machine for Multi-classification,J. Luo; C. M. Vong; P. K. Wong,"Department of Computer and Information Science, University of Macau, Taipa, Macao",IEEE Transactions on Neural Networks and Learning Systems,20170520,2014,25,4,836,843,"Extreme learning machine (ELM) has become a popular topic in machine learning in recent years. ELM is a new kind of single-hidden layer feedforward neural network with an extremely low computational cost. ELM, however, has two evident drawbacks: 1) the output weights solved by Moore-Penrose generalized inverse is a least squares minimization issue, which easily suffers from overfitting and 2) the accuracy of ELM is drastically sensitive to the number of hidden neurons so that a large model is usually generated. This brief presents a sparse Bayesian approach for learning the output weights of ELM in classification. The new model, called Sparse Bayesian ELM (SBELM), can resolve these two drawbacks by estimating the marginal likelihood of network outputs and automatically pruning most of the redundant hidden neurons during learning phase, which results in an accurate and compact model. The proposed SBELM is evaluated on wide types of benchmark classification problems, which verifies that the accuracy of SBELM model is relatively insensitive to the number of hidden neurons; and hence a much more compact model is always produced as compared with other state-of-the-art neural network classifiers.",2162-237X;2162237X,,10.1109/TNNLS.2013.2281839,; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615928,Bayesian learning;classification;extreme learning machine (ELM);sparsity,Accuracy;Bayes methods;Computational modeling;Couplings;Neurons;Support vector machines;Training,belief networks;feedforward neural nets;learning (artificial intelligence);minimisation;pattern classification,ELM;Moore-Penrose generalized inverse;benchmark classification problem;hidden neurons;least squares minimization;machine learning;marginal likelihood estimation;redundant hidden neurons;single-hidden layer feedforward neural network;sparse Bayesian extreme learning machine,"0;Algorithms;Artificial Intelligence;Bayes Theorem;Computer Simulation;Models, Theoretical;Pattern Recognition, Automated",52,,25,,,20130930,Apr-14,,IEEE,IEEE Journals & Magazines
Incremental and Decremental Extreme Learning Machine Based on Generalized Inverse,B. Jin; Z. Jing; H. Zhao,"School of Computer Science and Software Engineering, East China Normal University, Shanghai, China",IEEE Access,20171025,2017,5,,20852,20865,"In online sequential applications, a machine learning model needs to have a self-updating ability to handle the situation, which the training set is changing. Conventional incremental extreme learning machine (ELM) and online sequential ELM are usually achieved in two approaches: directly updating the output weight and recursively computing the left pseudo inverse of the hidden layer output matrix. In this paper, we develop a novel solution for incremental and decremental ELM (DELM), via recursively updating and downdating the generalized inverse of the hidden layer output matrix. By preserving the global optimality and best generalization performance, our approach implements node incremental ELM (N-IELM) and sample incremental ELM (S-IELM) in a universal form, and overcomes the problem of self-starting and numerical instability in the conventional online sequential ELM. We also propose sample DELM (S-DELM), which is the first decremental version of ELM. The experiments on regression and classification problems with real-world data sets demonstrate the feasibility and effectiveness of the proposed algorithms with encouraging performances.",,,10.1109/ACCESS.2017.2758645,10.13039/501100001809 - NSFC-Zhejiang Joint Fund for the Integration of Industrialization and Information; 10.13039/501100003399 - Key Program of Shanghai Science and Technology Commission; 10.13039/501100003399 - Municipality Projects of Shanghai Science and Technology Commission; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057769,Extreme learning machine;decremental ELM;generalized inverse;incremental ELM;online sequential ELM,Approximation error;Classification algorithms;Computational complexity;Computational modeling;Machine learning algorithms;Neurons;Training,,,,,,,,OAPA,20171004,2017,,IEEE,IEEE Journals & Magazines
A novel hidden danger prediction method in cloud-based intelligent industrial production management using timeliness managing extreme learning machine,X. Luo; Xiaona Yang; Weiping Wang; X. Chang; X. Wang; Zhigang Zhao,"School of Computer and Communication Engineering, University of Science and Technology Beijing, 100083, China",China Communications,20160902,2016,13,7,74,82,"To prevent possible accidents, the study of data-driven analytics to predict hidden dangers in cloud service-based intelligent industrial production management has been the subject of increasing interest recently. A machine learning algorithm that uses timeliness managing extreme learning machine is utilized in this article to achieve the above prediction. Compared with traditional learning algorithms, extreme learning machine (ELM) exhibits high performance because of its unique feature of a high generalization capability at a fast learning speed. Timeliness managing ELM is proposed by incorporating timeliness management scheme into ELM. When using the timeliness managing ELM scheme to predict hidden dangers, newly incremental data could be added prior to the historical data to maximize the contribution of the newly incremental training data, because the incremental data may be able to contribute reasonable weights to represent the current production situation according to practical analysis of accidents in some industrial productions. Experimental results from a coal mine show that the use of timeliness managing ELM can improve the prediction accuracy of hidden dangers with better stability compared with other similar machine learning methods.",1673-5447;16735447,,10.1109/CC.2016.7559078,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559078,cloud service;extreme learning machine;incremental learning;prediction,Artificial neural networks;Cloud computing;Data models;Machine learning algorithms;Neurons;Prediction algorithms;Production,accident prevention;cloud computing;learning (artificial intelligence);production engineering computing;production management,ELM;cloud-based intelligent industrial production management;data-driven analytics;hidden danger prediction method;machine learning algorithm;timeliness management scheme;timeliness managing extreme learning machine,,,,,,,,Jul-16,,IEEE,IEEE Journals & Magazines
Exploiting Intrinsic Variability of Filamentary Resistive Memory for Extreme Learning Machine Architectures,M. Suri; V. Parmar,"Department of Electrical Engineering, Indian Institute of Technology, New Delhi, India",IEEE Transactions on Nanotechnology,20151109,2015,14,6,963,968,"In this paper, we show for the first time how unavoidable device variability of emerging nonvolatile resistive memory devices can be exploited to design efficient low-power, low-footprint extreme learning machine (ELM) architectures. In particular, we utilize the uncontrollable off-state resistance (Roff/HRS) spreads, of nanoscale filamentary-resistive memory devices, to realize random input weights and random hidden neuron biases; a characteristic requirement of ELM. We propose a novel RRAM-ELM architecture. To validate our approach, experimental data from different filamentary-resistive switching devices (CBRAM, OXRAM) are used for full-network simulations. Learning capability of our RRAM-ELM architecture is illustrated with the help of two real-world applications: 1) diabetes diagnosis test (classification) and 2) SinC curve fitting (regression).",1536-125X;1536125X,,10.1109/TNANO.2015.2441112,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123635,Brain-Inspired;CBRAM;Cognitive Computing;Extreme Learning;Machine;Machine Learning;Neuromorphic;OXRAM;RRAM;Resistive Memory;brain-inspired;extreme learning machine;machine learning;neuromorphic;resistive memory;stochastic computing,Machine learning;Memory architecture;Nanoscale devices;Neuromorphic engineering;Random access memory;Stochastic systems,learning (artificial intelligence);memory architecture;random-access storage,CBRAM;OXRAM;RRAM-ELM architecture;SinC curve fitting;diabetes diagnosis test;extreme learning machine architectures;filamentary-resistive switching devices;full-network simulations;intrinsic variability;nanoscale filamentary-resistive memory devices;nonvolatile resistive memory devices;random hidden neuron biases;random input weights;unavoidable device variability;uncontrollable off-state resistance spreads,,12,,25,,,20150615,Nov. 2015,,IEEE,IEEE Journals & Magazines
Differential Evolution Extreme Learning Machine for the Classification of Hyperspectral Images,Y. Bazi; N. Alajlan; F. Melgani; H. AlHichri; S. Malek; R. R. Yager,"Adv. Lab. for Intell. Syst. Res. Lab., King Saud Univ., Riyadh, Saudi Arabia",IEEE Geoscience and Remote Sensing Letters,20140128,2014,11,6,1066,1070,"Recently, a new machine learning approach that is termed as the extreme learning machine (ELM) has been introduced in the literature. This approach is characterized by a unified formulation for regression, binary, and multiclass classification problems, and the related solution is given in an analytical compact form. In this letter, we propose an efficient classification method for hyperspectral images based on this machine learning approach. To address the model selection issue that is associated with the ELM, we develop an automatic-solution-based differential evolution (DE). This simple yet powerful evolutionary optimization algorithm uses cross-validation accuracy as a performance indicator for determining the optimal ELM parameters. Experimental results obtained from four benchmark hyperspectral data sets confirm the attractive properties of the proposed DE-ELM method in terms of classification accuracy and computation time.",1545-598X;1545598X,,10.1109/LGRS.2013.2286078,Distinguished Scientist Fellowship Program of King Saud University; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6656874,Differential evolution (DE);extreme learning machine (ELM);feature extraction;hyperspectral images,Hyperspectral imaging;Kernel;Support vector machines;Training;Vectors,evolutionary computation;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);regression analysis,DE-ELM method;automatic solution-based differential evolution;binary classification;differential evolution extreme learning machine;evolutionary optimization algorithm;hyperspectral image classification;model selection;multiclass classification problem;regression classification,,42,,20,,,20131106,Jun-14,,IEEE,IEEE Journals & Magazines
Fusion of Extreme Learning Machine and Graph-Based Optimization Methods for Active Classification of Remote Sensing Images,M. A. Bencherif; Y. Bazi; A. Guessoum; N. Alajlan; F. Melgani; H. AlHichri,"Dept. of Electron., Saad Dahlab Univ., Blida, Algeria",IEEE Geoscience and Remote Sensing Letters,20140916,2015,12,3,527,531,"In this letter, we propose an efficient multiclass active learning (AL) method for remote sensing image classification. We fuse the capabilities of an extreme learning machine (ELM) classifier and graph-based optimization methods to boost the classification accuracy while minimizing the user interaction. First, we use the ELM to generate an initial label estimation of the unlabeled image pixels. Then, we optimize a graph-based functional energy that integrates the ELM outputs as an initial estimation of the image structure. As for the ELM, the solution to this multiclass optimization problem leads to a system of linear equations. Due to the sparse Laplacian matrix built from the lattice graph defined on the image pixels, the optimization problem is solved in a linear time. In the experiments, we report and discuss the results of the proposed AL method on two very high resolution images acquired by IKONOS-2 and GoeEye-1, as well as the well-known Pavia University hyperspectral image.",1545-598X;1545598X,,10.1109/LGRS.2014.2349538,Deanship of Scientific Research of the King Saud University; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891215,Active learning (AL);extreme learning machine (ELM);graph-based optimization;multiclass classification,Accuracy;Educational institutions;Estimation;Optimization;Remote sensing;Sparse matrices;Training,Laplace equations;geophysical image processing;geophysical techniques;graph theory;hyperspectral imaging;image classification;image resolution;learning (artificial intelligence);matrix algebra;remote sensing,ELM classifier;GoeEye-1;IKONOS-2;Pavia University hyperspectral image;active classification;extreme learning machine;graph-based functional energy optimization;graph-based optimization method;image resolution;image structure estimation;initial label estimation;lattice graph;linear equations;multiclass active learning method;multiclass optimization problem;remote sensing image classification;sparse Laplacian matrix;unlabeled image pixels;user interaction minimization,,21,,11,,,20140904,Mar-15,,IEEE,IEEE Journals & Magazines
Reinforced Extreme Learning Machines for Fast Robust Regression in the Presence of Outliers,B. Frénay; M. Verleysen,"Faculty of Computer Science, Universit&#x00E9; de Namur, Namur, Belgium",IEEE Transactions on Cybernetics,20161115,2016,46,12,3351,3363,"Extreme learning machines (ELMs) are fast methods that obtain state-of-the-art results in regression. However, they are not robust to outliers and their meta-parameter (i.e., the number of neurons for standard ELMs and the regularization constant of output weights for L2-regularized ELMs) selection is biased by such instances. This paper proposes a new robust inference algorithm for ELMs which is based on the pointwise probability reinforcement methodology. Experiments show that the proposed approach produces results which are comparable to the state of the art, while being often faster.",2168-2267;21682267,,10.1109/TCYB.2015.2504404,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358117,Extreme learning machines (ELMs);outliers;pointwise probability reinforcements (PPRs);regularization;robust inference,Cybernetics;Inference algorithms;Linear regression;Maximum likelihood estimation;Neurons;Robustness;Standards,feedforward neural nets;inference mechanisms;learning (artificial intelligence);probability;regression analysis,ELM;inference algorithm;outliers;pointwise probability reinforcement methodology;regression;reinforced extreme learning machines,,1,,,,,20151217,Dec. 2016,,IEEE,IEEE Journals & Magazines
Distributed and weighted extreme learning machine for imbalanced big data learning,Z. Wang; J. Xin; H. Yang; S. Tian; G. Yu; C. Xu; Y. Yao,"Sino- Dutch Biomedical & Information Engineering School, Northeastern University, Shenyang 110169, China",Tsinghua Science and Technology,20170406,2017,22,2,160,173,"The Extreme Learning Machine (ELM) and its variants are effective in many machine learning applications such as Imbalanced Learning (IL) or Big Data (BD) learning. However, they are unable to solve both imbalanced and large-volume data learning problems. This study addresses the IL problem in BD applications. The Distributed and Weighted ELM (DW-ELM) algorithm is proposed, which is based on the MapReduce framework. To confirm the feasibility of parallel computation, first, the fact that matrix multiplication operators are decomposable is illustrated. Then, to further improve the computational efficiency, an Improved DW-ELM algorithm (IDW-ELM) is developed using only one MapReduce job. The successful operations of the proposed DW-ELM and IDW-ELM algorithms are finally validated through experiments.",,,10.23919/TST.2017.7889638,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889638,weighted Extreme Learning Machine (ELM); imbalanced big data; MapReduce framework; user-definedcounter,Big Data;Computer science;Distributed databases;Machine learning algorithms;Matrix decomposition;Prediction algorithms;Training,,,,,,,,,,Apr-17,,TUP,TUP Journals & Magazines
An Efficient Method for Traffic Sign Recognition Based on Extreme Learning Machine,Z. Huang; Y. Yu; J. Gu; H. Liu,"College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China",IEEE Transactions on Cybernetics,20170520,2017,47,4,920,933,"This paper proposes a computationally efficient method for traffic sign recognition (TSR). This proposed method consists of two modules: (1) extraction of histogram of oriented gradient variant (HOGv) feature and (2) a single classifier trained by extreme learning machine (ELM) algorithm. The presented HOGv feature keeps a good balance between redundancy and local details such that it can represent distinctive shapes better. The classifier is a single-hidden-layer feedforward network. Based on ELM algorithm, the connection between input and hidden layers realizes the random feature mapping while only the weights between hidden and output layers are trained. As a result, layer-by-layer tuning is not required. Meanwhile, the norm of output weights is included in the cost function. Therefore, the ELM-based classifier can achieve an optimal and generalized solution for multiclass TSR. Furthermore, it can balance the recognition accuracy and computational cost. Three datasets, including the German TSR benchmark dataset, the Belgium traffic sign classification dataset and the revised mapping and assessing the state of traffic infrastructure (revised MASTIF) dataset, are used to evaluate this proposed method. Experimental results have shown that this proposed method obtains not only high recognition accuracy but also extremely high computational efficiency in both training and recognition processes in these three datasets.",2168-2267;21682267,,10.1109/TCYB.2016.2533424,10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433451,Extreme learning machine (ELM);HOG variant (HOGv);traffic sign recognition (TSR),Computational efficiency;Feature extraction;Histograms;Neural networks;Robustness;Shape;Training,feature extraction;feedforward neural nets;image recognition;learning (artificial intelligence);traffic engineering computing,ELM;HOGv;TSR;extreme learning machine;feature extraction;histogram of oriented gradient variant;single-hidden-layer feedforward network;traffic sign recognition,,8,,,,,20160314,Apr-17,,IEEE,IEEE Journals & Magazines
Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares,N. Wang; M. J. Er; M. Han,"Marine Engineering College, Dalian Maritime University, Dalian, China",IEEE Transactions on Neural Networks and Learning Systems,20170520,2014,25,10,1828,1841,"Novel constructive and destructive parsimonious extreme learning machines (CP- and DP-ELM) are proposed in this paper. By virtue of the proposed ELMs, parsimonious structure and excellent generalization of multiinput-multioutput single hidden-layer feedforward networks (SLFNs) are obtained. The proposed ELMs are developed by innovative decomposition of the recursive orthogonal least squares procedure into sequential partial orthogonalization (SPO). The salient features of the proposed approaches are as follows: 1) Initial hidden nodes are randomly generated by the ELM methodology and recursively orthogonalized into an upper triangular matrix with dramatic reduction in matrix size; 2) the constructive SPO in the CP-ELM focuses on the partial matrix with the subcolumn of the selected regressor including nonzeros as the first column while the destructive SPO in the DP-ELM operates on the partial matrix including elements determined by the removed regressor; 3) termination criteria for CP- and DP-ELM are simplified by the additional residual error reduction method; and 4) the output weights of the SLFN need not be solved in the model selection procedure and is derived from the final upper triangular equation by backward substitution. Both single- and multi-output real-world regression data sets are used to verify the effectiveness and superiority of the CP- and DP-ELM in terms of parsimonious architecture and generalization accuracy. Innovative applications to nonlinear time-series modeling demonstrate superior identification results.",2162-237X;2162237X,,10.1109/TNNLS.2013.2296048,Applied Basic Research Funds from the Ministry of Transport of China; Fundamental Research Funds for the Central Universities of China; Program for Liaoning Excellent Talents in University; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100002858 - China Post-Doctoral Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704311,Extreme learning machine (ELM);parsimonious model selection;recursive orthogonal least squares (ROLS);sequential partial orthogonalization (SPO);single hidden-layer feedforward network (SLFN);single hidden-layer feedforward network (SLFN).,Context;Educational institutions;Mathematical model;Matrix decomposition;Training;Training data;Vectors,data analysis;feedforward neural nets;generalisation (artificial intelligence);learning (artificial intelligence);least squares approximations;matrix algebra;recursive functions;regression analysis;time series,CP-ELM;DP-ELM;ELM methodology;SLFN;backward substitution;constructive parsimonious extreme learning machine;destructive SPO;destructive parsimonious extreme learning machine;generalization accuracy;hidden node random generation;matrix size reduction;multiinput-multioutput single hidden-layer feedforward networks;nonlinear time-series modeling;parsimonious architecture;parsimonious structure;partial matrix;recursive orthogonal least squares decomposition;recursive orthogonalization;regression data set;regressor;residual error reduction method;sequential partial orthogonalization;termination criteria;upper triangular equation;upper triangular matrix,,53,,36,,,20140109,Oct. 2014,,IEEE,IEEE Journals & Magazines
Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation,L. Zhang; D. Zhang,"College of Communication Engineering, Chongqing University, Chongqing, China",IEEE Transactions on Image Processing,20160830,2016,25,10,4959,4973,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ<sub>2,1</sub>-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.",1057-7149;10577149,,10.1109/TIP.2016.2598679,Research fund of Central Universities; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539280,Domain adaptation;cross-domain learning;extreme learning machine;knowledge adaptation,Kernel;Object recognition;Robustness;Support vector machines;Training;Training data;Visualization,image classification;learning (artificial intelligence);neural nets,"ELM classifier;ELM-based domain adaptation;Laplacian graph;MvEDA;cross-domain network learning framework;extreme learning machine;l<sub>2,1</sub>-norm minimization;learning error minimization;manifold regularization;random projection;semisupervised learning;visual knowledge adaptation;visual knowledge transfer",,21,,,,,20160810,Oct. 2016,,IEEE,IEEE Journals & Magazines
Extreme Learning Machines [Trends & Controversies],E. Cambria; G. B. Huang; L. L. C. Kasun; H. Zhou; C. M. Vong; J. Lin; J. Yin; Z. Cai; Q. Liu; K. Li; V. C. M. Leung; L. Feng; Y. S. Ong; M. H. Lim; A. Akusok; A. Lendasse; F. Corona; R. Nian; Y. Miche; P. Gastaldo; R. Zunino; S. Decherchi; X. Yang; K. Mao; B. S. Oh; J. Jeon; K. A. Toh; A. B. J. Teoh; J. Kim; H. Yu; Y. Chen; J. Liu,MIT Media Laboratory,IEEE Intelligent Systems,20140206,2013,28,6,30,59,"This special issue includes eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In ""Representational Learning with ELMs for Big Data,"" Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In ""A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing,"" Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C.M. Leung propose a method for handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In ""ELM-Guided Memetic Computation for Vehicle Routing,"" Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In ""ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs,"" Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In ""Combining ELMs with Random Projections,"" Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In ""Reduced ELMs for Causal Relation Extraction from Unstructured Text,"" Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In ""A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures,"" Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry- for touchless applications without the need for handheld devices. Finally, in ""An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System,"" Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for touchless human-machine interaction.",1541-1672;15411672,,10.1109/MIS.2013.140,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733226,ELM;Kinect;OS-ELM;cloud computing;computation outsourcing;deep networks;ensemble;evolutionary optimization;extreme learning machine;hand gesture signature verification;human-computer interaction;knowledge extraction;memetic computation;meta meme;online sequential ELM;partitioned ELM;random permutations;random projections;representational learning;signature biometrics;text mining;visualization,Adaptive learning;Artificial intelligence;Big data;Biological neural networks;Data visualization;Gesture recognition;Learning systems;Machine learning;Nonhomogeneous media;Special issues and sections,,,,118,,4,,,,Nov.-Dec. 2013,,IEEE,IEEE Journals & Magazines
Classification of Hyperspectral Remote Sensing Image Using Hierarchical Local-Receptive-Field-Based Extreme Learning Machine,Q. Lv; X. Niu; Y. Dou; J. Xu; Y. Lei,"Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China",IEEE Geoscience and Remote Sensing Letters,20160229,2016,13,3,434,438,"This letter proposes a novel classification approach for a hyperspectral image (HSI) using a hierarchical local-receptive-field (LRF)-based extreme learning machine (ELM). As a fast and accurate pattern classification algorithm, ELM has been applied in numerous fields, including the HSI classification. The LRF concept originates from research in neuroscience. Considering the local correlations of spectral features, it is promising to improve the performance of HSI classification by introducing the LRFs. Recent research on deep learning has shown that hierarchical architectures with more layers can potentially extract abstract representation and invariant features for better classification performance. Therefore, we further extend the LRF-based ELM method to a hierarchical model for HSI classification. Experimental results on two widely used real hyperspectral data sets confirm the effectiveness of the proposed HSI classification approach.",1545-598X;1545598X,,10.1109/LGRS.2016.2517178,10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403893,Deep learning;extreme learning machine (ELM);hyperspectral image (HSI) classification;local receptive field (LRF),Convolution;Feature extraction;Hyperspectral imaging;Neurons;Training,geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);pattern classification;terrain mapping,ELM;HSI classification;LRF-based ELM method;hierarchical local-receptive-field-based extreme learning machine;hyperspectral remote sensing image classification;image representation;neuroscience;pattern classification algorithm;spectral feature,,1,,19,,,20160211,Mar-16,,IEEE,IEEE Journals & Magazines
Extreme Learning Machine for Regression and Multiclass Classification,G. B. Huang; H. Zhou; X. Ding; R. Zhang,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",20120315,2012,42,2,513,529,"Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.",1083-4419;10834419,,10.1109/TSMCB.2011.2168604,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035797,Extreme learning machine (ELM);feature mapping;kernel;least square support vector machine (LS-SVM);proximal support vector machine (PSVM);regularization network,Approximation methods;Feedforward neural networks;Kernel;Machine learning;Optimization;Support vector machines;Training,computational complexity;feedforward neural nets;learning (artificial intelligence);least squares approximations;optimisation;pattern classification;polynomials;regression analysis;support vector machines,ELM;LS-SVM;PSVM;binary classification applications;computational complexity;conventional feedforward neural networks;extreme learning machine;feature mapping;generalized single-hidden-layer feedforward networks;least square support vector machine;multiclass classification;optimization method;polynomial network;proximal support vector machine;regression;regularization algorithms,,1333,2,56,,,20111006,Apr-12,,IEEE,IEEE Journals & Magazines
A heterogeneous ensemble of extreme learning machines with correntropy and negative correlation,A. O. M. Abuassba; Y. Zhang; X. Luo; D. Zhang; W. Aziguli,"School of Computer and Communication Engineering, University of Science and Technology Beijing (USTB), Beijing 100083, China, and the Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing 100083, China",Tsinghua Science and Technology,20171214,2017,22,6,691,701,"The Extreme Learning Machine (ELM) is an effective learning algorithm for a Single-Layer Feedforward Network (SLFN). It performs well in managing some problems due to its fast learning speed. However, in practical applications, its performance might be affected by the noise in the training data. To tackle the noise issue, we propose a novel heterogeneous ensemble of ELMs in this article. Specifically, the correntropy is used to achieve insensitive performance to outliers, while implementing Negative Correlation Learning (NCL) to enhance diversity among the ensemble. The proposed Heterogeneous Ensemble of ELMs (HE<sup>2</sup>LM) for classification has different ELM algorithms including the Regularized ELM (RELM), the Kernel ELM (KELM), and the L<sub>2</sub>-norm-optimized ELM (ELML2). The ensemble is constructed by training a randomly selected ELM classifier on a subset of the training data selected through random resampling. Then, the class label of unseen data is predicted using a maximum weighted sum approach. After splitting the training data into subsets, the proposed HE<sup>2</sup>LM is tested through classification and regression tasks on real-world benchmark datasets and synthetic datasets. Hence, the simulation results show that compared with other algorithms, our proposed method can achieve higher prediction accuracy, better generalization, and less sensitivity to outliers.",,,10.23919/TST.2017.8195351,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8195351,Extreme Learning Machine (ELM);classification;correntropy;ensemble;negative correlation,Correlation;Diversity reception;Kernel;Optimization;Prediction algorithms;Training;Training data,,,,,,,,,,Dec-17,,TUP,TUP Journals & Magazines
